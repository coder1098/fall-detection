{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f865591-de74-4cdc-90b3-aa4971a6d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a6d15f0-3cfa-4a2a-98c2-b5a0740f4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "labels = []\n",
    "dir_path = r\"C:\\Users\\gdiva\\Downloads\\Fall-detection.v2-clean-elderly-efds-test.yolov8\\train\\labels\"\n",
    "\n",
    "try :\n",
    "    for path in os.listdir(dir_path) :\n",
    "        try :\n",
    "            file_path = os.path.join(dir_path, path)\n",
    "            if os.path.isfile(file_path) :\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data  = []\n",
    "                    data.extend(f.readlines())\n",
    "                    if len(data) == 1 :\n",
    "                        values = [float(val) for val in data[0].split()]\n",
    "                        labels.append(values)\n",
    "                        label.append([data,[\".\".join(path.split(\".\")[:-1])]])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"The directory {dir_path} does not exist\")\n",
    "        except e:\n",
    "            print(f\"error: {e}\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied to access the directory {dir_path}\")\n",
    "except OSError as e:\n",
    "    print(f\"An OS error occurred: {e}\")\n",
    "\n",
    "\n",
    "labels = np.array(labels)\n",
    "columns1 = labels[:, 0]\n",
    "columns2 = labels[:, 1]\n",
    "columns3 = labels[:, 2]\n",
    "columns4 = labels[:, 3]\n",
    "columns5 = labels[:, 4]\n",
    "\n",
    "for i in range(len(columns1)):\n",
    "    if columns1[i] == 2:\n",
    "        columns1[i] = 0\n",
    "columns1 = columns1.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80e18a35-2a34-4db0-85e2-29d0b1447c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "\n",
    "dir_path = r\"C:\\Users\\gdiva\\Downloads\\Fall-detection.v2-clean-elderly-efds-test.yolov8\\train\\images\"\n",
    "\n",
    "for lbel, path in label :\n",
    "    try :\n",
    "        file_path = os.path.join(dir_path, path[0] + \".jpg\")\n",
    "        img = cv2.imread(file_path)\n",
    "        raw_data.append(img)\n",
    "    except e:\n",
    "        print(f\"error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f71a4fa-5421-4bf1-9511-562e28340792",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_image = []\n",
    "for i in range(len(columns2)) :\n",
    "    img = np.copy(raw_data[i])\n",
    "    height, width, channels = img.shape\n",
    "    x_center, y_center, w, h = float(columns2[i])*width, float(columns3[i])*height, float(columns4[i])*width, float(columns5[i])*height\n",
    "    x1 = round(x_center-w/2)\n",
    "    y1 = round(y_center-h/2)\n",
    "    x2 = round(x_center+w/2)\n",
    "    y2 = round(y_center+h/2)\n",
    "    c = np.copy(img[y1:y2, x1:x2])\n",
    "    c = cv2.resize(c, (224, 224))\n",
    "    edit_image.append(c)\n",
    "\n",
    "edit_image = np.array(edit_image)\n",
    "edit_image = edit_image/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7338e444-650c-442b-80e2-1ec4e76f1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_num_images_fall_not_fall_bar(fall_count, not_fall_count):\n",
    "  fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "  ax.bar(['Fall', 'Not Fall'], [fall_count, not_fall_count], width=0.5)\n",
    "  ax.set_xlabel('Category')\n",
    "  ax.set_ylabel('Number of Images')\n",
    "  ax.set_title('Number of Images of Fall and Not Fall Categories (Bar Chart)')\n",
    "\n",
    "  # Adjust the font sizes to make the plot more readable.\n",
    "  ax.tick_params(labelsize=12)\n",
    "  ax.legend(prop={'size': 12})\n",
    "  ax.set_title(ax.get_title(), fontsize=14)\n",
    "\n",
    "  # Show the plot.\n",
    "  plt.show()\n",
    "\n",
    "# Example usage:\n",
    "fall_count = np.count_nonzero(columns1 == 1)\n",
    "not_fall_count = np.count_nonzero(columns1 == 0)\n",
    "\n",
    "plot_num_images_fall_not_fall_bar(fall_count, not_fall_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3e00eb4-5eb2-483e-8658-673831f5b9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 54, 54, 96)        34944     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 26, 26, 96)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 22, 22, 256)       614656    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 10, 10, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 384)         885120    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 384)         1327488   \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 4, 4, 256)         884992    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 1, 1, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              1052672   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21589378 (82.36 MB)\n",
      "Trainable params: 21589378 (82.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(256, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(384, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(384, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07fd46ff-e298-45d5-ab98-ce7a467ce19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 35s 974ms/step - loss: 0.6720 - accuracy: 0.5944 - val_loss: 0.5506 - val_accuracy: 0.7538\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 32s 967ms/step - loss: 0.6820 - accuracy: 0.6098 - val_loss: 0.6438 - val_accuracy: 0.6808\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 31s 948ms/step - loss: 0.6399 - accuracy: 0.6310 - val_loss: 0.6362 - val_accuracy: 0.6731\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 31s 936ms/step - loss: 0.6145 - accuracy: 0.6493 - val_loss: 0.6263 - val_accuracy: 0.5923\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 31s 953ms/step - loss: 0.5717 - accuracy: 0.6859 - val_loss: 0.4974 - val_accuracy: 0.7500\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 31s 938ms/step - loss: 0.5212 - accuracy: 0.7303 - val_loss: 0.5293 - val_accuracy: 0.7038\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 31s 941ms/step - loss: 0.4470 - accuracy: 0.7861 - val_loss: 0.5236 - val_accuracy: 0.7154\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 31s 937ms/step - loss: 0.4491 - accuracy: 0.7890 - val_loss: 0.6858 - val_accuracy: 0.6346\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 31s 943ms/step - loss: 0.4890 - accuracy: 0.7486 - val_loss: 0.4095 - val_accuracy: 0.7846\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 31s 941ms/step - loss: 0.4155 - accuracy: 0.7967 - val_loss: 0.4019 - val_accuracy: 0.8038\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 32s 958ms/step - loss: 0.4244 - accuracy: 0.8121 - val_loss: 0.3517 - val_accuracy: 0.8500\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 32s 971ms/step - loss: 0.3323 - accuracy: 0.8632 - val_loss: 0.3492 - val_accuracy: 0.8500\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 31s 944ms/step - loss: 0.2864 - accuracy: 0.8911 - val_loss: 0.2907 - val_accuracy: 0.8769\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 31s 939ms/step - loss: 0.2820 - accuracy: 0.8796 - val_loss: 0.3492 - val_accuracy: 0.8500\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 31s 934ms/step - loss: 0.2527 - accuracy: 0.9046 - val_loss: 0.3228 - val_accuracy: 0.8769\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 31s 940ms/step - loss: 0.2276 - accuracy: 0.9152 - val_loss: 0.2791 - val_accuracy: 0.8962\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 32s 961ms/step - loss: 0.1859 - accuracy: 0.9287 - val_loss: 0.2075 - val_accuracy: 0.9231\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 32s 964ms/step - loss: 0.2686 - accuracy: 0.9017 - val_loss: 0.7676 - val_accuracy: 0.8846\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 31s 938ms/step - loss: 0.2327 - accuracy: 0.9200 - val_loss: 0.3055 - val_accuracy: 0.8885\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 31s 932ms/step - loss: 0.1624 - accuracy: 0.9345 - val_loss: 0.2673 - val_accuracy: 0.9192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22715078190>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = train_test_split(edit_image, columns1, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, to_categorical(X_val, num_classes=2), validation_data=(y_train, to_categorical(y_val, num_classes=2)), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4cc1aad-5f70-4d34-94dc-1000cb761197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fall       0.96      0.90      0.93       147\n",
      "     No Fall       0.88      0.95      0.91       113\n",
      "\n",
      "    accuracy                           0.92       260\n",
      "   macro avg       0.92      0.92      0.92       260\n",
      "weighted avg       0.92      0.92      0.92       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plot_confusion_matrix(cm, labels):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.matshow(cm, cmap='Blues')\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], labels=labels, rotation=45)\n",
    "    plt.yticks([0, 1], labels=labels)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "y_true = np.copy(y_val)\n",
    "y_pred = []\n",
    "for img in y_train:\n",
    "    y_pred_0 = model.predict(np.expand_dims(img, axis=0))\n",
    "    class_i = 0 if y_pred_0[0][0] > 0.5 else 1\n",
    "    y_pred.append(class_i)\n",
    "y_pred = np.array(y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm, labels=[\"Fall\", \"No Fall\"])\n",
    "classification_report = classification_report(y_true, y_pred, target_names=[\"Fall\", \"No Fall\"])\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b164480a-29ab-4269-bb26-638efaf27932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved asD:/model/fall_detection_model1.pk\n"
     ]
    }
   ],
   "source": [
    "model_filename = \"D:/model/fall_detection_model1.pk\"\n",
    "joblib.dump(model, model_filename)\n",
    "print(\"Model saved as\" + model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f624b88-946d-4bf9-9083-2c200f1bd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grey_image(image):\n",
    "    if image.dtype != 'uint8':\n",
    "        image = (image * 255).astype('uint8')\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return gray_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b17f8b89-0146-4e33-9b66-4f40358cb9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_images = []\n",
    "for i in range(len(edit_image)):\n",
    "    grey_images.append(grey_image(edit_image[i]))\n",
    "grey_images = np.array(grey_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1623fa28-d189-4a7b-be10-583dd3e261d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 54, 54, 96)        11712     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 26, 26, 96)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 22, 22, 256)       614656    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 10, 10, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 384)         885120    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 384)         1327488   \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 4, 4, 256)         884992    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 1, 1, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              1052672   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21566146 (82.27 MB)\n",
      "Trainable params: 21566146 (82.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(224, 224, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(256, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(384, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(384, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61eb0b26-a4fb-4ece-ace5-2cea6fbbd3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1298, 224, 224)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grey_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c39f8d-bfda-4f4f-bb68-5ce09552cfe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33/33 [==============================] - 24s 724ms/step - loss: 0.3394 - accuracy: 0.8680 - val_loss: 0.4104 - val_accuracy: 0.8154\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 24s 715ms/step - loss: 0.2830 - accuracy: 0.9104 - val_loss: 0.4562 - val_accuracy: 0.8308\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 25s 750ms/step - loss: 0.2672 - accuracy: 0.8834 - val_loss: 0.3479 - val_accuracy: 0.8577\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 25s 758ms/step - loss: 0.2039 - accuracy: 0.9191 - val_loss: 0.4332 - val_accuracy: 0.8615\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 25s 755ms/step - loss: 0.1872 - accuracy: 0.9306 - val_loss: 0.4530 - val_accuracy: 0.8615\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 25s 761ms/step - loss: 0.1592 - accuracy: 0.9461 - val_loss: 0.3377 - val_accuracy: 0.8923\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 27s 809ms/step - loss: 0.4139 - accuracy: 0.8728 - val_loss: 0.4473 - val_accuracy: 0.8154\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 27s 807ms/step - loss: 0.4759 - accuracy: 0.8786 - val_loss: 0.3508 - val_accuracy: 0.8500\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 27s 804ms/step - loss: 0.2496 - accuracy: 0.9027 - val_loss: 0.4198 - val_accuracy: 0.8615\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 26s 795ms/step - loss: 0.2065 - accuracy: 0.9229 - val_loss: 0.3550 - val_accuracy: 0.8346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17bc40ae790>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = train_test_split(grey_images, columns1, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, to_categorical(X_val, num_classes=2), validation_data=(y_train, to_categorical(y_val, num_classes=2)), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae6c976a-2d8c-4540-82fb-32b6021d167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, labels):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.matshow(cm, cmap='Blues')\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], labels=labels, rotation=45)\n",
    "    plt.yticks([0, 1], labels=labels)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ad4b4b9-56d8-467d-a782-69d777af3315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fall       0.80      0.95      0.87       147\n",
      "     No Fall       0.91      0.69      0.78       113\n",
      "\n",
      "    accuracy                           0.83       260\n",
      "   macro avg       0.85      0.82      0.82       260\n",
      "weighted avg       0.85      0.83      0.83       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = np.copy(y_val)\n",
    "y_pred = []\n",
    "for img in y_train:\n",
    "    y_pred_0 = model.predict(np.expand_dims(img, axis=0))\n",
    "    class_i = 0 if y_pred_0[0][0] > 0.5 else 1\n",
    "    y_pred.append(class_i)\n",
    "y_pred = np.array(y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm, labels=[\"Fall\", \"No Fall\"])\n",
    "classification_report = classification_report(y_true, y_pred, target_names=[\"Fall\", \"No Fall\"])\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d45631d1-98cd-4c3c-8e36-acf23b715ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved asD:/model/grey_fall_detection_model1.pk\n"
     ]
    }
   ],
   "source": [
    "model_filename = \"D:/model/grey_fall_detection_model1.pk\"\n",
    "joblib.dump(model, model_filename)\n",
    "print(\"Model saved as\" + model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21f7975-5e08-42a1-8076-e843d30b1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model_filename = \"D:/model/fall_detection_model.pk\"\n",
    "model = joblib.load(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b94097a-7d76-42db-b827-c1172805f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83778476-d5bb-454d-80e6-7f7234141d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    img = np.copy(image)\n",
    "    if img.any():\n",
    "        img = cv2.resize(img, (224, 224)) \n",
    "        img = img / 255.0  \n",
    "    \n",
    "        # Make a prediction\n",
    "        prediction = model.predict(np.expand_dims(img, axis=0))\n",
    "    \n",
    "        # Convert the prediction to a class label\n",
    "        class_label = \"No Fall\" if prediction[0][0] > 0.5 else \"Fall\"\n",
    "    \n",
    "        return prediction[0][0]\n",
    "    return 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "576a6a97-3db6-4d66-8376-594485325d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c8a7b3-5048-4454-b6c3-49b5a3c59ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba9d00a-43e8-4499-940e-881004f9e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = YOLO('model_fall2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e22c53-d18b-4b3c-aa8c-5bdb8c6e6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sort.sort import *\n",
    "mot_tracker = Sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8e9708-5a96-4486-b94c-139fc1611cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 115.9ms\n",
      "Speed: 5.6ms preprocess, 115.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x320 1 -1, 93.3ms\n",
      "Speed: 4.4ms preprocess, 93.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 134.3ms\n",
      "Speed: 1.9ms preprocess, 134.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7228])\n",
      "data: tensor([[  0.7283,  46.4086, 114.0000, 196.5595,   0.7228,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (232, 114)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 57.3642, 121.4840, 113.2717, 150.1509]])\n",
      "xywhn: tensor([[0.5032, 0.5236, 0.9936, 0.6472]])\n",
      "xyxy: tensor([[  0.7283,  46.4086, 114.0000, 196.5595]])\n",
      "xyxyn: tensor([[0.0064, 0.2000, 1.0000, 0.8472]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 2 -1s, 108.4ms\n",
      "Speed: 3.1ms preprocess, 108.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 106.7ms\n",
      "Speed: 2.0ms preprocess, 106.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6881])\n",
      "data: tensor([[ 16.1523,  36.8054, 113.0000, 189.7759,   0.6881,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (195, 113)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 64.5761, 113.2906,  96.8477, 152.9705]])\n",
      "xywhn: tensor([[0.5715, 0.5810, 0.8571, 0.7845]])\n",
      "xyxy: tensor([[ 16.1523,  36.8054, 113.0000, 189.7759]])\n",
      "xyxyn: tensor([[0.1429, 0.1887, 1.0000, 0.9732]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2854])\n",
      "data: tensor([[  8.1337,  37.2231, 107.2488, 159.2812,   0.2854,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (195, 113)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 57.6912,  98.2521,  99.1151, 122.0581]])\n",
      "xywhn: tensor([[0.5105, 0.5039, 0.8771, 0.6259]])\n",
      "xyxy: tensor([[  8.1337,  37.2231, 107.2488, 159.2812]])\n",
      "xyxyn: tensor([[0.0720, 0.1909, 0.9491, 0.8168]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 1 -1, 139.5ms\n",
      "Speed: 0.0ms preprocess, 139.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 106.7ms\n",
      "Speed: 5.0ms preprocess, 106.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6911])\n",
      "data: tensor([[  6.0163,  29.3245, 111.0000, 165.1470,   0.6911,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (171, 111)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 58.5081,  97.2358, 104.9837, 135.8225]])\n",
      "xywhn: tensor([[0.5271, 0.5686, 0.9458, 0.7943]])\n",
      "xyxy: tensor([[  6.0163,  29.3245, 111.0000, 165.1470]])\n",
      "xyxyn: tensor([[0.0542, 0.1715, 1.0000, 0.9658]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 3 -1s, 191.2ms\n",
      "Speed: 5.1ms preprocess, 191.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 -1, 160.7ms\n",
      "Speed: 12.9ms preprocess, 160.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4660])\n",
      "data: tensor([[ 0.0000,  0.0000, 45.8189, 65.3115,  0.4660,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 73)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.9094, 32.6558, 45.8189, 65.3115]])\n",
      "xywhn: tensor([[0.3138, 0.4599, 0.6277, 0.9199]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 45.8189, 65.3115]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6277, 0.9199]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3293])\n",
      "data: tensor([[ 0.1067,  1.7050, 30.9019, 64.1386,  0.3293,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 73)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.5043, 32.9218, 30.7951, 62.4337]])\n",
      "xywhn: tensor([[0.2124, 0.4637, 0.4219, 0.8793]])\n",
      "xyxy: tensor([[ 0.1067,  1.7050, 30.9019, 64.1386]])\n",
      "xyxyn: tensor([[0.0015, 0.0240, 0.4233, 0.9034]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2734])\n",
      "data: tensor([[18.3902,  1.9970, 67.9986, 67.6506,  0.2734,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 73)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[43.1944, 34.8238, 49.6085, 65.6536]])\n",
      "xywhn: tensor([[0.5917, 0.4905, 0.6796, 0.9247]])\n",
      "xyxy: tensor([[18.3902,  1.9970, 67.9986, 67.6506]])\n",
      "xyxyn: tensor([[0.2519, 0.0281, 0.9315, 0.9528]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6470])\n",
      "data: tensor([[ 14.4414,  21.5801, 105.8734, 147.8079,   0.6470,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (154, 109)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 60.1574,  84.6940,  91.4319, 126.2278]])\n",
      "xywhn: tensor([[0.5519, 0.5500, 0.8388, 0.8197]])\n",
      "xyxy: tensor([[ 14.4414,  21.5801, 105.8734, 147.8079]])\n",
      "xyxyn: tensor([[0.1325, 0.1401, 0.9713, 0.9598]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 122.9ms\n",
      "Speed: 0.0ms preprocess, 122.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 148.9ms\n",
      "Speed: 10.9ms preprocess, 148.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 -1, 120.7ms\n",
      "Speed: 0.0ms preprocess, 120.7ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4472])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.3419, 63.5707,  0.4472,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 69)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.6710, 31.7853, 47.3419, 63.5707]])\n",
      "xywhn: tensor([[0.3431, 0.4477, 0.6861, 0.8954]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.3419, 63.5707]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6861, 0.8954]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5259])\n",
      "data: tensor([[  4.5250,  15.3708,  94.7935, 124.2804,   0.5259,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (142, 106)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 49.6592,  69.8256,  90.2685, 108.9096]])\n",
      "xywhn: tensor([[0.4685, 0.4917, 0.8516, 0.7670]])\n",
      "xyxy: tensor([[  4.5250,  15.3708,  94.7935, 124.2804]])\n",
      "xyxyn: tensor([[0.0427, 0.1082, 0.8943, 0.8752]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 97.0ms\n",
      "Speed: 2.5ms preprocess, 97.0ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 2 -1s, 157.4ms\n",
      "Speed: 3.1ms preprocess, 157.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x512 2 -1s, 131.0ms\n",
      "Speed: 0.0ms preprocess, 131.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3814])\n",
      "data: tensor([[26.0169,  3.0272, 67.0000, 67.8768,  0.3814,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (69, 67)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[46.5084, 35.4520, 40.9831, 64.8495]])\n",
      "xywhn: tensor([[0.6942, 0.5138, 0.6117, 0.9398]])\n",
      "xyxy: tensor([[26.0169,  3.0272, 67.0000, 67.8768]])\n",
      "xyxyn: tensor([[0.3883, 0.0439, 1.0000, 0.9837]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3259])\n",
      "data: tensor([[ 0.1510,  0.4013, 34.0567, 67.2082,  0.3259,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (69, 67)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[17.1038, 33.8047, 33.9057, 66.8068]])\n",
      "xywhn: tensor([[0.2553, 0.4899, 0.5061, 0.9682]])\n",
      "xyxy: tensor([[ 0.1510,  0.4013, 34.0567, 67.2082]])\n",
      "xyxyn: tensor([[0.0023, 0.0058, 0.5083, 0.9740]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6032])\n",
      "data: tensor([[ 15.0137,  12.5345,  77.8367, 118.8576,   0.6032,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (133, 106)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 46.4252,  65.6961,  62.8229, 106.3231]])\n",
      "xywhn: tensor([[0.4380, 0.4940, 0.5927, 0.7994]])\n",
      "xyxy: tensor([[ 15.0137,  12.5345,  77.8367, 118.8576]])\n",
      "xyxyn: tensor([[0.1416, 0.0942, 0.7343, 0.8937]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4237])\n",
      "data: tensor([[  9.9102,  11.8414, 103.9333, 117.2884,   0.4237,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (133, 106)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 56.9218,  64.5649,  94.0231, 105.4470]])\n",
      "xywhn: tensor([[0.5370, 0.4855, 0.8870, 0.7928]])\n",
      "xyxy: tensor([[  9.9102,  11.8414, 103.9333, 117.2884]])\n",
      "xyxyn: tensor([[0.0935, 0.0890, 0.9805, 0.8819]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 122.1ms\n",
      "Speed: 0.0ms preprocess, 122.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 2 -1s, 158.9ms\n",
      "Speed: 8.1ms preprocess, 158.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 -1, 148.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4138])\n",
      "data: tensor([[ 0.0000,  0.0000, 45.9032, 64.3330,  0.4138,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 73)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.9516, 32.1665, 45.9032, 64.3330]])\n",
      "xywhn: tensor([[0.3144, 0.4530, 0.6288, 0.9061]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 45.9032, 64.3330]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6288, 0.9061]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3349])\n",
      "data: tensor([[2.6845e-02, 1.3571e+00, 3.0147e+01, 6.3260e+01, 3.3493e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 73)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.0867, 32.3085, 30.1197, 61.9028]])\n",
      "xywhn: tensor([[0.2067, 0.4550, 0.4126, 0.8719]])\n",
      "xyxy: tensor([[2.6845e-02, 1.3571e+00, 3.0147e+01, 6.3260e+01]])\n",
      "xyxyn: tensor([[3.6774e-04, 1.9114e-02, 4.1297e-01, 8.9098e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4220])\n",
      "data: tensor([[ 12.3925,   5.7140,  85.5421, 120.8956,   0.4220,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (127, 104)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 48.9673,  63.3048,  73.1496, 115.1816]])\n",
      "xywhn: tensor([[0.4708, 0.4985, 0.7034, 0.9069]])\n",
      "xyxy: tensor([[ 12.3925,   5.7140,  85.5421, 120.8956]])\n",
      "xyxyn: tensor([[0.1192, 0.0450, 0.8225, 0.9519]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.3ms preprocess, 148.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 89.3ms\n",
      "Speed: 3.3ms preprocess, 89.3ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 1 1, 157.2ms\n",
      "Speed: 6.1ms preprocess, 157.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 -1, 136.7ms\n",
      "Speed: 7.0ms preprocess, 136.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4571])\n",
      "data: tensor([[ 0.0000,  0.9248, 46.2722, 66.9650,  0.4571,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 75)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.1361, 33.9449, 46.2722, 66.0402]])\n",
      "xywhn: tensor([[0.3085, 0.4650, 0.6170, 0.9047]])\n",
      "xyxy: tensor([[ 0.0000,  0.9248, 46.2722, 66.9650]])\n",
      "xyxyn: tensor([[0.0000, 0.0127, 0.6170, 0.9173]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3247])\n",
      "data: tensor([[27.4497, 59.0139, 74.9206, 72.7890,  0.3247,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 75)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[51.1852, 65.9015, 47.4709, 13.7751]])\n",
      "xywhn: tensor([[0.6825, 0.9028, 0.6329, 0.1887]])\n",
      "xyxy: tensor([[27.4497, 59.0139, 74.9206, 72.7890]])\n",
      "xyxyn: tensor([[0.3660, 0.8084, 0.9989, 0.9971]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4265])\n",
      "data: tensor([[ 12.2353,   4.9187,  87.0846, 119.3016,   0.4265,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 103)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 49.6600,  62.1102,  74.8493, 114.3829]])\n",
      "xywhn: tensor([[0.4821, 0.5050, 0.7267, 0.9299]])\n",
      "xyxy: tensor([[ 12.2353,   4.9187,  87.0846, 119.3016]])\n",
      "xyxyn: tensor([[0.1188, 0.0400, 0.8455, 0.9699]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 127.3ms\n",
      "Speed: 3.3ms preprocess, 127.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 2 -1s, 1 1, 167.6ms\n",
      "Speed: 2.2ms preprocess, 167.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x576 1 -1, 168.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4991])\n",
      "data: tensor([[ 0.0000,  0.4869, 47.0653, 66.5772,  0.4991,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 76)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.5327, 33.5320, 47.0653, 66.0904]])\n",
      "xywhn: tensor([[0.3096, 0.4593, 0.6193, 0.9053]])\n",
      "xyxy: tensor([[ 0.0000,  0.4869, 47.0653, 66.5772]])\n",
      "xyxyn: tensor([[0.0000, 0.0067, 0.6193, 0.9120]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2826])\n",
      "data: tensor([[15.2627,  0.0000, 52.9097, 67.2048,  0.2826,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 76)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[34.0862, 33.6024, 37.6470, 67.2048]])\n",
      "xywhn: tensor([[0.4485, 0.4603, 0.4954, 0.9206]])\n",
      "xyxy: tensor([[15.2627,  0.0000, 52.9097, 67.2048]])\n",
      "xyxyn: tensor([[0.2008, 0.0000, 0.6962, 0.9206]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.2607])\n",
      "data: tensor([[23.9863, 58.4783, 75.8553, 72.9912,  0.2607,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 76)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[49.9208, 65.7347, 51.8690, 14.5129]])\n",
      "xywhn: tensor([[0.6569, 0.9005, 0.6825, 0.1988]])\n",
      "xyxy: tensor([[23.9863, 58.4783, 75.8553, 72.9912]])\n",
      "xyxyn: tensor([[0.3156, 0.8011, 0.9981, 0.9999]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3471])\n",
      "data: tensor([[ 13.3274,   2.5473, 100.6066, 117.4372,   0.3471,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 103)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 56.9670,  59.9922,  87.2793, 114.8899]])\n",
      "xywhn: tensor([[0.5531, 0.4999, 0.8474, 0.9574]])\n",
      "xyxy: tensor([[ 13.3274,   2.5473, 100.6066, 117.4372]])\n",
      "xyxyn: tensor([[0.1294, 0.0212, 0.9768, 0.9786]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 8.2ms preprocess, 168.2ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 109.1ms\n",
      "Speed: 0.0ms preprocess, 109.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 168.4ms\n",
      "Speed: 5.2ms preprocess, 168.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x576 2 -1s, 153.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4925])\n",
      "data: tensor([[ 0.0000,  0.0000, 49.1698, 64.7268,  0.4925,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 72)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[24.5849, 32.3634, 49.1698, 64.7268]])\n",
      "xywhn: tensor([[0.3415, 0.4558, 0.6829, 0.9116]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 49.1698, 64.7268]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6829, 0.9116]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3312])\n",
      "data: tensor([[17.8021,  2.7848, 70.1147, 67.6004,  0.3312,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 72)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[43.9584, 35.1926, 52.3126, 64.8157]])\n",
      "xywhn: tensor([[0.6105, 0.4957, 0.7266, 0.9129]])\n",
      "xyxy: tensor([[17.8021,  2.7848, 70.1147, 67.6004]])\n",
      "xyxyn: tensor([[0.2473, 0.0392, 0.9738, 0.9521]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3729])\n",
      "data: tensor([[ 14.6153,   3.3010,  97.0444, 111.0449,   0.3729,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (118, 102)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 55.8299,  57.1730,  82.4291, 107.7439]])\n",
      "xywhn: tensor([[0.5474, 0.4845, 0.8081, 0.9131]])\n",
      "xyxy: tensor([[ 14.6153,   3.3010,  97.0444, 111.0449]])\n",
      "xyxyn: tensor([[0.1433, 0.0280, 0.9514, 0.9411]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2646])\n",
      "data: tensor([[ 13.5004,   5.5792,  75.0371, 115.1367,   0.2646,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (118, 102)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 44.2688,  60.3579,  61.5367, 109.5574]])\n",
      "xywhn: tensor([[0.4340, 0.5115, 0.6033, 0.9285]])\n",
      "xyxy: tensor([[ 13.5004,   5.5792,  75.0371, 115.1367]])\n",
      "xyxyn: tensor([[0.1324, 0.0473, 0.7357, 0.9757]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 13.4ms preprocess, 153.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 125.1ms\n",
      "Speed: 4.6ms preprocess, 125.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 155.9ms\n",
      "Speed: 0.0ms preprocess, 155.9ms inference, 12.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x576 1 -1, 155.6ms\n",
      "Speed: 9.0ms preprocess, 155.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4058])\n",
      "data: tensor([[1.1382e-02, 0.0000e+00, 4.8501e+01, 6.4855e+01, 4.0576e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 70)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[24.2560, 32.4274, 48.4893, 64.8547]])\n",
      "xywhn: tensor([[0.3465, 0.4567, 0.6927, 0.9134]])\n",
      "xyxy: tensor([[1.1382e-02, 0.0000e+00, 4.8501e+01, 6.4855e+01]])\n",
      "xyxyn: tensor([[1.6260e-04, 0.0000e+00, 6.9287e-01, 9.1345e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3615])\n",
      "data: tensor([[20.3260,  3.3678, 68.5816, 68.4461,  0.3615,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 70)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[44.4538, 35.9070, 48.2556, 65.0784]])\n",
      "xywhn: tensor([[0.6351, 0.5057, 0.6894, 0.9166]])\n",
      "xyxy: tensor([[20.3260,  3.3678, 68.5816, 68.4461]])\n",
      "xyxyn: tensor([[0.2904, 0.0474, 0.9797, 0.9640]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3352])\n",
      "data: tensor([[ 14.4430,   2.1651,  99.9730, 114.6881,   0.3352,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (116, 103)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 57.2080,  58.4266,  85.5299, 112.5230]])\n",
      "xywhn: tensor([[0.5554, 0.5037, 0.8304, 0.9700]])\n",
      "xyxy: tensor([[ 14.4430,   2.1651,  99.9730, 114.6881]])\n",
      "xyxyn: tensor([[0.1402, 0.0187, 0.9706, 0.9887]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 99.0ms\n",
      "Speed: 0.0ms preprocess, 99.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x608 3 -1s, 192.4ms\n",
      "Speed: 0.0ms preprocess, 192.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x576 2 -1s, 137.5ms\n",
      "Speed: 5.7ms preprocess, 137.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4151])\n",
      "data: tensor([[22.5273,  3.1438, 67.0000, 68.6728,  0.4151,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 67)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[44.7636, 35.9083, 44.4727, 65.5290]])\n",
      "xywhn: tensor([[0.6681, 0.5058, 0.6638, 0.9229]])\n",
      "xyxy: tensor([[22.5273,  3.1438, 67.0000, 68.6728]])\n",
      "xyxyn: tensor([[0.3362, 0.0443, 1.0000, 0.9672]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2966])\n",
      "data: tensor([[ 0.8698,  2.0013, 45.4931, 64.7481,  0.2966,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 67)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.1814, 33.3747, 44.6234, 62.7469]])\n",
      "xywhn: tensor([[0.3460, 0.4701, 0.6660, 0.8838]])\n",
      "xyxy: tensor([[ 0.8698,  2.0013, 45.4931, 64.7481]])\n",
      "xyxyn: tensor([[0.0130, 0.0282, 0.6790, 0.9119]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2592])\n",
      "data: tensor([[ 0.3555,  5.6415, 34.7419, 63.7897,  0.2592,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 67)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[17.5487, 34.7156, 34.3864, 58.1482]])\n",
      "xywhn: tensor([[0.2619, 0.4890, 0.5132, 0.8190]])\n",
      "xyxy: tensor([[ 0.3555,  5.6415, 34.7419, 63.7897]])\n",
      "xyxyn: tensor([[0.0053, 0.0795, 0.5185, 0.8984]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3495])\n",
      "data: tensor([[ 19.1546,   0.0000, 100.4898, 106.4716,   0.3495,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (115, 103)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 59.8222,  53.2358,  81.3351, 106.4716]])\n",
      "xywhn: tensor([[0.5808, 0.4629, 0.7897, 0.9258]])\n",
      "xyxy: tensor([[ 19.1546,   0.0000, 100.4898, 106.4716]])\n",
      "xyxyn: tensor([[0.1860, 0.0000, 0.9756, 0.9258]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3273])\n",
      "data: tensor([[ 18.9955,   1.5635,  79.9492, 113.8965,   0.3273,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (115, 103)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 49.4724,  57.7300,  60.9537, 112.3330]])\n",
      "xywhn: tensor([[0.4803, 0.5020, 0.5918, 0.9768]])\n",
      "xyxy: tensor([[ 18.9955,   1.5635,  79.9492, 113.8965]])\n",
      "xyxyn: tensor([[0.1844, 0.0136, 0.7762, 0.9904]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 116.6ms\n",
      "Speed: 0.0ms preprocess, 116.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x608 3 -1s, 158.7ms\n",
      "Speed: 2.0ms preprocess, 158.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x608 1 -1, 144.0ms\n",
      "Speed: 8.6ms preprocess, 144.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4101])\n",
      "data: tensor([[24.9791,  2.8976, 65.0000, 68.8316,  0.4101,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[44.9895, 35.8646, 40.0209, 65.9340]])\n",
      "xywhn: tensor([[0.6921, 0.5051, 0.6157, 0.9286]])\n",
      "xyxy: tensor([[24.9791,  2.8976, 65.0000, 68.8316]])\n",
      "xyxyn: tensor([[0.3843, 0.0408, 1.0000, 0.9695]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3740])\n",
      "data: tensor([[ 0.0000,  0.5203, 46.6006, 68.2503,  0.3740,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.3003, 34.3853, 46.6006, 67.7300]])\n",
      "xywhn: tensor([[0.3585, 0.4843, 0.7169, 0.9539]])\n",
      "xyxy: tensor([[ 0.0000,  0.5203, 46.6006, 68.2503]])\n",
      "xyxyn: tensor([[0.0000, 0.0073, 0.7169, 0.9613]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3057])\n",
      "data: tensor([[ 1.7338,  3.9770, 34.6608, 67.9260,  0.3057,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[18.1973, 35.9515, 32.9270, 63.9490]])\n",
      "xywhn: tensor([[0.2800, 0.5064, 0.5066, 0.9007]])\n",
      "xyxy: tensor([[ 1.7338,  3.9770, 34.6608, 67.9260]])\n",
      "xyxyn: tensor([[0.0267, 0.0560, 0.5332, 0.9567]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3748])\n",
      "data: tensor([[ 20.4053,   1.9251,  80.8071, 113.8264,   0.3748,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (114, 103)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 50.6062,  57.8757,  60.4018, 111.9013]])\n",
      "xywhn: tensor([[0.4913, 0.5077, 0.5864, 0.9816]])\n",
      "xyxy: tensor([[ 20.4053,   1.9251,  80.8071, 113.8264]])\n",
      "xyxyn: tensor([[0.1981, 0.0169, 0.7845, 0.9985]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 2 mouses, 1 remote, 1 cell phone, 1 book, 96.2ms\n",
      "Speed: 0.0ms preprocess, 96.2ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x576 2 -1s, 148.9ms\n",
      "Speed: 2.7ms preprocess, 148.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x608 1 -1, 160.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4303])\n",
      "data: tensor([[ 0.1867,  0.0000, 48.0874, 64.8304,  0.4303,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 63)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[24.1370, 32.4152, 47.9008, 64.8304]])\n",
      "xywhn: tensor([[0.3831, 0.4566, 0.7603, 0.9131]])\n",
      "xyxy: tensor([[ 0.1867,  0.0000, 48.0874, 64.8304]])\n",
      "xyxyn: tensor([[0.0030, 0.0000, 0.7633, 0.9131]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3596])\n",
      "data: tensor([[26.8676,  2.8622, 63.0000, 68.9865,  0.3596,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 63)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[44.9338, 35.9243, 36.1324, 66.1243]])\n",
      "xywhn: tensor([[0.7132, 0.5060, 0.5735, 0.9313]])\n",
      "xyxy: tensor([[26.8676,  2.8622, 63.0000, 68.9865]])\n",
      "xyxyn: tensor([[0.4265, 0.0403, 1.0000, 0.9716]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3360])\n",
      "data: tensor([[ 15.8713,   5.9650,  77.9054, 112.2544,   0.3360,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (114, 103)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 46.8883,  59.1097,  62.0340, 106.2894]])\n",
      "xywhn: tensor([[0.4552, 0.5185, 0.6023, 0.9324]])\n",
      "xyxy: tensor([[ 15.8713,   5.9650,  77.9054, 112.2544]])\n",
      "xyxyn: tensor([[0.1541, 0.0523, 0.7564, 0.9847]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 8.5ms preprocess, 160.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 1 mouse, 1 remote, 1 cell phone, 1 book, 96.3ms\n",
      "Speed: 0.0ms preprocess, 96.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 1 -1, 163.7ms\n",
      "Speed: 6.3ms preprocess, 163.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x608 1 -1, 151.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4955])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.3033, 63.2256,  0.4955,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 62)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.6517, 31.6128, 47.3033, 63.2256]])\n",
      "xywhn: tensor([[0.3815, 0.4453, 0.7630, 0.8905]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.3033, 63.2256]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7630, 0.8905]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3224])\n",
      "data: tensor([[ 23.5007,   0.0000,  78.1303, 108.4225,   0.3224,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (114, 104)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 50.8155,  54.2112,  54.6296, 108.4225]])\n",
      "xywhn: tensor([[0.4886, 0.4755, 0.5253, 0.9511]])\n",
      "xyxy: tensor([[ 23.5007,   0.0000,  78.1303, 108.4225]])\n",
      "xyxyn: tensor([[0.2260, 0.0000, 0.7513, 0.9511]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 13.9ms preprocess, 151.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 remote, 1 cell phone, 1 book, 108.2ms\n",
      "Speed: 1.6ms preprocess, 108.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 3 -1s, 136.2ms\n",
      "Speed: 5.7ms preprocess, 136.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x608 1 -1, 157.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4028])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.2252, 64.6447,  0.4028,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 61)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.1126, 32.3223, 46.2252, 64.6447]])\n",
      "xywhn: tensor([[0.3789, 0.4552, 0.7578, 0.9105]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.2252, 64.6447]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7578, 0.9105]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3491])\n",
      "data: tensor([[28.3209,  4.2484, 61.0000, 68.5298,  0.3491,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 61)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[44.6605, 36.3891, 32.6791, 64.2814]])\n",
      "xywhn: tensor([[0.7321, 0.5125, 0.5357, 0.9054]])\n",
      "xyxy: tensor([[28.3209,  4.2484, 61.0000, 68.5298]])\n",
      "xyxyn: tensor([[0.4643, 0.0598, 1.0000, 0.9652]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2731])\n",
      "data: tensor([[ 1.6599,  3.3911, 33.9692, 67.7997,  0.2731,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 61)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[17.8145, 35.5954, 32.3093, 64.4086]])\n",
      "xywhn: tensor([[0.2920, 0.5013, 0.5297, 0.9072]])\n",
      "xyxy: tensor([[ 1.6599,  3.3911, 33.9692, 67.7997]])\n",
      "xyxyn: tensor([[0.0272, 0.0478, 0.5569, 0.9549]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3887])\n",
      "data: tensor([[ 25.9459,   0.0000,  78.6907, 109.6332,   0.3887,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (114, 105)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 52.3183,  54.8166,  52.7448, 109.6332]])\n",
      "xywhn: tensor([[0.4983, 0.4808, 0.5023, 0.9617]])\n",
      "xyxy: tensor([[ 25.9459,   0.0000,  78.6907, 109.6332]])\n",
      "xyxyn: tensor([[0.2471, 0.0000, 0.7494, 0.9617]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 10.0ms preprocess, 157.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 1 mouse, 1 remote, 1 cell phone, 1 book, 121.0ms\n",
      "Speed: 0.0ms preprocess, 121.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 2 -1s, 143.2ms\n",
      "Speed: 3.8ms preprocess, 143.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x608 1 -1, 148.0ms\n",
      "Speed: 4.2ms preprocess, 148.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4530])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.7894, 63.3332,  0.4530,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.8947, 31.6666, 47.7894, 63.3332]])\n",
      "xywhn: tensor([[0.3982, 0.4460, 0.7965, 0.8920]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.7894, 63.3332]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7965, 0.8920]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3100])\n",
      "data: tensor([[ 0.0000,  1.7778, 34.5730, 62.8406,  0.3100,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[17.2865, 32.3092, 34.5730, 61.0629]])\n",
      "xywhn: tensor([[0.2881, 0.4551, 0.5762, 0.8600]])\n",
      "xyxy: tensor([[ 0.0000,  1.7778, 34.5730, 62.8406]])\n",
      "xyxyn: tensor([[0.0000, 0.0250, 0.5762, 0.8851]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4451])\n",
      "data: tensor([[ 25.7381,   0.0000,  75.6963, 111.4973,   0.4451,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (115, 105)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 50.7172,  55.7486,  49.9582, 111.4973]])\n",
      "xywhn: tensor([[0.4830, 0.4848, 0.4758, 0.9695]])\n",
      "xyxy: tensor([[ 25.7381,   0.0000,  75.6963, 111.4973]])\n",
      "xyxyn: tensor([[0.2451, 0.0000, 0.7209, 0.9695]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 remote, 1 cell phone, 1 book, 114.1ms\n",
      "Speed: 0.0ms preprocess, 114.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 143.2ms\n",
      "Speed: 0.0ms preprocess, 143.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x608 1 -1, 160.2ms\n",
      "Speed: 16.5ms preprocess, 160.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4493])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.5738, 63.4898,  0.4493,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.2869, 31.7449, 46.5738, 63.4898]])\n",
      "xywhn: tensor([[0.3947, 0.4471, 0.7894, 0.8942]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.5738, 63.4898]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7894, 0.8942]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3093])\n",
      "data: tensor([[ 0.0000,  5.1010, 33.4719, 66.9813,  0.3093,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[16.7359, 36.0411, 33.4719, 61.8802]])\n",
      "xywhn: tensor([[0.2837, 0.5076, 0.5673, 0.8716]])\n",
      "xyxy: tensor([[ 0.0000,  5.1010, 33.4719, 66.9813]])\n",
      "xyxyn: tensor([[0.0000, 0.0718, 0.5673, 0.9434]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2999])\n",
      "data: tensor([[27.4451,  7.4007, 59.0000, 68.9989,  0.2999,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[43.2225, 38.1998, 31.5549, 61.5981]])\n",
      "xywhn: tensor([[0.7326, 0.5380, 0.5348, 0.8676]])\n",
      "xyxy: tensor([[27.4451,  7.4007, 59.0000, 68.9989]])\n",
      "xyxyn: tensor([[0.4652, 0.1042, 1.0000, 0.9718]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3844])\n",
      "data: tensor([[ 26.6346,   0.0000,  82.9554, 110.5724,   0.3844,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (116, 107)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 54.7950,  55.2862,  56.3208, 110.5724]])\n",
      "xywhn: tensor([[0.5121, 0.4766, 0.5264, 0.9532]])\n",
      "xyxy: tensor([[ 26.6346,   0.0000,  82.9554, 110.5724]])\n",
      "xyxyn: tensor([[0.2489, 0.0000, 0.7753, 0.9532]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 remote, 1 cell phone, 1 book, 97.3ms\n",
      "Speed: 0.0ms preprocess, 97.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 135.1ms\n",
      "Speed: 6.8ms preprocess, 135.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4152])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.7683, 63.5811,  0.4152,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.3841, 31.7906, 46.7683, 63.5811]])\n",
      "xywhn: tensor([[0.3963, 0.4478, 0.7927, 0.8955]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.7683, 63.5811]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7927, 0.8955]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3442])\n",
      "data: tensor([[27.7120,  8.6274, 59.0000, 68.8206,  0.3442,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[43.3560, 38.7240, 31.2880, 60.1931]])\n",
      "xywhn: tensor([[0.7348, 0.5454, 0.5303, 0.8478]])\n",
      "xyxy: tensor([[27.7120,  8.6274, 59.0000, 68.8206]])\n",
      "xyxyn: tensor([[0.4697, 0.1215, 1.0000, 0.9693]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2560])\n",
      "data: tensor([[ 0.0000,  4.2524, 34.2305, 66.2923,  0.2560,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[17.1152, 35.2724, 34.2305, 62.0399]])\n",
      "xywhn: tensor([[0.2901, 0.4968, 0.5802, 0.8738]])\n",
      "xyxy: tensor([[ 0.0000,  4.2524, 34.2305, 66.2923]])\n",
      "xyxyn: tensor([[0.0000, 0.0599, 0.5802, 0.9337]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 2 -1s, 162.8ms\n",
      "Speed: 3.2ms preprocess, 162.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 remote, 1 cell phone, 1 book, 103.0ms\n",
      "Speed: 3.6ms preprocess, 103.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3181])\n",
      "data: tensor([[ 22.2780,   0.0000, 108.0000, 109.7946,   0.3181,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (116, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 65.1390,  54.8973,  85.7220, 109.7946]])\n",
      "xywhn: tensor([[0.6031, 0.4733, 0.7937, 0.9465]])\n",
      "xyxy: tensor([[ 22.2780,   0.0000, 108.0000, 109.7946]])\n",
      "xyxyn: tensor([[0.2063, 0.0000, 1.0000, 0.9465]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2859])\n",
      "data: tensor([[ 26.5391,   0.0000,  81.5736, 109.8967,   0.2859,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (116, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 54.0564,  54.9483,  55.0345, 109.8967]])\n",
      "xywhn: tensor([[0.5005, 0.4737, 0.5096, 0.9474]])\n",
      "xyxy: tensor([[ 26.5391,   0.0000,  81.5736, 109.8967]])\n",
      "xyxyn: tensor([[0.2457, 0.0000, 0.7553, 0.9474]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 3 -1s, 146.0ms\n",
      "Speed: 0.9ms preprocess, 146.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x608 3 -1s, 158.1ms\n",
      "Speed: 4.1ms preprocess, 158.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3443])\n",
      "data: tensor([[24.0064,  5.0116, 59.0000, 68.8152,  0.3443,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.5032, 36.9134, 34.9936, 63.8037]])\n",
      "xywhn: tensor([[0.7034, 0.5199, 0.5931, 0.8986]])\n",
      "xyxy: tensor([[24.0064,  5.0116, 59.0000, 68.8152]])\n",
      "xyxyn: tensor([[0.4069, 0.0706, 1.0000, 0.9692]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3401])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.1573, 64.7108,  0.3401,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.5786, 32.3554, 47.1573, 64.7108]])\n",
      "xywhn: tensor([[0.3996, 0.4557, 0.7993, 0.9114]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.1573, 64.7108]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7993, 0.9114]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3061])\n",
      "data: tensor([[ 0.0000,  4.8961, 31.1726, 67.3299,  0.3061,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.5863, 36.1130, 31.1726, 62.4339]])\n",
      "xywhn: tensor([[0.2642, 0.5086, 0.5283, 0.8794]])\n",
      "xyxy: tensor([[ 0.0000,  4.8961, 31.1726, 67.3299]])\n",
      "xyxyn: tensor([[0.0000, 0.0690, 0.5283, 0.9483]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3051])\n",
      "data: tensor([[ 25.1773,   0.0000,  81.6791, 112.1209,   0.3051,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (118, 109)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 53.4282,  56.0605,  56.5019, 112.1209]])\n",
      "xywhn: tensor([[0.4902, 0.4751, 0.5184, 0.9502]])\n",
      "xyxy: tensor([[ 25.1773,   0.0000,  81.6791, 112.1209]])\n",
      "xyxyn: tensor([[0.2310, 0.0000, 0.7493, 0.9502]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2711])\n",
      "data: tensor([[ 17.9540,   0.0000, 107.9066, 113.9839,   0.2711,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (118, 109)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 62.9303,  56.9919,  89.9526, 113.9839]])\n",
      "xywhn: tensor([[0.5773, 0.4830, 0.8253, 0.9660]])\n",
      "xyxy: tensor([[ 17.9540,   0.0000, 107.9066, 113.9839]])\n",
      "xyxyn: tensor([[0.1647, 0.0000, 0.9900, 0.9660]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2695])\n",
      "data: tensor([[ 82.9396,  40.3857, 107.2821,  67.4286,   0.2695,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (118, 109)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[95.1109, 53.9071, 24.3425, 27.0429]])\n",
      "xywhn: tensor([[0.8726, 0.4568, 0.2233, 0.2292]])\n",
      "xyxy: tensor([[ 82.9396,  40.3857, 107.2821,  67.4286]])\n",
      "xyxyn: tensor([[0.7609, 0.3423, 0.9842, 0.5714]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 remote, 1 cell phone, 1 book, 110.4ms\n",
      "Speed: 0.0ms preprocess, 110.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 2 -1s, 137.4ms\n",
      "Speed: 3.4ms preprocess, 137.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x608 2 -1s, 157.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4152])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.7581, 64.1598,  0.4152,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.3791, 32.0799, 46.7581, 64.1598]])\n",
      "xywhn: tensor([[0.4031, 0.4518, 0.8062, 0.9037]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.7581, 64.1598]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8062, 0.9037]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2804])\n",
      "data: tensor([[ 0.0000,  0.8335, 33.9763, 65.9643,  0.2804,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[16.9882, 33.3989, 33.9763, 65.1308]])\n",
      "xywhn: tensor([[0.2929, 0.4704, 0.5858, 0.9173]])\n",
      "xyxy: tensor([[ 0.0000,  0.8335, 33.9763, 65.9643]])\n",
      "xyxyn: tensor([[0.0000, 0.0117, 0.5858, 0.9291]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3295])\n",
      "data: tensor([[ 17.7310,   0.3758, 108.8914, 114.0948,   0.3295,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (118, 111)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 63.3112,  57.2353,  91.1604, 113.7189]])\n",
      "xywhn: tensor([[0.5704, 0.4850, 0.8213, 0.9637]])\n",
      "xyxy: tensor([[ 17.7310,   0.3758, 108.8914, 114.0948]])\n",
      "xyxyn: tensor([[0.1597, 0.0032, 0.9810, 0.9669]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2867])\n",
      "data: tensor([[ 28.5751,   0.0000,  76.9854, 113.8503,   0.2867,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (118, 111)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 52.7803,  56.9252,  48.4103, 113.8503]])\n",
      "xywhn: tensor([[0.4755, 0.4824, 0.4361, 0.9648]])\n",
      "xyxy: tensor([[ 28.5751,   0.0000,  76.9854, 113.8503]])\n",
      "xyxyn: tensor([[0.2574, 0.0000, 0.6936, 0.9648]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 8.8ms preprocess, 157.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 remote, 1 cell phone, 1 book, 90.3ms\n",
      "Speed: 4.8ms preprocess, 90.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 2 -1s, 139.0ms\n",
      "Speed: 3.8ms preprocess, 139.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x608 2 -1s, 152.1ms\n",
      "Speed: 20.3ms preprocess, 152.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4121])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.7018, 64.1636,  0.4121,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.3509, 32.0818, 46.7018, 64.1636]])\n",
      "xywhn: tensor([[0.4026, 0.4519, 0.8052, 0.9037]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.7018, 64.1636]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8052, 0.9037]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2751])\n",
      "data: tensor([[ 0.0000,  0.9231, 33.9714, 66.1171,  0.2751,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[16.9857, 33.5201, 33.9714, 65.1941]])\n",
      "xywhn: tensor([[0.2929, 0.4721, 0.5857, 0.9182]])\n",
      "xyxy: tensor([[ 0.0000,  0.9231, 33.9714, 66.1171]])\n",
      "xyxyn: tensor([[0.0000, 0.0130, 0.5857, 0.9312]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3382])\n",
      "data: tensor([[1.9449e+01, 8.8656e-02, 1.1032e+02, 1.1564e+02, 3.3825e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 113)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 64.8848,  57.8620,  90.8716, 115.5467]])\n",
      "xywhn: tensor([[0.5742, 0.4862, 0.8042, 0.9710]])\n",
      "xyxy: tensor([[1.9449e+01, 8.8656e-02, 1.1032e+02, 1.1564e+02]])\n",
      "xyxyn: tensor([[1.7212e-01, 7.4501e-04, 9.7629e-01, 9.7173e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2533])\n",
      "data: tensor([[ 24.0472,   0.0000,  78.7169, 112.1609,   0.2533,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 113)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 51.3821,  56.0804,  54.6697, 112.1609]])\n",
      "xywhn: tensor([[0.4547, 0.4713, 0.4838, 0.9425]])\n",
      "xyxy: tensor([[ 24.0472,   0.0000,  78.7169, 112.1609]])\n",
      "xyxyn: tensor([[0.2128, 0.0000, 0.6966, 0.9425]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 remote, 1 book, 107.0ms\n",
      "Speed: 0.0ms preprocess, 107.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 140.3ms\n",
      "Speed: 4.3ms preprocess, 140.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x608 2 -1s, 157.0ms\n",
      "Speed: 8.1ms preprocess, 157.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3668])\n",
      "data: tensor([[ 0.0000,  5.9447, 30.9700, 67.3927,  0.3668,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.4850, 36.6687, 30.9700, 61.4480]])\n",
      "xywhn: tensor([[0.2625, 0.5165, 0.5249, 0.8655]])\n",
      "xyxy: tensor([[ 0.0000,  5.9447, 30.9700, 67.3927]])\n",
      "xyxyn: tensor([[0.0000, 0.0837, 0.5249, 0.9492]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3219])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.8747, 65.0340,  0.3219,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.4374, 32.5170, 46.8747, 65.0340]])\n",
      "xywhn: tensor([[0.3972, 0.4580, 0.7945, 0.9160]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.8747, 65.0340]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7945, 0.9160]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3206])\n",
      "data: tensor([[24.2825,  3.7885, 59.0000, 68.5830,  0.3206,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.6413, 36.1857, 34.7175, 64.7945]])\n",
      "xywhn: tensor([[0.7058, 0.5097, 0.5884, 0.9126]])\n",
      "xyxy: tensor([[24.2825,  3.7885, 59.0000, 68.5830]])\n",
      "xyxyn: tensor([[0.4116, 0.0534, 1.0000, 0.9660]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4844])\n",
      "data: tensor([[ 29.7752,   0.0000,  73.0381, 114.6741,   0.4844,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 114)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 51.4067,  57.3371,  43.2629, 114.6741]])\n",
      "xywhn: tensor([[0.4509, 0.4778, 0.3795, 0.9556]])\n",
      "xyxy: tensor([[ 29.7752,   0.0000,  73.0381, 114.6741]])\n",
      "xyxyn: tensor([[0.2612, 0.0000, 0.6407, 0.9556]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3574])\n",
      "data: tensor([[ 21.1988,   0.0000, 111.1381, 116.6745,   0.3574,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 114)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 66.1684,  58.3373,  89.9393, 116.6745]])\n",
      "xywhn: tensor([[0.5804, 0.4861, 0.7889, 0.9723]])\n",
      "xyxy: tensor([[ 21.1988,   0.0000, 111.1381, 116.6745]])\n",
      "xyxyn: tensor([[0.1860, 0.0000, 0.9749, 0.9723]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 remote, 1 book, 98.2ms\n",
      "Speed: 1.4ms preprocess, 98.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 126.1ms\n",
      "Speed: 1.5ms preprocess, 126.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 3 -1s, 139.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3554])\n",
      "data: tensor([[ 0.0000,  0.9590, 30.4953, 68.6360,  0.3554,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.2476, 34.7975, 30.4953, 67.6770]])\n",
      "xywhn: tensor([[0.2629, 0.4901, 0.5258, 0.9532]])\n",
      "xyxy: tensor([[ 0.0000,  0.9590, 30.4953, 68.6360]])\n",
      "xyxyn: tensor([[0.0000, 0.0135, 0.5258, 0.9667]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3034])\n",
      "data: tensor([[24.5289,  3.9654, 58.0000, 68.3521,  0.3034,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.2644, 36.1588, 33.4711, 64.3867]])\n",
      "xywhn: tensor([[0.7115, 0.5093, 0.5771, 0.9069]])\n",
      "xyxy: tensor([[24.5289,  3.9654, 58.0000, 68.3521]])\n",
      "xyxyn: tensor([[0.4229, 0.0559, 1.0000, 0.9627]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2993])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.5391, 66.1517,  0.2993,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.2695, 33.0759, 46.5391, 66.1517]])\n",
      "xywhn: tensor([[0.4012, 0.4659, 0.8024, 0.9317]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.5391, 66.1517]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8024, 0.9317]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4084])\n",
      "data: tensor([[  0.0000,   3.0009,  14.1066, 115.6188,   0.4084,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[  7.0533,  59.3098,  14.1066, 112.6179]])\n",
      "xywhn: tensor([[0.0608, 0.4902, 0.1216, 0.9307]])\n",
      "xyxy: tensor([[  0.0000,   3.0009,  14.1066, 115.6188]])\n",
      "xyxyn: tensor([[0.0000, 0.0248, 0.1216, 0.9555]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3588])\n",
      "data: tensor([[ 24.2558,   0.0000, 115.7996, 116.9282,   0.3588,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 70.0277,  58.4641,  91.5438, 116.9282]])\n",
      "xywhn: tensor([[0.6037, 0.4832, 0.7892, 0.9663]])\n",
      "xyxy: tensor([[ 24.2558,   0.0000, 115.7996, 116.9282]])\n",
      "xyxyn: tensor([[0.2091, 0.0000, 0.9983, 0.9663]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3542])\n",
      "data: tensor([[ 29.3831,   0.0000,  75.2574, 117.5353,   0.3542,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 52.3202,  58.7676,  45.8742, 117.5353]])\n",
      "xywhn: tensor([[0.4510, 0.4857, 0.3955, 0.9714]])\n",
      "xyxy: tensor([[ 29.3831,   0.0000,  75.2574, 117.5353]])\n",
      "xyxyn: tensor([[0.2533, 0.0000, 0.6488, 0.9714]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 10.8ms preprocess, 139.8ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 1 mouse, 1 remote, 1 cell phone, 1 book, 108.6ms\n",
      "Speed: 1.7ms preprocess, 108.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 3 -1s, 134.7ms\n",
      "Speed: 4.5ms preprocess, 134.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 2 -1s, 158.3ms\n",
      "Speed: 3.7ms preprocess, 158.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3533])\n",
      "data: tensor([[ 0.0000,  0.9139, 30.4804, 68.6256,  0.3533,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.2402, 34.7697, 30.4804, 67.7117]])\n",
      "xywhn: tensor([[0.2628, 0.4897, 0.5255, 0.9537]])\n",
      "xyxy: tensor([[ 0.0000,  0.9139, 30.4804, 68.6256]])\n",
      "xyxyn: tensor([[0.0000, 0.0129, 0.5255, 0.9666]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3149])\n",
      "data: tensor([[24.5923,  3.6695, 58.0000, 68.4075,  0.3149,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.2961, 36.0385, 33.4077, 64.7380]])\n",
      "xywhn: tensor([[0.7120, 0.5076, 0.5760, 0.9118]])\n",
      "xyxy: tensor([[24.5923,  3.6695, 58.0000, 68.4075]])\n",
      "xyxyn: tensor([[0.4240, 0.0517, 1.0000, 0.9635]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2906])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.4937, 66.2596,  0.2906,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.2469, 33.1298, 46.4937, 66.2596]])\n",
      "xywhn: tensor([[0.4008, 0.4666, 0.8016, 0.9332]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.4937, 66.2596]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8016, 0.9332]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3412])\n",
      "data: tensor([[ 24.2821,   1.1849, 113.3584, 120.1610,   0.3412,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 68.8202,  60.6730,  89.0763, 118.9761]])\n",
      "xywhn: tensor([[0.5832, 0.4973, 0.7549, 0.9752]])\n",
      "xyxy: tensor([[ 24.2821,   1.1849, 113.3584, 120.1610]])\n",
      "xyxyn: tensor([[0.2058, 0.0097, 0.9607, 0.9849]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2623])\n",
      "data: tensor([[  0.0000,   3.0778,  14.9612, 118.7705,   0.2623,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[  7.4806,  60.9241,  14.9612, 115.6926]])\n",
      "xywhn: tensor([[0.0634, 0.4994, 0.1268, 0.9483]])\n",
      "xyxy: tensor([[  0.0000,   3.0778,  14.9612, 118.7705]])\n",
      "xyxyn: tensor([[0.0000, 0.0252, 0.1268, 0.9735]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 1 mouse, 1 remote, 1 book, 110.1ms\n",
      "Speed: 3.5ms preprocess, 110.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 147.5ms\n",
      "Speed: 2.6ms preprocess, 147.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 2 -1s, 138.8ms\n",
      "Speed: 4.0ms preprocess, 138.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3614])\n",
      "data: tensor([[ 0.0000,  0.8811, 30.7174, 68.5551,  0.3614,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.3587, 34.7181, 30.7174, 67.6739]])\n",
      "xywhn: tensor([[0.2648, 0.4890, 0.5296, 0.9532]])\n",
      "xyxy: tensor([[ 0.0000,  0.8811, 30.7174, 68.5551]])\n",
      "xyxyn: tensor([[0.0000, 0.0124, 0.5296, 0.9656]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3103])\n",
      "data: tensor([[24.0910,  4.0176, 58.0000, 68.2552,  0.3103,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.0455, 36.1364, 33.9090, 64.2376]])\n",
      "xywhn: tensor([[0.7077, 0.5090, 0.5846, 0.9048]])\n",
      "xyxy: tensor([[24.0910,  4.0176, 58.0000, 68.2552]])\n",
      "xyxyn: tensor([[0.4154, 0.0566, 1.0000, 0.9613]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2788])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.4290, 66.2256,  0.2788,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.2145, 33.1128, 46.4290, 66.2256]])\n",
      "xywhn: tensor([[0.4003, 0.4664, 0.8005, 0.9328]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.4290, 66.2256]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8005, 0.9328]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4427])\n",
      "data: tensor([[ 24.8000,   5.4530, 114.4176, 121.0036,   0.4427,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.6088,  63.2283,  89.6176, 115.5506]])\n",
      "xywhn: tensor([[0.5899, 0.5141, 0.7595, 0.9394]])\n",
      "xyxy: tensor([[ 24.8000,   5.4530, 114.4176, 121.0036]])\n",
      "xyxyn: tensor([[0.2102, 0.0443, 0.9696, 0.9838]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3366])\n",
      "data: tensor([[  0.0000,   3.2033,  14.6784, 121.0437,   0.3366,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[  7.3392,  62.1235,  14.6784, 117.8404]])\n",
      "xywhn: tensor([[0.0622, 0.5051, 0.1244, 0.9581]])\n",
      "xyxy: tensor([[  0.0000,   3.2033,  14.6784, 121.0437]])\n",
      "xyxyn: tensor([[0.0000, 0.0260, 0.1244, 0.9841]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 remote, 1 book, 95.7ms\n",
      "Speed: 0.0ms preprocess, 95.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 139.1ms\n",
      "Speed: 3.6ms preprocess, 139.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 3 -1s, 139.2ms\n",
      "Speed: 16.2ms preprocess, 139.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3988])\n",
      "data: tensor([[ 0.0000,  0.9038, 30.6314, 68.3719,  0.3988,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.3157, 34.6378, 30.6314, 67.4681]])\n",
      "xywhn: tensor([[0.2641, 0.4879, 0.5281, 0.9503]])\n",
      "xyxy: tensor([[ 0.0000,  0.9038, 30.6314, 68.3719]])\n",
      "xyxyn: tensor([[0.0000, 0.0127, 0.5281, 0.9630]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3380])\n",
      "data: tensor([[24.6361,  4.0578, 58.0000, 68.3275,  0.3380,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.3181, 36.1926, 33.3639, 64.2697]])\n",
      "xywhn: tensor([[0.7124, 0.5098, 0.5752, 0.9052]])\n",
      "xyxy: tensor([[24.6361,  4.0578, 58.0000, 68.3275]])\n",
      "xyxyn: tensor([[0.4248, 0.0572, 1.0000, 0.9624]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2678])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.3078, 66.2796,  0.2678,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.1539, 33.1398, 46.3078, 66.2796]])\n",
      "xywhn: tensor([[0.3992, 0.4668, 0.7984, 0.9335]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.3078, 66.2796]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7984, 0.9335]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4273])\n",
      "data: tensor([[ 27.7505,   0.0000, 118.6493, 108.8156,   0.4273,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 119)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.1999,  54.4078,  90.8989, 108.8156]])\n",
      "xywhn: tensor([[0.6151, 0.4423, 0.7639, 0.8847]])\n",
      "xyxy: tensor([[ 27.7505,   0.0000, 118.6493, 108.8156]])\n",
      "xyxyn: tensor([[0.2332, 0.0000, 0.9971, 0.8847]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2763])\n",
      "data: tensor([[  0.0000,   2.6214,  16.2744, 121.2304,   0.2763,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 119)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[  8.1372,  61.9259,  16.2744, 118.6091]])\n",
      "xywhn: tensor([[0.0684, 0.5035, 0.1368, 0.9643]])\n",
      "xyxy: tensor([[  0.0000,   2.6214,  16.2744, 121.2304]])\n",
      "xyxyn: tensor([[0.0000, 0.0213, 0.1368, 0.9856]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2716])\n",
      "data: tensor([[ 32.9935,   0.0000,  78.0578, 116.9297,   0.2716,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 119)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 55.5257,  58.4648,  45.0643, 116.9297]])\n",
      "xywhn: tensor([[0.4666, 0.4753, 0.3787, 0.9506]])\n",
      "xyxy: tensor([[ 32.9935,   0.0000,  78.0578, 116.9297]])\n",
      "xyxyn: tensor([[0.2773, 0.0000, 0.6559, 0.9506]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 4 laptops, 1 mouse, 1 remote, 1 cell phone, 1 book, 97.1ms\n",
      "Speed: 2.9ms preprocess, 97.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 140.3ms\n",
      "Speed: 0.0ms preprocess, 140.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 154.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3885])\n",
      "data: tensor([[ 0.0000,  0.8587, 30.5867, 68.5836,  0.3885,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.2933, 34.7212, 30.5867, 67.7249]])\n",
      "xywhn: tensor([[0.2637, 0.4890, 0.5274, 0.9539]])\n",
      "xyxy: tensor([[ 0.0000,  0.8587, 30.5867, 68.5836]])\n",
      "xyxyn: tensor([[0.0000, 0.0121, 0.5274, 0.9660]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3497])\n",
      "data: tensor([[25.1011,  4.5065, 58.0000, 68.4733,  0.3497,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.5506, 36.4899, 32.8989, 63.9669]])\n",
      "xywhn: tensor([[0.7164, 0.5139, 0.5672, 0.9009]])\n",
      "xyxy: tensor([[25.1011,  4.5065, 58.0000, 68.4733]])\n",
      "xyxyn: tensor([[0.4328, 0.0635, 1.0000, 0.9644]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2772])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.5575, 66.4889,  0.2772,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.2787, 33.2444, 46.5575, 66.4889]])\n",
      "xywhn: tensor([[0.4014, 0.4682, 0.8027, 0.9365]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.5575, 66.4889]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8027, 0.9365]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3445])\n",
      "data: tensor([[ 25.4823,   3.6562, 119.2492, 117.4234,   0.3445,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.3657,  60.5398,  93.7669, 113.7673]])\n",
      "xywhn: tensor([[0.6030, 0.4922, 0.7814, 0.9249]])\n",
      "xyxy: tensor([[ 25.4823,   3.6562, 119.2492, 117.4234]])\n",
      "xyxyn: tensor([[0.2124, 0.0297, 0.9937, 0.9547]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 5.5ms preprocess, 154.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 1 mouse, 1 cell phone, 1 book, 110.8ms\n",
      "Speed: 0.0ms preprocess, 110.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 3 -1s, 126.5ms\n",
      "Speed: 5.1ms preprocess, 126.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 2 -1s, 165.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3956])\n",
      "data: tensor([[ 0.0000,  0.9013, 30.6195, 68.6916,  0.3956,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.3098, 34.7965, 30.6195, 67.7903]])\n",
      "xywhn: tensor([[0.2640, 0.4901, 0.5279, 0.9548]])\n",
      "xyxy: tensor([[ 0.0000,  0.9013, 30.6195, 68.6916]])\n",
      "xyxyn: tensor([[0.0000, 0.0127, 0.5279, 0.9675]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3589])\n",
      "data: tensor([[24.6197,  5.8820, 58.0000, 68.4763,  0.3589,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.3099, 37.1791, 33.3803, 62.5943]])\n",
      "xywhn: tensor([[0.7122, 0.5236, 0.5755, 0.8816]])\n",
      "xyxy: tensor([[24.6197,  5.8820, 58.0000, 68.4763]])\n",
      "xyxyn: tensor([[0.4245, 0.0828, 1.0000, 0.9645]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2863])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.0862, 66.6734,  0.2863,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.0431, 33.3367, 46.0862, 66.6734]])\n",
      "xywhn: tensor([[0.3973, 0.4695, 0.7946, 0.9391]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.0862, 66.6734]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7946, 0.9391]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3434])\n",
      "data: tensor([[ 27.3868,   5.7478, 118.2090, 118.4894,   0.3434,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.7979,  62.1186,  90.8222, 112.7416]])\n",
      "xywhn: tensor([[0.6016, 0.5050, 0.7506, 0.9166]])\n",
      "xyxy: tensor([[ 27.3868,   5.7478, 118.2090, 118.4894]])\n",
      "xyxyn: tensor([[0.2263, 0.0467, 0.9769, 0.9633]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3093])\n",
      "data: tensor([[  0.0000,   2.6219,  17.4962, 120.9314,   0.3093,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[  8.7481,  61.7766,  17.4962, 118.3095]])\n",
      "xywhn: tensor([[0.0723, 0.5022, 0.1446, 0.9619]])\n",
      "xyxy: tensor([[  0.0000,   2.6219,  17.4962, 120.9314]])\n",
      "xyxyn: tensor([[0.0000, 0.0213, 0.1446, 0.9832]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 10.0ms preprocess, 165.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 cell phone, 1 book, 93.6ms\n",
      "Speed: 0.0ms preprocess, 93.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 2 -1s, 125.6ms\n",
      "Speed: 9.4ms preprocess, 125.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 2 -1s, 156.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3886])\n",
      "data: tensor([[ 0.0000,  0.9139, 30.7162, 68.8312,  0.3886,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.3581, 34.8726, 30.7162, 67.9173]])\n",
      "xywhn: tensor([[0.2648, 0.4912, 0.5296, 0.9566]])\n",
      "xyxy: tensor([[ 0.0000,  0.9139, 30.7162, 68.8312]])\n",
      "xyxyn: tensor([[0.0000, 0.0129, 0.5296, 0.9695]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3573])\n",
      "data: tensor([[24.6456,  6.0152, 58.0000, 68.5964,  0.3573,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.3228, 37.3058, 33.3544, 62.5811]])\n",
      "xywhn: tensor([[0.7125, 0.5254, 0.5751, 0.8814]])\n",
      "xyxy: tensor([[24.6456,  6.0152, 58.0000, 68.5964]])\n",
      "xyxyn: tensor([[0.4249, 0.0847, 1.0000, 0.9661]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3783])\n",
      "data: tensor([[ 27.1227,   5.0226, 120.4518, 118.1247,   0.3783,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.7873,  61.5737,  93.3291, 113.1021]])\n",
      "xywhn: tensor([[0.6098, 0.5006, 0.7713, 0.9195]])\n",
      "xyxy: tensor([[ 27.1227,   5.0226, 120.4518, 118.1247]])\n",
      "xyxyn: tensor([[0.2242, 0.0408, 0.9955, 0.9604]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2677])\n",
      "data: tensor([[  0.0000,   2.8141,  17.5206, 121.7444,   0.2677,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[  8.7603,  62.2793,  17.5206, 118.9303]])\n",
      "xywhn: tensor([[0.0724, 0.5063, 0.1448, 0.9669]])\n",
      "xyxy: tensor([[  0.0000,   2.8141,  17.5206, 121.7444]])\n",
      "xyxyn: tensor([[0.0000, 0.0229, 0.1448, 0.9898]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.8ms preprocess, 156.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 cell phone, 1 book, 102.5ms\n",
      "Speed: 3.5ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 3 -1s, 123.0ms\n",
      "Speed: 3.1ms preprocess, 123.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 2 -1s, 140.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3525])\n",
      "data: tensor([[ 0.0000,  0.8877, 30.7348, 68.8598,  0.3525,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.3674, 34.8737, 30.7348, 67.9722]])\n",
      "xywhn: tensor([[0.2650, 0.4912, 0.5299, 0.9574]])\n",
      "xyxy: tensor([[ 0.0000,  0.8877, 30.7348, 68.8598]])\n",
      "xyxyn: tensor([[0.0000, 0.0125, 0.5299, 0.9699]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2903])\n",
      "data: tensor([[24.4833,  5.1982, 58.0000, 68.4893,  0.2903,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.2417, 36.8437, 33.5167, 63.2911]])\n",
      "xywhn: tensor([[0.7111, 0.5189, 0.5779, 0.8914]])\n",
      "xyxy: tensor([[24.4833,  5.1982, 58.0000, 68.4893]])\n",
      "xyxyn: tensor([[0.4221, 0.0732, 1.0000, 0.9646]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2684])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.8819, 65.8514,  0.2684,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.4409, 32.9257, 46.8819, 65.8514]])\n",
      "xywhn: tensor([[0.4042, 0.4637, 0.8083, 0.9275]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.8819, 65.8514]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8083, 0.9275]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3851])\n",
      "data: tensor([[ 27.5628,   5.1706, 117.3073, 117.9911,   0.3851,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.4350,  61.5809,  89.7445, 112.8205]])\n",
      "xywhn: tensor([[0.6036, 0.5007, 0.7479, 0.9172]])\n",
      "xyxy: tensor([[ 27.5628,   5.1706, 117.3073, 117.9911]])\n",
      "xyxyn: tensor([[0.2297, 0.0420, 0.9776, 0.9593]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2500])\n",
      "data: tensor([[  0.0000,   3.2194,  15.4202, 121.2486,   0.2500,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[  7.7101,  62.2340,  15.4202, 118.0292]])\n",
      "xywhn: tensor([[0.0643, 0.5060, 0.1285, 0.9596]])\n",
      "xyxy: tensor([[  0.0000,   3.2194,  15.4202, 121.2486]])\n",
      "xyxyn: tensor([[0.0000, 0.0262, 0.1285, 0.9858]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 12.8ms preprocess, 140.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 1 mouse, 1 cell phone, 1 book, 90.5ms\n",
      "Speed: 3.1ms preprocess, 90.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 2 -1s, 121.9ms\n",
      "Speed: 2.2ms preprocess, 121.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 2 -1s, 142.9ms\n",
      "Speed: 14.1ms preprocess, 142.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3572])\n",
      "data: tensor([[ 0.0000,  0.9329, 30.7523, 69.1716,  0.3572,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.3762, 35.0523, 30.7523, 68.2387]])\n",
      "xywhn: tensor([[0.2651, 0.4937, 0.5302, 0.9611]])\n",
      "xyxy: tensor([[ 0.0000,  0.9329, 30.7523, 69.1716]])\n",
      "xyxyn: tensor([[0.0000, 0.0131, 0.5302, 0.9742]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3259])\n",
      "data: tensor([[22.7543,  5.5160, 58.0000, 68.9160,  0.3259,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[40.3771, 37.2160, 35.2457, 63.3999]])\n",
      "xywhn: tensor([[0.6962, 0.5242, 0.6077, 0.8930]])\n",
      "xyxy: tensor([[22.7543,  5.5160, 58.0000, 68.9160]])\n",
      "xyxyn: tensor([[0.3923, 0.0777, 1.0000, 0.9706]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4103])\n",
      "data: tensor([[ 27.8117,   0.9225, 113.7868, 117.3484,   0.4103,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 70.7993,  59.1355,  85.9751, 116.4258]])\n",
      "xywhn: tensor([[0.5900, 0.4808, 0.7165, 0.9466]])\n",
      "xyxy: tensor([[ 27.8117,   0.9225, 113.7868, 117.3484]])\n",
      "xyxyn: tensor([[0.2318, 0.0075, 0.9482, 0.9541]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2644])\n",
      "data: tensor([[  0.0000,   3.1237,  15.4350, 120.2647,   0.2644,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[  7.7175,  61.6942,  15.4350, 117.1410]])\n",
      "xywhn: tensor([[0.0643, 0.5016, 0.1286, 0.9524]])\n",
      "xyxy: tensor([[  0.0000,   3.1237,  15.4350, 120.2647]])\n",
      "xyxyn: tensor([[0.0000, 0.0254, 0.1286, 0.9778]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 cell phone, 1 book, 96.3ms\n",
      "Speed: 0.0ms preprocess, 96.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 129.1ms\n",
      "Speed: 6.3ms preprocess, 129.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 161.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3251])\n",
      "data: tensor([[20.4053,  6.4769, 58.0000, 68.8797,  0.3251,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[39.2027, 37.6783, 37.5947, 62.4028]])\n",
      "xywhn: tensor([[0.6759, 0.5307, 0.6482, 0.8789]])\n",
      "xyxy: tensor([[20.4053,  6.4769, 58.0000, 68.8797]])\n",
      "xyxyn: tensor([[0.3518, 0.0912, 1.0000, 0.9701]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3169])\n",
      "data: tensor([[ 0.0000,  0.9741, 30.8080, 69.1742,  0.3169,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.4040, 35.0741, 30.8080, 68.2001]])\n",
      "xywhn: tensor([[0.2656, 0.4940, 0.5312, 0.9606]])\n",
      "xyxy: tensor([[ 0.0000,  0.9741, 30.8080, 69.1742]])\n",
      "xyxyn: tensor([[0.0000, 0.0137, 0.5312, 0.9743]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2610])\n",
      "data: tensor([[ 0.0000,  0.6493, 46.5489, 68.9651,  0.2610,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.2744, 34.8072, 46.5489, 68.3158]])\n",
      "xywhn: tensor([[0.4013, 0.4902, 0.8026, 0.9622]])\n",
      "xyxy: tensor([[ 0.0000,  0.6493, 46.5489, 68.9651]])\n",
      "xyxyn: tensor([[0.0000, 0.0091, 0.8026, 0.9713]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4307])\n",
      "data: tensor([[ 28.0273,   0.0000, 119.6960, 109.0816,   0.4307,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.8616,  54.5408,  91.6687, 109.0816]])\n",
      "xywhn: tensor([[0.6104, 0.4434, 0.7576, 0.8868]])\n",
      "xyxy: tensor([[ 28.0273,   0.0000, 119.6960, 109.0816]])\n",
      "xyxyn: tensor([[0.2316, 0.0000, 0.9892, 0.8868]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 8.4ms preprocess, 161.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 1 mouse, 1 cell phone, 1 book, 110.0ms\n",
      "Speed: 1.5ms preprocess, 110.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 2 -1s, 119.7ms\n",
      "Speed: 5.2ms preprocess, 119.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 150.2ms\n",
      "Speed: 10.3ms preprocess, 150.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3178])\n",
      "data: tensor([[19.2442,  6.8402, 58.0000, 68.9813,  0.3178,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[38.6221, 37.9107, 38.7558, 62.1411]])\n",
      "xywhn: tensor([[0.6659, 0.5340, 0.6682, 0.8752]])\n",
      "xyxy: tensor([[19.2442,  6.8402, 58.0000, 68.9813]])\n",
      "xyxyn: tensor([[0.3318, 0.0963, 1.0000, 0.9716]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2968])\n",
      "data: tensor([[ 0.0000,  0.9979, 30.7372, 69.3064,  0.2968,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.3686, 35.1521, 30.7372, 68.3084]])\n",
      "xywhn: tensor([[0.2650, 0.4951, 0.5300, 0.9621]])\n",
      "xyxy: tensor([[ 0.0000,  0.9979, 30.7372, 69.3064]])\n",
      "xyxyn: tensor([[0.0000, 0.0141, 0.5300, 0.9761]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3914])\n",
      "data: tensor([[ 25.1918,   0.7255, 120.1415, 114.6727,   0.3914,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (124, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.6666,  57.6991,  94.9497, 113.9472]])\n",
      "xywhn: tensor([[0.6006, 0.4653, 0.7847, 0.9189]])\n",
      "xyxy: tensor([[ 25.1918,   0.7255, 120.1415, 114.6727]])\n",
      "xyxyn: tensor([[0.2082, 0.0059, 0.9929, 0.9248]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 cell phone, 1 book, 110.2ms\n",
      "Speed: 3.0ms preprocess, 110.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 123.7ms\n",
      "Speed: 18.2ms preprocess, 123.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 140.7ms\n",
      "Speed: 14.1ms preprocess, 140.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3602])\n",
      "data: tensor([[19.9494,  6.7690, 58.0000, 69.1143,  0.3602,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[38.9747, 37.9417, 38.0506, 62.3453]])\n",
      "xywhn: tensor([[0.6720, 0.5344, 0.6560, 0.8781]])\n",
      "xyxy: tensor([[19.9494,  6.7690, 58.0000, 69.1143]])\n",
      "xyxyn: tensor([[0.3440, 0.0953, 1.0000, 0.9734]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3171])\n",
      "data: tensor([[ 0.0000,  1.0221, 30.5200, 69.5088,  0.3171,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.2600, 35.2654, 30.5200, 68.4867]])\n",
      "xywhn: tensor([[0.2631, 0.4967, 0.5262, 0.9646]])\n",
      "xyxy: tensor([[ 0.0000,  1.0221, 30.5200, 69.5088]])\n",
      "xyxyn: tensor([[0.0000, 0.0144, 0.5262, 0.9790]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2696])\n",
      "data: tensor([[ 0.0000,  0.7084, 46.0987, 69.3820,  0.2696,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.0493, 35.0452, 46.0987, 68.6736]])\n",
      "xywhn: tensor([[0.3974, 0.4936, 0.7948, 0.9672]])\n",
      "xyxy: tensor([[ 0.0000,  0.7084, 46.0987, 69.3820]])\n",
      "xyxyn: tensor([[0.0000, 0.0100, 0.7948, 0.9772]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3617])\n",
      "data: tensor([[ 29.8882,   0.0000, 119.7993, 112.7638,   0.3617,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.8438,  56.3819,  89.9111, 112.7638]])\n",
      "xywhn: tensor([[0.6185, 0.4584, 0.7431, 0.9168]])\n",
      "xyxy: tensor([[ 29.8882,   0.0000, 119.7993, 112.7638]])\n",
      "xyxyn: tensor([[0.2470, 0.0000, 0.9901, 0.9168]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 cell phone, 1 book, 96.6ms\n",
      "Speed: 0.0ms preprocess, 96.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 135.3ms\n",
      "Speed: 3.4ms preprocess, 135.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 157.4ms\n",
      "Speed: 8.1ms preprocess, 157.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3433])\n",
      "data: tensor([[19.8922,  5.9172, 58.0000, 69.1076,  0.3433,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[38.9461, 37.5124, 38.1078, 63.1904]])\n",
      "xywhn: tensor([[0.6715, 0.5283, 0.6570, 0.8900]])\n",
      "xyxy: tensor([[19.8922,  5.9172, 58.0000, 69.1076]])\n",
      "xyxyn: tensor([[0.3430, 0.0833, 1.0000, 0.9733]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2993])\n",
      "data: tensor([[ 0.0000,  1.0379, 30.5733, 69.5161,  0.2993,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.2866, 35.2770, 30.5733, 68.4782]])\n",
      "xywhn: tensor([[0.2636, 0.4969, 0.5271, 0.9645]])\n",
      "xyxy: tensor([[ 0.0000,  1.0379, 30.5733, 69.5161]])\n",
      "xyxyn: tensor([[0.0000, 0.0146, 0.5271, 0.9791]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2991])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.3626, 65.6592,  0.2991,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.1813, 32.8296, 46.3626, 65.6592]])\n",
      "xywhn: tensor([[0.3997, 0.4624, 0.7994, 0.9248]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.3626, 65.6592]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7994, 0.9248]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4070])\n",
      "data: tensor([[ 27.1796,   0.6355, 119.0168, 118.7749,   0.4070,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.0982,  59.7052,  91.8371, 118.1394]])\n",
      "xywhn: tensor([[0.6041, 0.4894, 0.7590, 0.9684]])\n",
      "xyxy: tensor([[ 27.1796,   0.6355, 119.0168, 118.7749]])\n",
      "xyxyn: tensor([[0.2246, 0.0052, 0.9836, 0.9736]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 3 laptops, 1 mouse, 1 cell phone, 1 book, 106.3ms\n",
      "Speed: 4.1ms preprocess, 106.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 127.1ms\n",
      "Speed: 12.0ms preprocess, 127.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 157.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4103])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.6372, 62.8891,  0.4103,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.3186, 31.4446, 46.6372, 62.8891]])\n",
      "xywhn: tensor([[0.4020, 0.4429, 0.8041, 0.8858]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.6372, 62.8891]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8041, 0.8858]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3030])\n",
      "data: tensor([[25.0754,  5.1762, 58.0000, 68.7503,  0.3030,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.5377, 36.9632, 32.9246, 63.5741]])\n",
      "xywhn: tensor([[0.7162, 0.5206, 0.5677, 0.8954]])\n",
      "xyxy: tensor([[25.0754,  5.1762, 58.0000, 68.7503]])\n",
      "xyxyn: tensor([[0.4323, 0.0729, 1.0000, 0.9683]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3024])\n",
      "data: tensor([[ 0.0000,  1.0849, 30.2290, 69.6071,  0.3024,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.1145, 35.3460, 30.2290, 68.5221]])\n",
      "xywhn: tensor([[0.2606, 0.4978, 0.5212, 0.9651]])\n",
      "xyxy: tensor([[ 0.0000,  1.0849, 30.2290, 69.6071]])\n",
      "xyxyn: tensor([[0.0000, 0.0153, 0.5212, 0.9804]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3743])\n",
      "data: tensor([[ 30.1900,   0.0000, 120.8221, 110.3477,   0.3743,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.5061,  55.1739,  90.6321, 110.3477]])\n",
      "xywhn: tensor([[0.6139, 0.4486, 0.7368, 0.8971]])\n",
      "xyxy: tensor([[ 30.1900,   0.0000, 120.8221, 110.3477]])\n",
      "xyxyn: tensor([[0.2454, 0.0000, 0.9823, 0.8971]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.3ms preprocess, 157.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 cell phone, 1 book, 96.6ms\n",
      "Speed: 0.0ms preprocess, 96.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 3 -1s, 143.6ms\n",
      "Speed: 0.0ms preprocess, 143.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 134.7ms\n",
      "Speed: 7.8ms preprocess, 134.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4337])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.6339, 62.9489,  0.4337,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.3170, 31.4745, 46.6339, 62.9489]])\n",
      "xywhn: tensor([[0.4020, 0.4433, 0.8040, 0.8866]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.6339, 62.9489]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8040, 0.8866]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2898])\n",
      "data: tensor([[ 0.0000,  1.1344, 30.1571, 69.1107,  0.2898,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.0786, 35.1226, 30.1571, 67.9763]])\n",
      "xywhn: tensor([[0.2600, 0.4947, 0.5200, 0.9574]])\n",
      "xyxy: tensor([[ 0.0000,  1.1344, 30.1571, 69.1107]])\n",
      "xyxyn: tensor([[0.0000, 0.0160, 0.5200, 0.9734]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2607])\n",
      "data: tensor([[24.8826,  1.2220, 58.0000, 69.9084,  0.2607,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (71, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.4413, 35.5652, 33.1174, 68.6864]])\n",
      "xywhn: tensor([[0.7145, 0.5009, 0.5710, 0.9674]])\n",
      "xyxy: tensor([[24.8826,  1.2220, 58.0000, 69.9084]])\n",
      "xyxyn: tensor([[0.4290, 0.0172, 1.0000, 0.9846]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4135])\n",
      "data: tensor([[ 31.6787,   0.0000, 121.3172, 110.2542,   0.4135,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (124, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.4979,  55.1271,  89.6385, 110.2542]])\n",
      "xywhn: tensor([[0.6219, 0.4446, 0.7288, 0.8891]])\n",
      "xyxy: tensor([[ 31.6787,   0.0000, 121.3172, 110.2542]])\n",
      "xyxyn: tensor([[0.2576, 0.0000, 0.9863, 0.8891]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 book, 94.5ms\n",
      "Speed: 0.0ms preprocess, 94.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 1 -1, 121.3ms\n",
      "Speed: 3.0ms preprocess, 121.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 152.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4886])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.1343, 64.1480,  0.4886,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.0672, 32.0740, 46.1343, 64.1480]])\n",
      "xywhn: tensor([[0.3977, 0.4455, 0.7954, 0.8909]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.1343, 64.1480]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7954, 0.8909]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4011])\n",
      "data: tensor([[ 29.9088,   0.0000, 121.3191, 110.3562,   0.4011,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (124, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.6139,  55.1781,  91.4103, 110.3562]])\n",
      "xywhn: tensor([[0.6147, 0.4450, 0.7432, 0.8900]])\n",
      "xyxy: tensor([[ 29.9088,   0.0000, 121.3191, 110.3562]])\n",
      "xyxyn: tensor([[0.2432, 0.0000, 0.9863, 0.8900]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 8.2ms preprocess, 152.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 book, 109.5ms\n",
      "Speed: 0.0ms preprocess, 109.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 1 -1, 126.5ms\n",
      "Speed: 4.0ms preprocess, 126.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 146.1ms\n",
      "Speed: 8.6ms preprocess, 146.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4660])\n",
      "data: tensor([[ 0.0000,  0.0000, 45.9612, 63.9497,  0.4660,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.9806, 31.9749, 45.9612, 63.9497]])\n",
      "xywhn: tensor([[0.3962, 0.4441, 0.7924, 0.8882]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 45.9612, 63.9497]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7924, 0.8882]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3561])\n",
      "data: tensor([[ 27.4711,   0.0000, 120.9829, 109.1241,   0.3561,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.2270,  54.5620,  93.5118, 109.1241]])\n",
      "xywhn: tensor([[0.6084, 0.4365, 0.7665, 0.8730]])\n",
      "xyxy: tensor([[ 27.4711,   0.0000, 120.9829, 109.1241]])\n",
      "xyxyn: tensor([[0.2252, 0.0000, 0.9917, 0.8730]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 cell phone, 1 book, 96.0ms\n",
      "Speed: 4.4ms preprocess, 96.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 116.8ms\n",
      "Speed: 3.0ms preprocess, 116.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3528])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.3454, 63.6277,  0.3528,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.6727, 31.8138, 47.3454, 63.6277]])\n",
      "xywhn: tensor([[0.4012, 0.4419, 0.8025, 0.8837]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.3454, 63.6277]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8025, 0.8837]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2948])\n",
      "data: tensor([[ 0.0000,  1.0777, 30.2955, 70.4701,  0.2948,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.1478, 35.7739, 30.2955, 69.3924]])\n",
      "xywhn: tensor([[0.2567, 0.4969, 0.5135, 0.9638]])\n",
      "xyxy: tensor([[ 0.0000,  1.0777, 30.2955, 70.4701]])\n",
      "xyxyn: tensor([[0.0000, 0.0150, 0.5135, 0.9788]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2901])\n",
      "data: tensor([[25.9143,  3.2279, 59.0000, 70.6721,  0.2901,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[42.4572, 36.9500, 33.0857, 67.4442]])\n",
      "xywhn: tensor([[0.7196, 0.5132, 0.5608, 0.9367]])\n",
      "xyxy: tensor([[25.9143,  3.2279, 59.0000, 70.6721]])\n",
      "xyxyn: tensor([[0.4392, 0.0448, 1.0000, 0.9816]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3900])\n",
      "data: tensor([[ 31.0789,   0.0000, 121.5450, 110.3502,   0.3900,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.3120,  55.1751,  90.4661, 110.3502]])\n",
      "xywhn: tensor([[0.6204, 0.4414, 0.7355, 0.8828]])\n",
      "xyxy: tensor([[ 31.0789,   0.0000, 121.5450, 110.3502]])\n",
      "xyxyn: tensor([[0.2527, 0.0000, 0.9882, 0.8828]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 142.2ms\n",
      "Speed: 9.3ms preprocess, 142.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 cell phone, 1 book, 95.4ms\n",
      "Speed: 0.0ms preprocess, 95.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 3 -1s, 138.6ms\n",
      "Speed: 0.0ms preprocess, 138.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 2 -1s, 139.9ms\n",
      "Speed: 17.9ms preprocess, 139.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3888])\n",
      "data: tensor([[25.2514,  3.0709, 59.0000, 70.5259,  0.3888,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[42.1257, 36.7984, 33.7486, 67.4550]])\n",
      "xywhn: tensor([[0.7140, 0.5111, 0.5720, 0.9369]])\n",
      "xyxy: tensor([[25.2514,  3.0709, 59.0000, 70.5259]])\n",
      "xyxyn: tensor([[0.4280, 0.0427, 1.0000, 0.9795]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3254])\n",
      "data: tensor([[ 0.0000,  0.9145, 30.5510, 70.1173,  0.3254,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.2755, 35.5159, 30.5510, 69.2028]])\n",
      "xywhn: tensor([[0.2589, 0.4933, 0.5178, 0.9611]])\n",
      "xyxy: tensor([[ 0.0000,  0.9145, 30.5510, 70.1173]])\n",
      "xyxyn: tensor([[0.0000, 0.0127, 0.5178, 0.9739]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3171])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.0657, 67.7403,  0.3171,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.5328, 33.8702, 47.0657, 67.7403]])\n",
      "xywhn: tensor([[0.3989, 0.4704, 0.7977, 0.9408]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.0657, 67.7403]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7977, 0.9408]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3314])\n",
      "data: tensor([[ 23.2276,   0.0000, 117.9091, 112.9248,   0.3314,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (126, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 70.5684,  56.4624,  94.6815, 112.9248]])\n",
      "xywhn: tensor([[0.5737, 0.4481, 0.7698, 0.8962]])\n",
      "xyxy: tensor([[ 23.2276,   0.0000, 117.9091, 112.9248]])\n",
      "xyxyn: tensor([[0.1888, 0.0000, 0.9586, 0.8962]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2805])\n",
      "data: tensor([[  0.0000,   4.2489,  22.3191, 123.7694,   0.2805,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (126, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 11.1596,  64.0092,  22.3191, 119.5205]])\n",
      "xywhn: tensor([[0.0907, 0.5080, 0.1815, 0.9486]])\n",
      "xyxy: tensor([[  0.0000,   4.2489,  22.3191, 123.7694]])\n",
      "xyxyn: tensor([[0.0000, 0.0337, 0.1815, 0.9823]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 cell phone, 1 book, 98.7ms\n",
      "Speed: 4.3ms preprocess, 98.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 3 -1s, 123.9ms\n",
      "Speed: 3.0ms preprocess, 123.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 140.6ms\n",
      "Speed: 10.2ms preprocess, 140.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4123])\n",
      "data: tensor([[25.3117,  3.1300, 59.0000, 70.3563,  0.4123,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[42.1559, 36.7432, 33.6883, 67.2263]])\n",
      "xywhn: tensor([[0.7145, 0.5103, 0.5710, 0.9337]])\n",
      "xyxy: tensor([[25.3117,  3.1300, 59.0000, 70.3563]])\n",
      "xyxyn: tensor([[0.4290, 0.0435, 1.0000, 0.9772]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3419])\n",
      "data: tensor([[ 0.0000,  0.9230, 30.4046, 70.0924,  0.3419,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.2023, 35.5077, 30.4046, 69.1693]])\n",
      "xywhn: tensor([[0.2577, 0.4932, 0.5153, 0.9607]])\n",
      "xyxy: tensor([[ 0.0000,  0.9230, 30.4046, 70.0924]])\n",
      "xyxyn: tensor([[0.0000, 0.0128, 0.5153, 0.9735]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2990])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.0148, 67.8355,  0.2990,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.5074, 33.9178, 47.0148, 67.8355]])\n",
      "xywhn: tensor([[0.3984, 0.4711, 0.7969, 0.9422]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.0148, 67.8355]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7969, 0.9422]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4409])\n",
      "data: tensor([[ 32.0241,   0.0000, 123.7863, 115.0526,   0.4409,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 126)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 77.9052,  57.5263,  91.7623, 115.0526]])\n",
      "xywhn: tensor([[0.6183, 0.4602, 0.7283, 0.9204]])\n",
      "xyxy: tensor([[ 32.0241,   0.0000, 123.7863, 115.0526]])\n",
      "xyxyn: tensor([[0.2542, 0.0000, 0.9824, 0.9204]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 2 laptops, 1 mouse, 1 cell phone, 1 book, 93.8ms\n",
      "Speed: 3.2ms preprocess, 93.8ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 3 -1s, 141.0ms\n",
      "Speed: 0.0ms preprocess, 141.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 156.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3472])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.7189, 65.6886,  0.3472,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.8595, 32.8443, 47.7189, 65.6886]])\n",
      "xywhn: tensor([[0.3977, 0.4562, 0.7953, 0.9123]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.7189, 65.6886]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7953, 0.9123]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3370])\n",
      "data: tensor([[ 0.0000,  1.8071, 29.7472, 69.4356,  0.3370,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[14.8736, 35.6214, 29.7472, 67.6285]])\n",
      "xywhn: tensor([[0.2479, 0.4947, 0.4958, 0.9393]])\n",
      "xyxy: tensor([[ 0.0000,  1.8071, 29.7472, 69.4356]])\n",
      "xyxyn: tensor([[0.0000, 0.0251, 0.4958, 0.9644]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3334])\n",
      "data: tensor([[24.0445,  3.3223, 60.0000, 68.8507,  0.3334,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[42.0223, 36.0865, 35.9555, 65.5284]])\n",
      "xywhn: tensor([[0.7004, 0.5012, 0.5993, 0.9101]])\n",
      "xyxy: tensor([[24.0445,  3.3223, 60.0000, 68.8507]])\n",
      "xyxyn: tensor([[0.4007, 0.0461, 1.0000, 0.9563]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4066])\n",
      "data: tensor([[ 29.1180,   0.0000, 123.8578, 114.4717,   0.4066,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (126, 126)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.4879,  57.2359,  94.7397, 114.4717]])\n",
      "xywhn: tensor([[0.6070, 0.4543, 0.7519, 0.9085]])\n",
      "xyxy: tensor([[ 29.1180,   0.0000, 123.8578, 114.4717]])\n",
      "xyxyn: tensor([[0.2311, 0.0000, 0.9830, 0.9085]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 9.4ms preprocess, 156.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 100.6ms\n",
      "Speed: 0.4ms preprocess, 100.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 3 -1s, 152.3ms\n",
      "Speed: 4.5ms preprocess, 152.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 167.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3576])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.7154, 65.7236,  0.3576,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.8577, 32.8618, 47.7154, 65.7236]])\n",
      "xywhn: tensor([[0.3976, 0.4564, 0.7953, 0.9128]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.7154, 65.7236]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7953, 0.9128]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3457])\n",
      "data: tensor([[ 0.0000,  1.8357, 29.5997, 68.8012,  0.3457,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[14.7999, 35.3184, 29.5997, 66.9655]])\n",
      "xywhn: tensor([[0.2467, 0.4905, 0.4933, 0.9301]])\n",
      "xyxy: tensor([[ 0.0000,  1.8357, 29.5997, 68.8012]])\n",
      "xyxyn: tensor([[0.0000, 0.0255, 0.4933, 0.9556]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3341])\n",
      "data: tensor([[24.5035,  3.4354, 60.0000, 68.4538,  0.3341,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[42.2517, 35.9446, 35.4965, 65.0184]])\n",
      "xywhn: tensor([[0.7042, 0.4992, 0.5916, 0.9030]])\n",
      "xyxy: tensor([[24.5035,  3.4354, 60.0000, 68.4538]])\n",
      "xyxyn: tensor([[0.4084, 0.0477, 1.0000, 0.9507]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3691])\n",
      "data: tensor([[ 30.7280,   0.0000, 123.1827, 113.8428,   0.3691,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (126, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.9554,  56.9214,  92.4548, 113.8428]])\n",
      "xywhn: tensor([[0.6156, 0.4518, 0.7396, 0.9035]])\n",
      "xyxy: tensor([[ 30.7280,   0.0000, 123.1827, 113.8428]])\n",
      "xyxyn: tensor([[0.2458, 0.0000, 0.9855, 0.9035]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.8ms preprocess, 167.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 90.4ms\n",
      "Speed: 1.9ms preprocess, 90.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 3 -1s, 125.7ms\n",
      "Speed: 3.3ms preprocess, 125.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 181.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3720])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.7691, 65.8113,  0.3720,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.8845, 32.9056, 47.7691, 65.8113]])\n",
      "xywhn: tensor([[0.3981, 0.4570, 0.7962, 0.9140]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.7691, 65.8113]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7962, 0.9140]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3493])\n",
      "data: tensor([[ 0.0000,  1.7928, 29.5767, 68.3496,  0.3493,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[14.7883, 35.0712, 29.5767, 66.5568]])\n",
      "xywhn: tensor([[0.2465, 0.4871, 0.4929, 0.9244]])\n",
      "xyxy: tensor([[ 0.0000,  1.7928, 29.5767, 68.3496]])\n",
      "xyxyn: tensor([[0.0000, 0.0249, 0.4929, 0.9493]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3346])\n",
      "data: tensor([[24.8165,  3.5387, 60.0000, 68.1931,  0.3346,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[42.4082, 35.8659, 35.1835, 64.6544]])\n",
      "xywhn: tensor([[0.7068, 0.4981, 0.5864, 0.8980]])\n",
      "xyxy: tensor([[24.8165,  3.5387, 60.0000, 68.1931]])\n",
      "xyxyn: tensor([[0.4136, 0.0491, 1.0000, 0.9471]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3486])\n",
      "data: tensor([[ 28.5713,   0.0000, 119.4071, 111.0722,   0.3486,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.9892,  55.5361,  90.8358, 111.0722]])\n",
      "xywhn: tensor([[0.5919, 0.4443, 0.7267, 0.8886]])\n",
      "xyxy: tensor([[ 28.5713,   0.0000, 119.4071, 111.0722]])\n",
      "xyxyn: tensor([[0.2286, 0.0000, 0.9553, 0.8886]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 8.0ms preprocess, 181.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 114.3ms\n",
      "Speed: 4.5ms preprocess, 114.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 3 -1s, 128.8ms\n",
      "Speed: 3.3ms preprocess, 128.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 159.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3705])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.8421, 65.2685,  0.3705,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.9210, 32.6343, 47.8421, 65.2685]])\n",
      "xywhn: tensor([[0.3987, 0.4533, 0.7974, 0.9065]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.8421, 65.2685]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7974, 0.9065]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3617])\n",
      "data: tensor([[ 0.0000,  2.1727, 29.4201, 67.5076,  0.3617,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[14.7101, 34.8401, 29.4201, 65.3348]])\n",
      "xywhn: tensor([[0.2452, 0.4839, 0.4903, 0.9074]])\n",
      "xyxy: tensor([[ 0.0000,  2.1727, 29.4201, 67.5076]])\n",
      "xyxyn: tensor([[0.0000, 0.0302, 0.4903, 0.9376]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3415])\n",
      "data: tensor([[25.3142,  3.4853, 60.0000, 68.1455,  0.3415,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[42.6571, 35.8154, 34.6858, 64.6603]])\n",
      "xywhn: tensor([[0.7110, 0.4974, 0.5781, 0.8981]])\n",
      "xyxy: tensor([[25.3142,  3.4853, 60.0000, 68.1455]])\n",
      "xyxyn: tensor([[0.4219, 0.0484, 1.0000, 0.9465]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4307])\n",
      "data: tensor([[ 28.6150,   0.0000, 118.2374, 110.4078,   0.4307,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.4262,  55.2039,  89.6224, 110.4078]])\n",
      "xywhn: tensor([[0.5921, 0.4416, 0.7228, 0.8833]])\n",
      "xyxy: tensor([[ 28.6150,   0.0000, 118.2374, 110.4078]])\n",
      "xyxyn: tensor([[0.2308, 0.0000, 0.9535, 0.8833]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 5.6ms preprocess, 159.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 102.0ms\n",
      "Speed: 2.8ms preprocess, 102.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 3 -1s, 139.2ms\n",
      "Speed: 0.5ms preprocess, 139.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 1 1, 163.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5291])\n",
      "data: tensor([[24.4660,  5.8633, 61.0000, 69.1449,  0.5291,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 61)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[42.7330, 37.5041, 36.5340, 63.2815]])\n",
      "xywhn: tensor([[0.7005, 0.5209, 0.5989, 0.8789]])\n",
      "xyxy: tensor([[24.4660,  5.8633, 61.0000, 69.1449]])\n",
      "xyxyn: tensor([[0.4011, 0.0814, 1.0000, 0.9603]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4459])\n",
      "data: tensor([[ 0.0000,  3.0636, 29.2578, 67.3251,  0.4459,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 61)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[14.6289, 35.1944, 29.2578, 64.2616]])\n",
      "xywhn: tensor([[0.2398, 0.4888, 0.4796, 0.8925]])\n",
      "xyxy: tensor([[ 0.0000,  3.0636, 29.2578, 67.3251]])\n",
      "xyxyn: tensor([[0.0000, 0.0425, 0.4796, 0.9351]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3050])\n",
      "data: tensor([[12.2779,  0.2187, 61.0000, 70.0121,  0.3050,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 61)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[36.6389, 35.1154, 48.7221, 69.7934]])\n",
      "xywhn: tensor([[0.6006, 0.4877, 0.7987, 0.9694]])\n",
      "xyxy: tensor([[12.2779,  0.2187, 61.0000, 70.0121]])\n",
      "xyxyn: tensor([[0.2013, 0.0030, 1.0000, 0.9724]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4225])\n",
      "data: tensor([[ 29.1702,   0.0000, 119.2404, 111.9910,   0.4225,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.2053,  55.9955,  90.0702, 111.9910]])\n",
      "xywhn: tensor([[0.5984, 0.4480, 0.7264, 0.8959]])\n",
      "xyxy: tensor([[ 29.1702,   0.0000, 119.2404, 111.9910]])\n",
      "xyxyn: tensor([[0.2352, 0.0000, 0.9616, 0.8959]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.2531])\n",
      "data: tensor([[39.0742,  0.0000, 69.8816, 28.9565,  0.2531,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[54.4779, 14.4783, 30.8074, 28.9565]])\n",
      "xywhn: tensor([[0.4393, 0.1158, 0.2484, 0.2317]])\n",
      "xyxy: tensor([[39.0742,  0.0000, 69.8816, 28.9565]])\n",
      "xyxyn: tensor([[0.3151, 0.0000, 0.5636, 0.2317]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 11.7ms preprocess, 163.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 122.8ms\n",
      "Speed: 1.5ms preprocess, 122.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 4 -1s, 179.9ms\n",
      "Speed: 8.0ms preprocess, 179.9ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4670])\n",
      "data: tensor([[23.2633,  3.1503, 62.0000, 68.9933,  0.4670,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 62)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[42.6316, 36.0718, 38.7367, 65.8430]])\n",
      "xywhn: tensor([[0.6876, 0.5010, 0.6248, 0.9145]])\n",
      "xyxy: tensor([[23.2633,  3.1503, 62.0000, 68.9933]])\n",
      "xyxyn: tensor([[0.3752, 0.0438, 1.0000, 0.9582]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3821])\n",
      "data: tensor([[ 0.0000,  4.9565, 27.8359, 69.0495,  0.3821,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 62)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[13.9180, 37.0030, 27.8359, 64.0930]])\n",
      "xywhn: tensor([[0.2245, 0.5139, 0.4490, 0.8902]])\n",
      "xyxy: tensor([[ 0.0000,  4.9565, 27.8359, 69.0495]])\n",
      "xyxyn: tensor([[0.0000, 0.0688, 0.4490, 0.9590]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3632])\n",
      "data: tensor([[ 0.0000,  0.0000, 43.7542, 66.0583,  0.3632,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 62)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[21.8771, 33.0292, 43.7542, 66.0583]])\n",
      "xywhn: tensor([[0.3529, 0.4587, 0.7057, 0.9175]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 43.7542, 66.0583]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7057, 0.9175]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2732])\n",
      "data: tensor([[10.4894,  0.3438, 61.5906, 69.9237,  0.2732,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 62)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[36.0400, 35.1337, 51.1012, 69.5798]])\n",
      "xywhn: tensor([[0.5813, 0.4880, 0.8242, 0.9664]])\n",
      "xyxy: tensor([[10.4894,  0.3438, 61.5906, 69.9237]])\n",
      "xyxyn: tensor([[0.1692, 0.0048, 0.9934, 0.9712]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 199.8ms\n",
      "Speed: 4.3ms preprocess, 199.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 129.8ms\n",
      "Speed: 0.0ms preprocess, 129.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3704])\n",
      "data: tensor([[ 27.2600,   0.0000, 118.6569, 112.2537,   0.3704,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (124, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.9585,  56.1269,  91.3969, 112.2537]])\n",
      "xywhn: tensor([[0.5884, 0.4526, 0.7371, 0.9053]])\n",
      "xyxy: tensor([[ 27.2600,   0.0000, 118.6569, 112.2537]])\n",
      "xyxyn: tensor([[0.2198, 0.0000, 0.9569, 0.9053]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 3 -1s, 190.0ms\n",
      "Speed: 5.0ms preprocess, 190.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5304])\n",
      "data: tensor([[ 0.0000,  0.0000, 42.5198, 64.8001,  0.5304,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[21.2599, 32.4001, 42.5198, 64.8001]])\n",
      "xywhn: tensor([[0.3271, 0.4500, 0.6542, 0.9000]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 42.5198, 64.8001]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6542, 0.9000]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3937])\n",
      "data: tensor([[18.2453,  3.9894, 64.3968, 68.0523,  0.3937,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.3210, 36.0209, 46.1515, 64.0629]])\n",
      "xywhn: tensor([[0.6357, 0.5003, 0.7100, 0.8898]])\n",
      "xyxy: tensor([[18.2453,  3.9894, 64.3968, 68.0523]])\n",
      "xyxyn: tensor([[0.2807, 0.0554, 0.9907, 0.9452]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2656])\n",
      "data: tensor([[ 0.0000,  5.1379, 28.0806, 69.1333,  0.2656,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[14.0403, 37.1356, 28.0806, 63.9954]])\n",
      "xywhn: tensor([[0.2160, 0.5158, 0.4320, 0.8888]])\n",
      "xyxy: tensor([[ 0.0000,  5.1379, 28.0806, 69.1333]])\n",
      "xyxyn: tensor([[0.0000, 0.0714, 0.4320, 0.9602]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 195.0ms\n",
      "Speed: 5.0ms preprocess, 195.0ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 95.3ms\n",
      "Speed: 8.0ms preprocess, 95.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3969])\n",
      "data: tensor([[ 27.4571,   0.0000, 118.3664, 112.2878,   0.3969,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.9118,  56.1439,  90.9093, 112.2878]])\n",
      "xywhn: tensor([[0.5880, 0.4565, 0.7331, 0.9129]])\n",
      "xyxy: tensor([[ 27.4571,   0.0000, 118.3664, 112.2878]])\n",
      "xyxyn: tensor([[0.2214, 0.0000, 0.9546, 0.9129]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 2 -1s, 180.6ms\n",
      "Speed: 3.9ms preprocess, 180.6ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4151])\n",
      "data: tensor([[17.5498,  6.8806, 65.1588, 67.1489,  0.4151,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 67)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.3543, 37.0148, 47.6090, 60.2683]])\n",
      "xywhn: tensor([[0.6172, 0.5141, 0.7106, 0.8371]])\n",
      "xyxy: tensor([[17.5498,  6.8806, 65.1588, 67.1489]])\n",
      "xyxyn: tensor([[0.2619, 0.0956, 0.9725, 0.9326]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3832])\n",
      "data: tensor([[ 0.0000,  0.0000, 43.2827, 65.0231,  0.3832,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (72, 67)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[21.6414, 32.5115, 43.2827, 65.0231]])\n",
      "xywhn: tensor([[0.3230, 0.4515, 0.6460, 0.9031]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 43.2827, 65.0231]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6460, 0.9031]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 199.4ms\n",
      "Speed: 7.8ms preprocess, 199.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 92.6ms\n",
      "Speed: 7.3ms preprocess, 92.6ms inference, 13.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4674])\n",
      "data: tensor([[ 25.8527,   0.0000, 113.7033, 110.3318,   0.4674,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.7780,  55.1659,  87.8506, 110.3318]])\n",
      "xywhn: tensor([[0.5720, 0.4485, 0.7201, 0.8970]])\n",
      "xyxy: tensor([[ 25.8527,   0.0000, 113.7033, 110.3318]])\n",
      "xyxyn: tensor([[0.2119, 0.0000, 0.9320, 0.8970]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 3 -1s, 156.2ms\n",
      "Speed: 2.3ms preprocess, 156.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x640 2 -1s, 153.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4562])\n",
      "data: tensor([[ 0.0000,  0.0000, 42.8026, 65.9190,  0.4562,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 69)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[21.4013, 32.9595, 42.8026, 65.9190]])\n",
      "xywhn: tensor([[0.3102, 0.4515, 0.6203, 0.9030]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 42.8026, 65.9190]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6203, 0.9030]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3567])\n",
      "data: tensor([[17.4429,  3.8238, 63.7551, 68.4064,  0.3567,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 69)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[40.5990, 36.1151, 46.3122, 64.5827]])\n",
      "xywhn: tensor([[0.5884, 0.4947, 0.6712, 0.8847]])\n",
      "xyxy: tensor([[17.4429,  3.8238, 63.7551, 68.4064]])\n",
      "xyxyn: tensor([[0.2528, 0.0524, 0.9240, 0.9371]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3264])\n",
      "data: tensor([[ 0.2417,  1.6797, 28.4975, 67.0967,  0.3264,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 69)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[14.3696, 34.3882, 28.2558, 65.4170]])\n",
      "xywhn: tensor([[0.2083, 0.4711, 0.4095, 0.8961]])\n",
      "xyxy: tensor([[ 0.2417,  1.6797, 28.4975, 67.0967]])\n",
      "xyxyn: tensor([[0.0035, 0.0230, 0.4130, 0.9191]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4303])\n",
      "data: tensor([[ 23.7627,   0.0000, 113.2624, 111.4749,   0.4303,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 68.5126,  55.7375,  89.4997, 111.4749]])\n",
      "xywhn: tensor([[0.5662, 0.4532, 0.7397, 0.9063]])\n",
      "xyxy: tensor([[ 23.7627,   0.0000, 113.2624, 111.4749]])\n",
      "xyxyn: tensor([[0.1964, 0.0000, 0.9361, 0.9063]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2961])\n",
      "data: tensor([[  0.0000,   2.4916,  21.7612, 118.3785,   0.2961,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 10.8806,  60.4350,  21.7612, 115.8869]])\n",
      "xywhn: tensor([[0.0899, 0.4913, 0.1798, 0.9422]])\n",
      "xyxy: tensor([[  0.0000,   2.4916,  21.7612, 118.3785]])\n",
      "xyxyn: tensor([[0.0000, 0.0203, 0.1798, 0.9624]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 19.8ms preprocess, 153.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 109.2ms\n",
      "Speed: 0.0ms preprocess, 109.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 3 -1s, 165.2ms\n",
      "Speed: 2.5ms preprocess, 165.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x640 2 -1s, 147.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4244])\n",
      "data: tensor([[ 0.0000,  0.0000, 47.3405, 66.9011,  0.4244,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 68)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.6703, 33.4506, 47.3405, 66.9011]])\n",
      "xywhn: tensor([[0.3481, 0.4582, 0.6962, 0.9165]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 47.3405, 66.9011]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6962, 0.9165]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3482])\n",
      "data: tensor([[15.1107,  3.2458, 66.5222, 68.9622,  0.3482,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 68)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[40.8165, 36.1040, 51.4115, 65.7163]])\n",
      "xywhn: tensor([[0.6002, 0.4946, 0.7561, 0.9002]])\n",
      "xyxy: tensor([[15.1107,  3.2458, 66.5222, 68.9622]])\n",
      "xyxyn: tensor([[0.2222, 0.0445, 0.9783, 0.9447]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3185])\n",
      "data: tensor([[ 0.0000,  2.5662, 31.0893, 67.9687,  0.3185,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 68)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.5447, 35.2674, 31.0893, 65.4025]])\n",
      "xywhn: tensor([[0.2286, 0.4831, 0.4572, 0.8959]])\n",
      "xyxy: tensor([[ 0.0000,  2.5662, 31.0893, 67.9687]])\n",
      "xyxyn: tensor([[0.0000, 0.0352, 0.4572, 0.9311]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4066])\n",
      "data: tensor([[ 24.2156,   0.0000, 117.1867, 110.9591,   0.4066,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 70.7011,  55.4795,  92.9711, 110.9591]])\n",
      "xywhn: tensor([[0.5843, 0.4511, 0.7684, 0.9021]])\n",
      "xyxy: tensor([[ 24.2156,   0.0000, 117.1867, 110.9591]])\n",
      "xyxyn: tensor([[0.2001, 0.0000, 0.9685, 0.9021]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2937])\n",
      "data: tensor([[  0.0000,   2.5798,  21.7722, 118.3979,   0.2937,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 10.8861,  60.4889,  21.7722, 115.8181]])\n",
      "xywhn: tensor([[0.0900, 0.4918, 0.1799, 0.9416]])\n",
      "xyxy: tensor([[  0.0000,   2.5798,  21.7722, 118.3979]])\n",
      "xyxyn: tensor([[0.0000, 0.0210, 0.1799, 0.9626]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 9.5ms preprocess, 147.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 110.3ms\n",
      "Speed: 0.0ms preprocess, 110.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 2 -1s, 155.4ms\n",
      "Speed: 2.2ms preprocess, 155.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x640 1 -1, 169.3ms\n",
      "Speed: 5.5ms preprocess, 169.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5738])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.0103, 66.6417,  0.5738,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 66)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.0051, 33.3209, 46.0103, 66.6417]])\n",
      "xywhn: tensor([[0.3486, 0.4565, 0.6971, 0.9129]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.0103, 66.6417]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6971, 0.9129]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4145])\n",
      "data: tensor([[22.0090,  4.0452, 66.0000, 68.5967,  0.4145,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (73, 66)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[44.0045, 36.3210, 43.9910, 64.5515]])\n",
      "xywhn: tensor([[0.6667, 0.4975, 0.6665, 0.8843]])\n",
      "xyxy: tensor([[22.0090,  4.0452, 66.0000, 68.5967]])\n",
      "xyxyn: tensor([[0.3335, 0.0554, 1.0000, 0.9397]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3627])\n",
      "data: tensor([[ 23.5031,   0.0000, 115.5024, 111.8891,   0.3627,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.5028,  55.9446,  91.9992, 111.8891]])\n",
      "xywhn: tensor([[0.5792, 0.4548, 0.7667, 0.9097]])\n",
      "xyxy: tensor([[ 23.5031,   0.0000, 115.5024, 111.8891]])\n",
      "xyxyn: tensor([[0.1959, 0.0000, 0.9625, 0.9097]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 92.7ms\n",
      "Speed: 0.0ms preprocess, 92.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x576 3 -1s, 147.4ms\n",
      "Speed: 3.5ms preprocess, 147.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x640 1 -1, 152.8ms\n",
      "Speed: 5.2ms preprocess, 152.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4310])\n",
      "data: tensor([[ 0.0000,  0.0000, 45.3780, 68.6917,  0.4310,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (74, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.6890, 34.3459, 45.3780, 68.6917]])\n",
      "xywhn: tensor([[0.3491, 0.4641, 0.6981, 0.9283]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 45.3780, 68.6917]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6981, 0.9283]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3835])\n",
      "data: tensor([[14.7821,  5.1373, 65.0000, 70.0423,  0.3835,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (74, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[39.8911, 37.5898, 50.2179, 64.9050]])\n",
      "xywhn: tensor([[0.6137, 0.5080, 0.7726, 0.8771]])\n",
      "xyxy: tensor([[14.7821,  5.1373, 65.0000, 70.0423]])\n",
      "xyxyn: tensor([[0.2274, 0.0694, 1.0000, 0.9465]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2601])\n",
      "data: tensor([[ 0.0000,  2.0645, 31.8121, 70.1188,  0.2601,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (74, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.9061, 36.0917, 31.8121, 68.0543]])\n",
      "xywhn: tensor([[0.2447, 0.4877, 0.4894, 0.9197]])\n",
      "xyxy: tensor([[ 0.0000,  2.0645, 31.8121, 70.1188]])\n",
      "xyxyn: tensor([[0.0000, 0.0279, 0.4894, 0.9476]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3720])\n",
      "data: tensor([[ 25.2753,   0.0000, 116.8763, 111.4363,   0.3720,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 71.0758,  55.7181,  91.6009, 111.4363]])\n",
      "xywhn: tensor([[0.5923, 0.4530, 0.7633, 0.9060]])\n",
      "xyxy: tensor([[ 25.2753,   0.0000, 116.8763, 111.4363]])\n",
      "xyxyn: tensor([[0.2106, 0.0000, 0.9740, 0.9060]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 98.3ms\n",
      "Speed: 0.6ms preprocess, 98.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x576 3 -1s, 131.1ms\n",
      "Speed: 5.1ms preprocess, 131.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x640 1 -1, 167.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3643])\n",
      "data: tensor([[ 0.0000,  0.4012, 44.9576, 71.5450,  0.3643,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (74, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.4788, 35.9731, 44.9576, 71.1438]])\n",
      "xywhn: tensor([[0.3458, 0.4861, 0.6917, 0.9614]])\n",
      "xyxy: tensor([[ 0.0000,  0.4012, 44.9576, 71.5450]])\n",
      "xyxyn: tensor([[0.0000, 0.0054, 0.6917, 0.9668]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3089])\n",
      "data: tensor([[15.7160,  2.2990, 65.0000, 72.1446,  0.3089,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (74, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[40.3580, 37.2218, 49.2840, 69.8456]])\n",
      "xywhn: tensor([[0.6209, 0.5030, 0.7582, 0.9439]])\n",
      "xyxy: tensor([[15.7160,  2.2990, 65.0000, 72.1446]])\n",
      "xyxyn: tensor([[0.2418, 0.0311, 1.0000, 0.9749]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2795])\n",
      "data: tensor([[ 0.0000,  6.7822, 31.1957, 70.7070,  0.2795,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (74, 65)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.5978, 38.7446, 31.1957, 63.9247]])\n",
      "xywhn: tensor([[0.2400, 0.5236, 0.4799, 0.8638]])\n",
      "xyxy: tensor([[ 0.0000,  6.7822, 31.1957, 70.7070]])\n",
      "xyxyn: tensor([[0.0000, 0.0917, 0.4799, 0.9555]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3833])\n",
      "data: tensor([[ 24.8492,   0.0000, 115.5162, 111.9644,   0.3833,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 70.1827,  55.9822,  90.6669, 111.9644]])\n",
      "xywhn: tensor([[0.5849, 0.4551, 0.7556, 0.9103]])\n",
      "xyxy: tensor([[ 24.8492,   0.0000, 115.5162, 111.9644]])\n",
      "xyxyn: tensor([[0.2071, 0.0000, 0.9626, 0.9103]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 6.7ms preprocess, 167.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 109.0ms\n",
      "Speed: 2.8ms preprocess, 109.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 3 -1s, 141.1ms\n",
      "Speed: 4.1ms preprocess, 141.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x640 1 -1, 164.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4092])\n",
      "data: tensor([[0.0000e+00, 6.5675e-02, 4.4484e+01, 7.3386e+01, 4.0922e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (75, 68)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.2420, 36.7260, 44.4841, 73.3207]])\n",
      "xywhn: tensor([[0.3271, 0.4897, 0.6542, 0.9776]])\n",
      "xyxy: tensor([[0.0000e+00, 6.5675e-02, 4.4484e+01, 7.3386e+01]])\n",
      "xyxyn: tensor([[0.0000e+00, 8.7566e-04, 6.5418e-01, 9.7848e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3169])\n",
      "data: tensor([[12.6731,  2.4988, 67.0984, 73.2599,  0.3169,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (75, 68)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[39.8857, 37.8794, 54.4253, 70.7611]])\n",
      "xywhn: tensor([[0.5866, 0.5051, 0.8004, 0.9435]])\n",
      "xyxy: tensor([[12.6731,  2.4988, 67.0984, 73.2599]])\n",
      "xyxyn: tensor([[0.1864, 0.0333, 0.9867, 0.9768]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2590])\n",
      "data: tensor([[ 0.0000,  5.2613, 29.6888, 73.0730,  0.2590,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (75, 68)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[14.8444, 39.1671, 29.6888, 67.8117]])\n",
      "xywhn: tensor([[0.2183, 0.5222, 0.4366, 0.9042]])\n",
      "xyxy: tensor([[ 0.0000,  5.2613, 29.6888, 73.0730]])\n",
      "xyxyn: tensor([[0.0000, 0.0702, 0.4366, 0.9743]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3521])\n",
      "data: tensor([[ 19.9430,   0.0000, 116.6443, 111.4738,   0.3521,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 68.2937,  55.7369,  96.7013, 111.4738]])\n",
      "xywhn: tensor([[0.5691, 0.4531, 0.8058, 0.9063]])\n",
      "xyxy: tensor([[ 19.9430,   0.0000, 116.6443, 111.4738]])\n",
      "xyxyn: tensor([[0.1662, 0.0000, 0.9720, 0.9063]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 12.9ms preprocess, 164.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 94.4ms\n",
      "Speed: 3.2ms preprocess, 94.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 2 -1s, 154.4ms\n",
      "Speed: 3.3ms preprocess, 154.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x640 2 -1s, 158.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3341])\n",
      "data: tensor([[ 0.0000,  0.0000, 45.1544, 72.7963,  0.3341,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (75, 69)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.5772, 36.3982, 45.1544, 72.7963]])\n",
      "xywhn: tensor([[0.3272, 0.4853, 0.6544, 0.9706]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 45.1544, 72.7963]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6544, 0.9706]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2799])\n",
      "data: tensor([[ 0.0000,  4.8990, 30.0240, 72.4580,  0.2799,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (75, 69)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.0120, 38.6785, 30.0240, 67.5590]])\n",
      "xywhn: tensor([[0.2176, 0.5157, 0.4351, 0.9008]])\n",
      "xyxy: tensor([[ 0.0000,  4.8990, 30.0240, 72.4580]])\n",
      "xyxyn: tensor([[0.0000, 0.0653, 0.4351, 0.9661]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3727])\n",
      "data: tensor([[ 24.2808,   0.0000, 114.7824, 110.5906,   0.3727,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 119)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.5316,  55.2953,  90.5015, 110.5906]])\n",
      "xywhn: tensor([[0.5843, 0.4496, 0.7605, 0.8991]])\n",
      "xyxy: tensor([[ 24.2808,   0.0000, 114.7824, 110.5906]])\n",
      "xyxyn: tensor([[0.2040, 0.0000, 0.9646, 0.8991]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2793])\n",
      "data: tensor([[  0.0000,   0.0000,  22.0993, 117.2029,   0.2793,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 119)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 11.0496,  58.6014,  22.0993, 117.2029]])\n",
      "xywhn: tensor([[0.0929, 0.4764, 0.1857, 0.9529]])\n",
      "xyxy: tensor([[  0.0000,   0.0000,  22.0993, 117.2029]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.1857, 0.9529]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 16.0ms preprocess, 158.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 107.5ms\n",
      "Speed: 3.6ms preprocess, 107.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 1 -1, 156.3ms\n",
      "Speed: 5.6ms preprocess, 156.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x640 1 -1, 171.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3418])\n",
      "data: tensor([[ 0.0000,  0.0000, 45.1078, 73.7043,  0.3418,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (76, 71)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.5539, 36.8521, 45.1078, 73.7043]])\n",
      "xywhn: tensor([[0.3177, 0.4849, 0.6353, 0.9698]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 45.1078, 73.7043]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6353, 0.9698]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3821])\n",
      "data: tensor([[ 25.4308,   0.0000, 115.6781, 110.5506,   0.3821,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 119)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 70.5545,  55.2753,  90.2473, 110.5506]])\n",
      "xywhn: tensor([[0.5929, 0.4531, 0.7584, 0.9062]])\n",
      "xyxy: tensor([[ 25.4308,   0.0000, 115.6781, 110.5506]])\n",
      "xyxyn: tensor([[0.2137, 0.0000, 0.9721, 0.9062]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 4.7ms preprocess, 171.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 120.1ms\n",
      "Speed: 6.5ms preprocess, 120.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 2 -1s, 182.3ms\n",
      "Speed: 6.3ms preprocess, 182.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3693])\n",
      "data: tensor([[ 0.0000,  0.0000, 42.6715, 74.9767,  0.3693,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (77, 72)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[21.3357, 37.4883, 42.6715, 74.9767]])\n",
      "xywhn: tensor([[0.2963, 0.4869, 0.5927, 0.9737]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 42.6715, 74.9767]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.5927, 0.9737]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2553])\n",
      "data: tensor([[ 0.0000,  6.1580, 30.7188, 73.7308,  0.2553,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (77, 72)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.3594, 39.9444, 30.7188, 67.5728]])\n",
      "xywhn: tensor([[0.2133, 0.5188, 0.4266, 0.8776]])\n",
      "xyxy: tensor([[ 0.0000,  6.1580, 30.7188, 73.7308]])\n",
      "xyxyn: tensor([[0.0000, 0.0800, 0.4266, 0.9575]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 198.1ms\n",
      "Speed: 15.6ms preprocess, 198.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 124.6ms\n",
      "Speed: 2.3ms preprocess, 124.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3484])\n",
      "data: tensor([[ 23.3047,   0.0000, 113.2653, 113.9546,   0.3484,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 68.2850,  56.9773,  89.9606, 113.9546]])\n",
      "xywhn: tensor([[0.5690, 0.4709, 0.7497, 0.9418]])\n",
      "xyxy: tensor([[ 23.3047,   0.0000, 113.2653, 113.9546]])\n",
      "xyxyn: tensor([[0.1942, 0.0000, 0.9439, 0.9418]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 1 1, 175.1ms\n",
      "Speed: 9.0ms preprocess, 175.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 -1, 175.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4561])\n",
      "data: tensor([[ 0.0000,  6.9857, 38.2397, 75.6561,  0.4561,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (78, 78)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[19.1198, 41.3209, 38.2397, 68.6704]])\n",
      "xywhn: tensor([[0.2451, 0.5298, 0.4903, 0.8804]])\n",
      "xyxy: tensor([[ 0.0000,  6.9857, 38.2397, 75.6561]])\n",
      "xyxyn: tensor([[0.0000, 0.0896, 0.4903, 0.9700]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4315])\n",
      "data: tensor([[ 0.1866, 11.3103, 28.0630, 76.2622,  0.4315,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (78, 78)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[14.1248, 43.7862, 27.8764, 64.9519]])\n",
      "xywhn: tensor([[0.1811, 0.5614, 0.3574, 0.8327]])\n",
      "xyxy: tensor([[ 0.1866, 11.3103, 28.0630, 76.2622]])\n",
      "xyxyn: tensor([[0.0024, 0.1450, 0.3598, 0.9777]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3163])\n",
      "data: tensor([[26.8593, 68.1598, 78.0000, 77.9012,  0.3163,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (78, 78)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[52.4296, 73.0305, 51.1407,  9.7414]])\n",
      "xywhn: tensor([[0.6722, 0.9363, 0.6557, 0.1249]])\n",
      "xyxy: tensor([[26.8593, 68.1598, 78.0000, 77.9012]])\n",
      "xyxyn: tensor([[0.3443, 0.8738, 1.0000, 0.9987]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3667])\n",
      "data: tensor([[ 25.0981,   0.0000, 115.9753, 109.6740,   0.3667,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 70.5367,  54.8370,  90.8772, 109.6740]])\n",
      "xywhn: tensor([[0.5782, 0.4495, 0.7449, 0.8990]])\n",
      "xyxy: tensor([[ 25.0981,   0.0000, 115.9753, 109.6740]])\n",
      "xyxyn: tensor([[0.2057, 0.0000, 0.9506, 0.8990]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 4.1ms preprocess, 175.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 100.6ms\n",
      "Speed: 0.0ms preprocess, 100.6ms inference, 15.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 163.6ms\n",
      "Speed: 3.1ms preprocess, 163.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 -1, 157.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4337])\n",
      "data: tensor([[ 0.0000, 10.5685, 37.6283, 73.1007,  0.4337,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (78, 80)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[18.8141, 41.8346, 37.6283, 62.5322]])\n",
      "xywhn: tensor([[0.2352, 0.5363, 0.4704, 0.8017]])\n",
      "xyxy: tensor([[ 0.0000, 10.5685, 37.6283, 73.1007]])\n",
      "xyxyn: tensor([[0.0000, 0.1355, 0.4704, 0.9372]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3771])\n",
      "data: tensor([[ 25.8519,   0.0000, 117.1369, 109.1045,   0.3771,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 71.4944,  54.5523,  91.2850, 109.1045]])\n",
      "xywhn: tensor([[0.5860, 0.4471, 0.7482, 0.8943]])\n",
      "xyxy: tensor([[ 25.8519,   0.0000, 117.1369, 109.1045]])\n",
      "xyxyn: tensor([[0.2119, 0.0000, 0.9601, 0.8943]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 7.0ms preprocess, 157.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 119.1ms\n",
      "Speed: 0.0ms preprocess, 119.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 181.7ms\n",
      "Speed: 7.5ms preprocess, 181.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 -1, 136.7ms\n",
      "Speed: 7.6ms preprocess, 136.7ms inference, 15.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4511])\n",
      "data: tensor([[ 0.0000,  0.0000, 42.4124, 75.6073,  0.4511,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (79, 83)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[21.2062, 37.8037, 42.4124, 75.6073]])\n",
      "xywhn: tensor([[0.2555, 0.4785, 0.5110, 0.9571]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 42.4124, 75.6073]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.5110, 0.9571]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2503])\n",
      "data: tensor([[0.0000e+00, 1.8142e-02, 6.0254e+01, 7.6728e+01, 2.5027e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (79, 83)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[30.1269, 38.3732, 60.2538, 76.7100]])\n",
      "xywhn: tensor([[0.3630, 0.4857, 0.7259, 0.9710]])\n",
      "xyxy: tensor([[0.0000e+00, 1.8142e-02, 6.0254e+01, 7.6728e+01]])\n",
      "xyxyn: tensor([[0.0000e+00, 2.2965e-04, 7.2595e-01, 9.7124e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4298])\n",
      "data: tensor([[ 27.3969,   0.0000, 117.1586, 111.7933,   0.4298,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.2777,  55.8967,  89.7617, 111.7933]])\n",
      "xywhn: tensor([[0.5876, 0.4620, 0.7298, 0.9239]])\n",
      "xyxy: tensor([[ 27.3969,   0.0000, 117.1586, 111.7933]])\n",
      "xyxyn: tensor([[0.2227, 0.0000, 0.9525, 0.9239]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 95.1ms\n",
      "Speed: 3.5ms preprocess, 95.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 608x640 2 -1s, 159.9ms\n",
      "Speed: 5.6ms preprocess, 159.9ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3591])\n",
      "data: tensor([[ 0.0000,  0.0000, 46.0145, 74.2856,  0.3591,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (79, 85)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.0073, 37.1428, 46.0145, 74.2856]])\n",
      "xywhn: tensor([[0.2707, 0.4702, 0.5413, 0.9403]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 46.0145, 74.2856]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.5413, 0.9403]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3223])\n",
      "data: tensor([[ 0.7300,  0.0000, 61.6244, 74.8574,  0.3223,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (79, 85)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[31.1772, 37.4287, 60.8944, 74.8574]])\n",
      "xywhn: tensor([[0.3668, 0.4738, 0.7164, 0.9476]])\n",
      "xyxy: tensor([[ 0.7300,  0.0000, 61.6244, 74.8574]])\n",
      "xyxyn: tensor([[0.0086, 0.0000, 0.7250, 0.9476]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4077])\n",
      "data: tensor([[ 26.9909,   0.0000, 117.6397, 111.7041,   0.4077,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.3153,  55.8521,  90.6488, 111.7041]])\n",
      "xywhn: tensor([[0.5879, 0.4616, 0.7370, 0.9232]])\n",
      "xyxy: tensor([[ 26.9909,   0.0000, 117.6397, 111.7041]])\n",
      "xyxyn: tensor([[0.2194, 0.0000, 0.9564, 0.9232]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 141.4ms\n",
      "Speed: 6.2ms preprocess, 141.4ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 91.4ms\n",
      "Speed: 3.3ms preprocess, 91.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 608x640 1 -1, 171.6ms\n",
      "Speed: 3.3ms preprocess, 171.6ms inference, 2.6ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x640 1 -1, 175.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3825])\n",
      "data: tensor([[ 0.0000,  0.0000, 48.4616, 76.2034,  0.3825,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (80, 87)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[24.2308, 38.1017, 48.4616, 76.2034]])\n",
      "xywhn: tensor([[0.2785, 0.4763, 0.5570, 0.9525]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 48.4616, 76.2034]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.5570, 0.9525]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5010])\n",
      "data: tensor([[ 27.3730,   0.0000, 118.7975, 111.7577,   0.5010,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.0853,  55.8789,  91.4245, 111.7577]])\n",
      "xywhn: tensor([[0.5894, 0.4618, 0.7373, 0.9236]])\n",
      "xyxy: tensor([[ 27.3730,   0.0000, 118.7975, 111.7577]])\n",
      "xyxyn: tensor([[0.2208, 0.0000, 0.9580, 0.9236]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 6.1ms preprocess, 175.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 96.0ms\n",
      "Speed: 2.2ms preprocess, 96.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 608x640 1 -1, 158.2ms\n",
      "Speed: 3.5ms preprocess, 158.2ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x640 1 -1, 150.9ms\n",
      "Speed: 15.2ms preprocess, 150.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4099])\n",
      "data: tensor([[ 0.0000,  0.0000, 49.1986, 75.0083,  0.4099,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (80, 88)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[24.5993, 37.5041, 49.1986, 75.0083]])\n",
      "xywhn: tensor([[0.2795, 0.4688, 0.5591, 0.9376]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 49.1986, 75.0083]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.5591, 0.9376]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4422])\n",
      "data: tensor([[ 27.5760,   0.0000, 118.7512, 110.3575,   0.4422,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.1636,  55.1787,  91.1752, 110.3575]])\n",
      "xywhn: tensor([[0.5948, 0.4637, 0.7413, 0.9274]])\n",
      "xyxy: tensor([[ 27.5760,   0.0000, 118.7512, 110.3575]])\n",
      "xyxyn: tensor([[0.2242, 0.0000, 0.9655, 0.9274]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 89.7ms\n",
      "Speed: 3.5ms preprocess, 89.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 608x640 1 -1, 140.4ms\n",
      "Speed: 9.5ms preprocess, 140.4ms inference, 15.6ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x640 1 -1, 147.6ms\n",
      "Speed: 2.0ms preprocess, 147.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4130])\n",
      "data: tensor([[ 0.0000,  0.0000, 48.8064, 74.8176,  0.4130,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (80, 88)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[24.4032, 37.4088, 48.8064, 74.8176]])\n",
      "xywhn: tensor([[0.2773, 0.4676, 0.5546, 0.9352]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 48.8064, 74.8176]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.5546, 0.9352]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4428])\n",
      "data: tensor([[ 26.9460,   0.0000, 118.6413, 110.1678,   0.4428,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.7936,  55.0839,  91.6953, 110.1678]])\n",
      "xywhn: tensor([[0.5918, 0.4629, 0.7455, 0.9258]])\n",
      "xyxy: tensor([[ 26.9460,   0.0000, 118.6413, 110.1678]])\n",
      "xyxyn: tensor([[0.2191, 0.0000, 0.9646, 0.9258]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 105.0ms\n",
      "Speed: 2.5ms preprocess, 105.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 608x640 1 -1, 155.7ms\n",
      "Speed: 2.5ms preprocess, 155.7ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x640 1 -1, 1 1, 153.6ms\n",
      "Speed: 4.7ms preprocess, 153.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4953])\n",
      "data: tensor([[ 0.0000,  0.0000, 50.0312, 75.4106,  0.4953,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 89)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[25.0156, 37.7053, 50.0312, 75.4106]])\n",
      "xywhn: tensor([[0.2811, 0.4655, 0.5621, 0.9310]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 50.0312, 75.4106]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.5621, 0.9310]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4668])\n",
      "data: tensor([[ 27.3817,   0.0000, 120.1195, 110.1689,   0.4668,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.7506,  55.0844,  92.7378, 110.1689]])\n",
      "xywhn: tensor([[0.5948, 0.4590, 0.7479, 0.9181]])\n",
      "xyxy: tensor([[ 27.3817,   0.0000, 120.1195, 110.1689]])\n",
      "xyxyn: tensor([[0.2208, 0.0000, 0.9687, 0.9181]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3017])\n",
      "data: tensor([[ 0.0000, 31.6310, 13.7270, 53.6143,  0.3017,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 6.8635, 42.6226, 13.7270, 21.9833]])\n",
      "xywhn: tensor([[0.0554, 0.3552, 0.1107, 0.1832]])\n",
      "xyxy: tensor([[ 0.0000, 31.6310, 13.7270, 53.6143]])\n",
      "xyxyn: tensor([[0.0000, 0.2636, 0.1107, 0.4468]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 102.7ms\n",
      "Speed: 0.0ms preprocess, 102.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 608x640 1 -1, 137.1ms\n",
      "Speed: 5.8ms preprocess, 137.1ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x640 1 -1, 1 1, 180.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5159])\n",
      "data: tensor([[ 0.0000,  0.0000, 48.6314, 77.6098,  0.5159,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 88)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[24.3157, 38.8049, 48.6314, 77.6098]])\n",
      "xywhn: tensor([[0.2763, 0.4791, 0.5526, 0.9581]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 48.6314, 77.6098]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.5526, 0.9581]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4814])\n",
      "data: tensor([[ 31.1811,   0.0000, 122.4499, 110.2152,   0.4814,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.8155,  55.1076,  91.2688, 110.2152]])\n",
      "xywhn: tensor([[0.6195, 0.4592, 0.7360, 0.9185]])\n",
      "xyxy: tensor([[ 31.1811,   0.0000, 122.4499, 110.2152]])\n",
      "xyxyn: tensor([[0.2515, 0.0000, 0.9875, 0.9185]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3042])\n",
      "data: tensor([[ 0.0000, 31.6171, 13.7112, 53.5182,  0.3042,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 6.8556, 42.5676, 13.7112, 21.9010]])\n",
      "xywhn: tensor([[0.0553, 0.3547, 0.1106, 0.1825]])\n",
      "xyxy: tensor([[ 0.0000, 31.6171, 13.7112, 53.5182]])\n",
      "xyxyn: tensor([[0.0000, 0.2635, 0.1106, 0.4460]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 6.7ms preprocess, 180.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 117.4ms\n",
      "Speed: 0.0ms preprocess, 117.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 608x640 1 -1, 173.3ms\n",
      "Speed: 7.5ms preprocess, 173.3ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x640 1 -1, 1 1, 157.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5100])\n",
      "data: tensor([[ 0.0000,  0.0000, 49.0264, 77.8331,  0.5100,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 88)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[24.5132, 38.9165, 49.0264, 77.8331]])\n",
      "xywhn: tensor([[0.2786, 0.4805, 0.5571, 0.9609]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 49.0264, 77.8331]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.5571, 0.9609]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4760])\n",
      "data: tensor([[ 28.8715,   0.0000, 119.8616, 110.0290,   0.4760,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.3666,  55.0145,  90.9901, 110.0290]])\n",
      "xywhn: tensor([[0.5997, 0.4585, 0.7338, 0.9169]])\n",
      "xyxy: tensor([[ 28.8715,   0.0000, 119.8616, 110.0290]])\n",
      "xyxyn: tensor([[0.2328, 0.0000, 0.9666, 0.9169]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3072])\n",
      "data: tensor([[ 0.0000, 31.6366, 13.7009, 53.5389,  0.3072,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 6.8504, 42.5877, 13.7009, 21.9022]])\n",
      "xywhn: tensor([[0.0552, 0.3549, 0.1105, 0.1825]])\n",
      "xyxy: tensor([[ 0.0000, 31.6366, 13.7009, 53.5389]])\n",
      "xyxyn: tensor([[0.0000, 0.2636, 0.1105, 0.4462]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 13.7ms preprocess, 157.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 102.4ms\n",
      "Speed: 0.0ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 158.8ms\n",
      "Speed: 16.1ms preprocess, 158.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 -1, 159.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4424])\n",
      "data: tensor([[ 0.0000,  0.0000, 50.3872, 76.1439,  0.4424,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (82, 84)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[25.1936, 38.0719, 50.3872, 76.1439]])\n",
      "xywhn: tensor([[0.2999, 0.4643, 0.5998, 0.9286]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 50.3872, 76.1439]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.5998, 0.9286]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5091])\n",
      "data: tensor([[ 32.8537,   0.0000, 123.3670, 110.7052,   0.5091,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 78.1103,  55.3526,  90.5133, 110.7052]])\n",
      "xywhn: tensor([[0.6249, 0.4613, 0.7241, 0.9225]])\n",
      "xyxy: tensor([[ 32.8537,   0.0000, 123.3670, 110.7052]])\n",
      "xyxyn: tensor([[0.2628, 0.0000, 0.9869, 0.9225]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 7.0ms preprocess, 159.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 126.1ms\n",
      "Speed: 4.0ms preprocess, 126.1ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 180.7ms\n",
      "Speed: 7.4ms preprocess, 180.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 -1, 145.1ms\n",
      "Speed: 8.1ms preprocess, 145.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4578])\n",
      "data: tensor([[ 0.9141,  8.1763, 51.1432, 80.2477,  0.4578,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (82, 79)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[26.0286, 44.2120, 50.2291, 72.0714]])\n",
      "xywhn: tensor([[0.3295, 0.5392, 0.6358, 0.8789]])\n",
      "xyxy: tensor([[ 0.9141,  8.1763, 51.1432, 80.2477]])\n",
      "xyxyn: tensor([[0.0116, 0.0997, 0.6474, 0.9786]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5026])\n",
      "data: tensor([[ 32.1355,   0.0000, 123.4799, 110.4756,   0.5026,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 77.8077,  55.2378,  91.3444, 110.4756]])\n",
      "xywhn: tensor([[0.6225, 0.4642, 0.7308, 0.9284]])\n",
      "xyxy: tensor([[ 32.1355,   0.0000, 123.4799, 110.4756]])\n",
      "xyxyn: tensor([[0.2571, 0.0000, 0.9878, 0.9284]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 102.6ms\n",
      "Speed: 3.4ms preprocess, 102.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x608 1 -1, 142.8ms\n",
      "Speed: 14.1ms preprocess, 142.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 640x640 1 -1, 139.2ms\n",
      "Speed: 18.4ms preprocess, 139.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5664])\n",
      "data: tensor([[ 3.0227,  0.0000, 52.2197, 79.3583,  0.5664,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (82, 74)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[27.6212, 39.6792, 49.1969, 79.3583]])\n",
      "xywhn: tensor([[0.3733, 0.4839, 0.6648, 0.9678]])\n",
      "xyxy: tensor([[ 3.0227,  0.0000, 52.2197, 79.3583]])\n",
      "xyxyn: tensor([[0.0408, 0.0000, 0.7057, 0.9678]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4872])\n",
      "data: tensor([[ 33.4498,   0.0000, 123.4548, 110.7147,   0.4872,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 78.4523,  55.3573,  90.0049, 110.7147]])\n",
      "xywhn: tensor([[0.6276, 0.4652, 0.7200, 0.9304]])\n",
      "xyxy: tensor([[ 33.4498,   0.0000, 123.4548, 110.7147]])\n",
      "xyxyn: tensor([[0.2676, 0.0000, 0.9876, 0.9304]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 95.8ms\n",
      "Speed: 2.1ms preprocess, 95.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x576 1 -1, 145.2ms\n",
      "Speed: 5.9ms preprocess, 145.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x640 1 -1, 1 1, 136.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6071])\n",
      "data: tensor([[ 2.5524,  6.2743, 51.1205, 78.8864,  0.6071,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 70)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[26.8364, 42.5804, 48.5682, 72.6121]])\n",
      "xywhn: tensor([[0.3834, 0.5257, 0.6938, 0.8964]])\n",
      "xyxy: tensor([[ 2.5524,  6.2743, 51.1205, 78.8864]])\n",
      "xyxyn: tensor([[0.0365, 0.0775, 0.7303, 0.9739]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5207])\n",
      "data: tensor([[ 33.8230,   0.0000, 123.2867, 111.6863,   0.5207,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 78.5549,  55.8432,  89.4637, 111.6863]])\n",
      "xywhn: tensor([[0.6284, 0.4654, 0.7157, 0.9307]])\n",
      "xyxy: tensor([[ 33.8230,   0.0000, 123.2867, 111.6863]])\n",
      "xyxyn: tensor([[0.2706, 0.0000, 0.9863, 0.9307]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.2523])\n",
      "data: tensor([[ 0.0000, 31.8697, 14.7173, 47.4113,  0.2523,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 7.3586, 39.6405, 14.7173, 15.5416]])\n",
      "xywhn: tensor([[0.0589, 0.3303, 0.1177, 0.1295]])\n",
      "xyxy: tensor([[ 0.0000, 31.8697, 14.7173, 47.4113]])\n",
      "xyxyn: tensor([[0.0000, 0.2656, 0.1177, 0.3951]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 5.4ms preprocess, 136.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 99.1ms\n",
      "Speed: 0.0ms preprocess, 99.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 2 -1s, 126.9ms\n",
      "Speed: 4.8ms preprocess, 126.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x640 1 -1, 142.5ms\n",
      "Speed: 7.9ms preprocess, 142.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5599])\n",
      "data: tensor([[ 0.8718, 10.4873, 49.8368, 79.2158,  0.5599,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 66)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[25.3543, 44.8516, 48.9649, 68.7285]])\n",
      "xywhn: tensor([[0.3842, 0.5537, 0.7419, 0.8485]])\n",
      "xyxy: tensor([[ 0.8718, 10.4873, 49.8368, 79.2158]])\n",
      "xyxyn: tensor([[0.0132, 0.1295, 0.7551, 0.9780]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2730])\n",
      "data: tensor([[25.3483,  0.2975, 50.1081, 66.2655,  0.2730,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 66)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[37.7282, 33.2815, 24.7598, 65.9681]])\n",
      "xywhn: tensor([[0.5716, 0.4109, 0.3751, 0.8144]])\n",
      "xyxy: tensor([[25.3483,  0.2975, 50.1081, 66.2655]])\n",
      "xyxyn: tensor([[0.3841, 0.0037, 0.7592, 0.8181]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4982])\n",
      "data: tensor([[ 33.0448,   0.0000, 122.3424, 110.0933,   0.4982,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 77.6936,  55.0466,  89.2976, 110.0933]])\n",
      "xywhn: tensor([[0.6266, 0.4626, 0.7201, 0.9252]])\n",
      "xyxy: tensor([[ 33.0448,   0.0000, 122.3424, 110.0933]])\n",
      "xyxyn: tensor([[0.2665, 0.0000, 0.9866, 0.9252]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 118.5ms\n",
      "Speed: 0.0ms preprocess, 118.5ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x512 1 -1, 139.3ms\n",
      "Speed: 2.2ms preprocess, 139.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x640 1 -1, 157.2ms\n",
      "Speed: 8.4ms preprocess, 157.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3261])\n",
      "data: tensor([[ 0.7761,  3.2246, 48.3946, 79.5651,  0.3261,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 63)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[24.5854, 41.3949, 47.6185, 76.3405]])\n",
      "xywhn: tensor([[0.3902, 0.5110, 0.7558, 0.9425]])\n",
      "xyxy: tensor([[ 0.7761,  3.2246, 48.3946, 79.5651]])\n",
      "xyxyn: tensor([[0.0123, 0.0398, 0.7682, 0.9823]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5119])\n",
      "data: tensor([[ 33.1195,   0.0000, 122.3237, 109.9953,   0.5119,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 77.7216,  54.9977,  89.2042, 109.9953]])\n",
      "xywhn: tensor([[0.6268, 0.4622, 0.7194, 0.9243]])\n",
      "xyxy: tensor([[ 33.1195,   0.0000, 122.3237, 109.9953]])\n",
      "xyxyn: tensor([[0.2671, 0.0000, 0.9865, 0.9243]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 109.0ms\n",
      "Speed: 2.3ms preprocess, 109.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x512 3 -1s, 125.9ms\n",
      "Speed: 0.0ms preprocess, 125.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x640 1 -1, 147.3ms\n",
      "Speed: 3.6ms preprocess, 147.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4299])\n",
      "data: tensor([[ 0.0000,  0.9209, 50.2489, 80.1615,  0.4299,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 62)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[25.1244, 40.5412, 50.2489, 79.2406]])\n",
      "xywhn: tensor([[0.4052, 0.5005, 0.8105, 0.9783]])\n",
      "xyxy: tensor([[ 0.0000,  0.9209, 50.2489, 80.1615]])\n",
      "xyxyn: tensor([[0.0000, 0.0114, 0.8105, 0.9896]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2623])\n",
      "data: tensor([[20.2734,  0.0000, 50.4466, 72.6585,  0.2623,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 62)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[35.3600, 36.3292, 30.1733, 72.6585]])\n",
      "xywhn: tensor([[0.5703, 0.4485, 0.4867, 0.8970]])\n",
      "xyxy: tensor([[20.2734,  0.0000, 50.4466, 72.6585]])\n",
      "xyxyn: tensor([[0.3270, 0.0000, 0.8137, 0.8970]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2615])\n",
      "data: tensor([[ 0.3651, 10.7057, 32.2327, 79.2572,  0.2615,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 62)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[16.2989, 44.9815, 31.8676, 68.5515]])\n",
      "xywhn: tensor([[0.2629, 0.5553, 0.5140, 0.8463]])\n",
      "xyxy: tensor([[ 0.3651, 10.7057, 32.2327, 79.2572]])\n",
      "xyxyn: tensor([[0.0059, 0.1322, 0.5199, 0.9785]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4932])\n",
      "data: tensor([[ 32.1621,   0.0000, 122.3703, 109.8288,   0.4932,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 77.2662,  54.9144,  90.2082, 109.8288]])\n",
      "xywhn: tensor([[0.6231, 0.4615, 0.7275, 0.9229]])\n",
      "xyxy: tensor([[ 32.1621,   0.0000, 122.3703, 109.8288]])\n",
      "xyxyn: tensor([[0.2594, 0.0000, 0.9869, 0.9229]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 102.3ms\n",
      "Speed: 0.0ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x480 2 -1s, 119.1ms\n",
      "Speed: 4.7ms preprocess, 119.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 -1, 148.6ms\n",
      "Speed: 10.1ms preprocess, 148.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4670])\n",
      "data: tensor([[ 0.9164,  7.5399, 47.8893, 79.1455,  0.4670,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[24.4028, 43.3427, 46.9728, 71.6056]])\n",
      "xywhn: tensor([[0.4067, 0.5351, 0.7829, 0.8840]])\n",
      "xyxy: tensor([[ 0.9164,  7.5399, 47.8893, 79.1455]])\n",
      "xyxyn: tensor([[0.0153, 0.0931, 0.7982, 0.9771]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3531])\n",
      "data: tensor([[2.2162e-02, 1.3472e+01, 3.1514e+01, 7.8327e+01, 3.5309e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.7679, 45.8994, 31.4914, 64.8547]])\n",
      "xywhn: tensor([[0.2628, 0.5667, 0.5249, 0.8007]])\n",
      "xyxy: tensor([[2.2162e-02, 1.3472e+01, 3.1514e+01, 7.8327e+01]])\n",
      "xyxyn: tensor([[3.6937e-04, 1.6632e-01, 5.2523e-01, 9.6700e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4831])\n",
      "data: tensor([[ 31.7374,   0.0000, 122.4656, 109.8278,   0.4831,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 77.1015,  54.9139,  90.7282, 109.8278]])\n",
      "xywhn: tensor([[0.6218, 0.4615, 0.7317, 0.9229]])\n",
      "xyxy: tensor([[ 31.7374,   0.0000, 122.4656, 109.8278]])\n",
      "xyxyn: tensor([[0.2559, 0.0000, 0.9876, 0.9229]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 2 books, 116.5ms\n",
      "Speed: 2.5ms preprocess, 116.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x480 2 -1s, 125.0ms\n",
      "Speed: 4.9ms preprocess, 125.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 -1, 149.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4604])\n",
      "data: tensor([[ 0.2663,  8.9202, 44.2943, 78.2141,  0.4604,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.2803, 43.5672, 44.0280, 69.2939]])\n",
      "xywhn: tensor([[0.3776, 0.5379, 0.7462, 0.8555]])\n",
      "xyxy: tensor([[ 0.2663,  8.9202, 44.2943, 78.2141]])\n",
      "xyxyn: tensor([[0.0045, 0.1101, 0.7508, 0.9656]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3684])\n",
      "data: tensor([[ 0.0000, 11.6625, 31.4768, 78.0972,  0.3684,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 59)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.7384, 44.8798, 31.4768, 66.4346]])\n",
      "xywhn: tensor([[0.2668, 0.5541, 0.5335, 0.8202]])\n",
      "xyxy: tensor([[ 0.0000, 11.6625, 31.4768, 78.0972]])\n",
      "xyxyn: tensor([[0.0000, 0.1440, 0.5335, 0.9642]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4744])\n",
      "data: tensor([[ 30.4880,   0.0000, 122.4126, 110.1792,   0.4744,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.4503,  55.0896,  91.9245, 110.1792]])\n",
      "xywhn: tensor([[0.6165, 0.4629, 0.7413, 0.9259]])\n",
      "xyxy: tensor([[ 30.4880,   0.0000, 122.4126, 110.1792]])\n",
      "xyxyn: tensor([[0.2459, 0.0000, 0.9872, 0.9259]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 18.4ms preprocess, 149.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 95.3ms\n",
      "Speed: 2.0ms preprocess, 95.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x480 2 -1s, 141.5ms\n",
      "Speed: 0.0ms preprocess, 141.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 -1, 143.5ms\n",
      "Speed: 3.6ms preprocess, 143.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4683])\n",
      "data: tensor([[ 0.1656,  8.3500, 43.2291, 78.4717,  0.4683,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[21.6974, 43.4109, 43.0636, 70.1217]])\n",
      "xywhn: tensor([[0.3741, 0.5359, 0.7425, 0.8657]])\n",
      "xyxy: tensor([[ 0.1656,  8.3500, 43.2291, 78.4717]])\n",
      "xyxyn: tensor([[0.0029, 0.1031, 0.7453, 0.9688]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3962])\n",
      "data: tensor([[ 0.0000, 11.0857, 31.6056, 78.3611,  0.3962,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.8028, 44.7234, 31.6056, 67.2754]])\n",
      "xywhn: tensor([[0.2725, 0.5521, 0.5449, 0.8306]])\n",
      "xyxy: tensor([[ 0.0000, 11.0857, 31.6056, 78.3611]])\n",
      "xyxyn: tensor([[0.0000, 0.1369, 0.5449, 0.9674]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4596])\n",
      "data: tensor([[ 31.3512,   0.0000, 122.0002, 111.3449,   0.4596,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.6757,  55.6725,  90.6489, 111.3449]])\n",
      "xywhn: tensor([[0.6184, 0.4639, 0.7310, 0.9279]])\n",
      "xyxy: tensor([[ 31.3512,   0.0000, 122.0002, 111.3449]])\n",
      "xyxyn: tensor([[0.2528, 0.0000, 0.9839, 0.9279]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 109.1ms\n",
      "Speed: 5.1ms preprocess, 109.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x480 2 -1s, 125.8ms\n",
      "Speed: 2.1ms preprocess, 125.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 -1, 151.5ms\n",
      "Speed: 6.4ms preprocess, 151.5ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4478])\n",
      "data: tensor([[ 0.0000, 12.6079, 32.0892, 78.7149,  0.4478,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[16.0446, 45.6614, 32.0892, 66.1070]])\n",
      "xywhn: tensor([[0.2766, 0.5637, 0.5533, 0.8161]])\n",
      "xyxy: tensor([[ 0.0000, 12.6079, 32.0892, 78.7149]])\n",
      "xyxyn: tensor([[0.0000, 0.1557, 0.5533, 0.9718]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4433])\n",
      "data: tensor([[ 0.2203,  9.9127, 42.9992, 78.5102,  0.4433,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 58)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[21.6097, 44.2115, 42.7788, 68.5974]])\n",
      "xywhn: tensor([[0.3726, 0.5458, 0.7376, 0.8469]])\n",
      "xyxy: tensor([[ 0.2203,  9.9127, 42.9992, 78.5102]])\n",
      "xyxyn: tensor([[0.0038, 0.1224, 0.7414, 0.9693]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4654])\n",
      "data: tensor([[ 28.3096,   0.0000, 119.1744, 109.5860,   0.4654,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.7420,  54.7930,  90.8649, 109.5860]])\n",
      "xywhn: tensor([[0.5947, 0.4566, 0.7328, 0.9132]])\n",
      "xyxy: tensor([[ 28.3096,   0.0000, 119.1744, 109.5860]])\n",
      "xyxyn: tensor([[0.2283, 0.0000, 0.9611, 0.9132]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 110.8ms\n",
      "Speed: 3.5ms preprocess, 110.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x480 2 -1s, 129.1ms\n",
      "Speed: 4.4ms preprocess, 129.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 -1, 149.7ms\n",
      "Speed: 5.2ms preprocess, 149.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3888])\n",
      "data: tensor([[ 0.0000, 10.3504, 31.0774, 78.3540,  0.3888,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 57)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.5387, 44.3522, 31.0774, 68.0035]])\n",
      "xywhn: tensor([[0.2726, 0.5476, 0.5452, 0.8395]])\n",
      "xyxy: tensor([[ 0.0000, 10.3504, 31.0774, 78.3540]])\n",
      "xyxyn: tensor([[0.0000, 0.1278, 0.5452, 0.9673]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3359])\n",
      "data: tensor([[ 0.0000,  5.9235, 41.9551, 78.5781,  0.3359,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 57)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[20.9775, 42.2508, 41.9551, 72.6546]])\n",
      "xywhn: tensor([[0.3680, 0.5216, 0.7361, 0.8970]])\n",
      "xyxy: tensor([[ 0.0000,  5.9235, 41.9551, 78.5781]])\n",
      "xyxyn: tensor([[0.0000, 0.0731, 0.7361, 0.9701]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4552])\n",
      "data: tensor([[ 28.2651,   0.0000, 119.2024, 109.7130,   0.4552,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.7337,  54.8565,  90.9373, 109.7130]])\n",
      "xywhn: tensor([[0.5946, 0.4571, 0.7334, 0.9143]])\n",
      "xyxy: tensor([[ 28.2651,   0.0000, 119.2024, 109.7130]])\n",
      "xyxyn: tensor([[0.2279, 0.0000, 0.9613, 0.9143]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 93.5ms\n",
      "Speed: 0.0ms preprocess, 93.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 -1, 144.9ms\n",
      "Speed: 0.0ms preprocess, 144.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 2 -1s, 157.8ms\n",
      "Speed: 3.1ms preprocess, 157.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2991])\n",
      "data: tensor([[ 0.0000,  2.3159, 47.5740, 78.8318,  0.2991,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (81, 56)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.7870, 40.5738, 47.5740, 76.5160]])\n",
      "xywhn: tensor([[0.4248, 0.5009, 0.8495, 0.9446]])\n",
      "xyxy: tensor([[ 0.0000,  2.3159, 47.5740, 78.8318]])\n",
      "xyxyn: tensor([[0.0000, 0.0286, 0.8495, 0.9732]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4610])\n",
      "data: tensor([[ 30.2146,   0.0000, 122.2903, 109.3912,   0.4610,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.2524,  54.6956,  92.0757, 109.3912]])\n",
      "xywhn: tensor([[0.6149, 0.4558, 0.7425, 0.9116]])\n",
      "xyxy: tensor([[ 30.2146,   0.0000, 122.2903, 109.3912]])\n",
      "xyxyn: tensor([[0.2437, 0.0000, 0.9862, 0.9116]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2610])\n",
      "data: tensor([[  0.2461,   4.5314,  28.5430, 110.9277,   0.2610,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 14.3945,  57.7295,  28.2969, 106.3963]])\n",
      "xywhn: tensor([[0.1161, 0.4811, 0.2282, 0.8866]])\n",
      "xyxy: tensor([[  0.2461,   4.5314,  28.5430, 110.9277]])\n",
      "xyxyn: tensor([[0.0020, 0.0378, 0.2302, 0.9244]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 97.4ms\n",
      "Speed: 0.7ms preprocess, 97.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x448 (no detections), 115.7ms\n",
      "Speed: 2.8ms preprocess, 115.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 -1, 157.4ms\n",
      "Speed: 8.4ms preprocess, 157.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 101.8ms\n",
      "Speed: 0.0ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4446])\n",
      "data: tensor([[ 28.3157,   0.0000, 118.0468, 110.9473,   0.4446,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.1813,  55.4736,  89.7312, 110.9473]])\n",
      "xywhn: tensor([[0.5950, 0.4623, 0.7295, 0.9246]])\n",
      "xyxy: tensor([[ 28.3157,   0.0000, 118.0468, 110.9473]])\n",
      "xyxyn: tensor([[0.2302, 0.0000, 0.9597, 0.9246]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x448 (no detections), 109.9ms\n",
      "Speed: 1.8ms preprocess, 109.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 -1, 156.6ms\n",
      "Speed: 10.2ms preprocess, 156.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 111.2ms\n",
      "Speed: 0.0ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4588])\n",
      "data: tensor([[ 27.6529,   0.0000, 118.1306, 110.8598,   0.4588,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.8917,  55.4299,  90.4777, 110.8598]])\n",
      "xywhn: tensor([[0.5926, 0.4619, 0.7356, 0.9238]])\n",
      "xyxy: tensor([[ 27.6529,   0.0000, 118.1306, 110.8598]])\n",
      "xyxyn: tensor([[0.2248, 0.0000, 0.9604, 0.9238]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x448 (no detections), 107.1ms\n",
      "Speed: 0.0ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 -1, 142.2ms\n",
      "Speed: 8.5ms preprocess, 142.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 103.1ms\n",
      "Speed: 1.0ms preprocess, 103.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4483])\n",
      "data: tensor([[ 27.6629,   0.0000, 117.9588, 110.9367,   0.4483,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.8109,  55.4683,  90.2959, 110.9367]])\n",
      "xywhn: tensor([[0.5920, 0.4622, 0.7341, 0.9245]])\n",
      "xyxy: tensor([[ 27.6629,   0.0000, 117.9588, 110.9367]])\n",
      "xyxyn: tensor([[0.2249, 0.0000, 0.9590, 0.9245]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x448 (no detections), 123.1ms\n",
      "Speed: 0.0ms preprocess, 123.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 -1, 152.3ms\n",
      "Speed: 10.5ms preprocess, 152.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 91.5ms\n",
      "Speed: 2.4ms preprocess, 91.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4465])\n",
      "data: tensor([[ 26.7149,   0.0000, 118.2582, 110.9058,   0.4465,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.4865,  55.4529,  91.5432, 110.9058]])\n",
      "xywhn: tensor([[0.5893, 0.4621, 0.7443, 0.9242]])\n",
      "xyxy: tensor([[ 26.7149,   0.0000, 118.2582, 110.9058]])\n",
      "xyxyn: tensor([[0.2172, 0.0000, 0.9614, 0.9242]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x448 1 -1, 101.4ms\n",
      "Speed: 2.6ms preprocess, 101.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 2 -1s, 140.0ms\n",
      "Speed: 11.0ms preprocess, 140.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2892])\n",
      "data: tensor([[ 0.0000, 12.5520, 30.6470, 79.2306,  0.2892,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (82, 55)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[15.3235, 45.8913, 30.6470, 66.6787]])\n",
      "xywhn: tensor([[0.2786, 0.5597, 0.5572, 0.8132]])\n",
      "xyxy: tensor([[ 0.0000, 12.5520, 30.6470, 79.2306]])\n",
      "xyxyn: tensor([[0.0000, 0.1531, 0.5572, 0.9662]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4069])\n",
      "data: tensor([[ 26.2868,   0.0000, 117.6274, 109.8400,   0.4069,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 71.9571,  54.9200,  91.3406, 109.8400]])\n",
      "xywhn: tensor([[0.5898, 0.4577, 0.7487, 0.9153]])\n",
      "xyxy: tensor([[ 26.2868,   0.0000, 117.6274, 109.8400]])\n",
      "xyxyn: tensor([[0.2155, 0.0000, 0.9642, 0.9153]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2556])\n",
      "data: tensor([[  0.3603,   0.0000,  26.9363, 110.5490,   0.2556,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 13.6483,  55.2745,  26.5760, 110.5490]])\n",
      "xywhn: tensor([[0.1119, 0.4606, 0.2178, 0.9212]])\n",
      "xyxy: tensor([[  0.3603,   0.0000,  26.9363, 110.5490]])\n",
      "xyxyn: tensor([[0.0030, 0.0000, 0.2208, 0.9212]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 101.9ms\n",
      "Speed: 2.5ms preprocess, 101.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 -1, 108.8ms\n",
      "Speed: 17.4ms preprocess, 108.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 2 -1s, 153.5ms\n",
      "Speed: 4.7ms preprocess, 153.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4029])\n",
      "data: tensor([[ 0.0000,  0.0000, 45.1891, 80.5412,  0.4029,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (82, 55)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.5945, 40.2706, 45.1891, 80.5412]])\n",
      "xywhn: tensor([[0.4108, 0.4911, 0.8216, 0.9822]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 45.1891, 80.5412]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8216, 0.9822]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4019])\n",
      "data: tensor([[ 26.6493,   0.0000, 117.8168, 109.7309,   0.4019,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.2331,  54.8654,  91.1675, 109.7309]])\n",
      "xywhn: tensor([[0.5921, 0.4572, 0.7473, 0.9144]])\n",
      "xyxy: tensor([[ 26.6493,   0.0000, 117.8168, 109.7309]])\n",
      "xyxyn: tensor([[0.2184, 0.0000, 0.9657, 0.9144]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2594])\n",
      "data: tensor([[  0.3587,   0.0000,  26.9282, 110.6306,   0.2594,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 13.6435,  55.3153,  26.5695, 110.6306]])\n",
      "xywhn: tensor([[0.1118, 0.4610, 0.2178, 0.9219]])\n",
      "xyxy: tensor([[  0.3587,   0.0000,  26.9282, 110.6306]])\n",
      "xyxyn: tensor([[0.0029, 0.0000, 0.2207, 0.9219]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 93.5ms\n",
      "Speed: 5.2ms preprocess, 93.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 -1, 117.5ms\n",
      "Speed: 4.3ms preprocess, 117.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 -1, 155.0ms\n",
      "Speed: 15.0ms preprocess, 155.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5252])\n",
      "data: tensor([[ 0.0000,  0.2720, 47.4997, 80.6802,  0.5252,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (82, 54)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[23.7499, 40.4761, 47.4997, 80.4082]])\n",
      "xywhn: tensor([[0.4398, 0.4936, 0.8796, 0.9806]])\n",
      "xyxy: tensor([[ 0.0000,  0.2720, 47.4997, 80.6802]])\n",
      "xyxyn: tensor([[0.0000, 0.0033, 0.8796, 0.9839]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4002])\n",
      "data: tensor([[ 26.6275,   0.0000, 117.7747, 109.8106,   0.4002,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.2011,  54.9053,  91.1472, 109.8106]])\n",
      "xywhn: tensor([[0.5918, 0.4575, 0.7471, 0.9151]])\n",
      "xyxy: tensor([[ 26.6275,   0.0000, 117.7747, 109.8106]])\n",
      "xyxyn: tensor([[0.2183, 0.0000, 0.9654, 0.9151]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 104.5ms\n",
      "Speed: 3.2ms preprocess, 104.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x416 1 -1, 111.7ms\n",
      "Speed: 2.2ms preprocess, 111.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 2 -1s, 156.3ms\n",
      "Speed: 1.9ms preprocess, 156.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3284])\n",
      "data: tensor([[ 0.0000,  0.1213, 43.8337, 81.4009,  0.3284,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (83, 54)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[21.9169, 40.7611, 43.8337, 81.2796]])\n",
      "xywhn: tensor([[0.4059, 0.4911, 0.8117, 0.9793]])\n",
      "xyxy: tensor([[ 0.0000,  0.1213, 43.8337, 81.4009]])\n",
      "xyxyn: tensor([[0.0000, 0.0015, 0.8117, 0.9807]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4022])\n",
      "data: tensor([[ 26.5369,   0.0000, 117.7953, 110.1198,   0.4022,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.1661,  55.0599,  91.2584, 110.1198]])\n",
      "xywhn: tensor([[0.5915, 0.4588, 0.7480, 0.9177]])\n",
      "xyxy: tensor([[ 26.5369,   0.0000, 117.7953, 110.1198]])\n",
      "xyxyn: tensor([[0.2175, 0.0000, 0.9655, 0.9177]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3185])\n",
      "data: tensor([[  0.3465,   0.0000,  26.8293, 110.8060,   0.3185,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 13.5879,  55.4030,  26.4828, 110.8060]])\n",
      "xywhn: tensor([[0.1114, 0.4617, 0.2171, 0.9234]])\n",
      "xyxy: tensor([[  0.3465,   0.0000,  26.8293, 110.8060]])\n",
      "xyxyn: tensor([[0.0028, 0.0000, 0.2199, 0.9234]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 99.4ms\n",
      "Speed: 3.4ms preprocess, 99.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x416 1 -1, 97.2ms\n",
      "Speed: 0.0ms preprocess, 97.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 2 -1s, 167.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3420])\n",
      "data: tensor([[ 0.0000,  0.0000, 45.3475, 80.3690,  0.3420,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (83, 54)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.6737, 40.1845, 45.3475, 80.3690]])\n",
      "xywhn: tensor([[0.4199, 0.4842, 0.8398, 0.9683]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 45.3475, 80.3690]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8398, 0.9683]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4000])\n",
      "data: tensor([[ 26.6377,   0.0000, 117.7588, 109.9679,   0.4000,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.1982,  54.9839,  91.1211, 109.9679]])\n",
      "xywhn: tensor([[0.5918, 0.4582, 0.7469, 0.9164]])\n",
      "xyxy: tensor([[ 26.6377,   0.0000, 117.7588, 109.9679]])\n",
      "xyxyn: tensor([[0.2183, 0.0000, 0.9652, 0.9164]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2890])\n",
      "data: tensor([[  0.3503,   0.0000,  26.8377, 110.6405,   0.2890,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 13.5940,  55.3203,  26.4874, 110.6405]])\n",
      "xywhn: tensor([[0.1114, 0.4610, 0.2171, 0.9220]])\n",
      "xyxy: tensor([[  0.3503,   0.0000,  26.8377, 110.6405]])\n",
      "xyxyn: tensor([[0.0029, 0.0000, 0.2200, 0.9220]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.4ms preprocess, 167.7ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 123.6ms\n",
      "Speed: 3.9ms preprocess, 123.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 1 -1, 116.9ms\n",
      "Speed: 6.6ms preprocess, 116.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 -1, 171.1ms\n",
      "Speed: 6.1ms preprocess, 171.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2954])\n",
      "data: tensor([[ 0.0000,  0.0000, 44.3891, 81.4038,  0.2954,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (83, 54)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[22.1945, 40.7019, 44.3891, 81.4038]])\n",
      "xywhn: tensor([[0.4110, 0.4904, 0.8220, 0.9808]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 44.3891, 81.4038]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.8220, 0.9808]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3972])\n",
      "data: tensor([[ 26.5801,   0.0000, 117.8292, 109.9477,   0.3972,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.2047,  54.9739,  91.2491, 109.9477]])\n",
      "xywhn: tensor([[0.5918, 0.4581, 0.7479, 0.9162]])\n",
      "xyxy: tensor([[ 26.5801,   0.0000, 117.8292, 109.9477]])\n",
      "xyxyn: tensor([[0.2179, 0.0000, 0.9658, 0.9162]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 107.6ms\n",
      "Speed: 0.7ms preprocess, 107.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x416 (no detections), 131.0ms\n",
      "Speed: 2.0ms preprocess, 131.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 -1, 199.5ms\n",
      "Speed: 3.5ms preprocess, 199.5ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 105.8ms\n",
      "Speed: 0.0ms preprocess, 105.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3212])\n",
      "data: tensor([[ 21.9296,   0.0000, 117.9326, 109.2261,   0.3212,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.9311,  54.6131,  96.0030, 109.2261]])\n",
      "xywhn: tensor([[0.5779, 0.4551, 0.7934, 0.9102]])\n",
      "xyxy: tensor([[ 21.9296,   0.0000, 117.9326, 109.2261]])\n",
      "xyxyn: tensor([[0.1812, 0.0000, 0.9746, 0.9102]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 (no detections), 117.2ms\n",
      "Speed: 2.4ms preprocess, 117.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 -1, 142.4ms\n",
      "Speed: 7.5ms preprocess, 142.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 106.3ms\n",
      "Speed: 3.3ms preprocess, 106.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4173])\n",
      "data: tensor([[ 26.5909,   0.0000, 117.6557, 109.9347,   0.4173,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.1233,  54.9674,  91.0648, 109.9347]])\n",
      "xywhn: tensor([[0.5912, 0.4581, 0.7464, 0.9161]])\n",
      "xyxy: tensor([[ 26.5909,   0.0000, 117.6557, 109.9347]])\n",
      "xyxyn: tensor([[0.2180, 0.0000, 0.9644, 0.9161]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 (no detections), 116.2ms\n",
      "Speed: 2.2ms preprocess, 116.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 -1, 141.6ms\n",
      "Speed: 8.5ms preprocess, 141.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 109.5ms\n",
      "Speed: 0.0ms preprocess, 109.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4133])\n",
      "data: tensor([[ 26.3289,   0.0000, 117.5843, 110.1569,   0.4133,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 71.9566,  55.0784,  91.2554, 110.1569]])\n",
      "xywhn: tensor([[0.5898, 0.4590, 0.7480, 0.9180]])\n",
      "xyxy: tensor([[ 26.3289,   0.0000, 117.5843, 110.1569]])\n",
      "xyxyn: tensor([[0.2158, 0.0000, 0.9638, 0.9180]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 (no detections), 107.9ms\n",
      "Speed: 2.3ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 -1, 141.3ms\n",
      "Speed: 4.8ms preprocess, 141.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 119.9ms\n",
      "Speed: 2.8ms preprocess, 119.9ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4100])\n",
      "data: tensor([[ 26.6201,   0.0000, 117.6497, 110.1713,   0.4100,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.1349,  55.0856,  91.0295, 110.1713]])\n",
      "xywhn: tensor([[0.5913, 0.4590, 0.7461, 0.9181]])\n",
      "xyxy: tensor([[ 26.6201,   0.0000, 117.6497, 110.1713]])\n",
      "xyxyn: tensor([[0.2182, 0.0000, 0.9643, 0.9181]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 (no detections), 112.9ms\n",
      "Speed: 4.0ms preprocess, 112.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 -1, 195.1ms\n",
      "Speed: 5.0ms preprocess, 195.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 125.3ms\n",
      "Speed: 0.0ms preprocess, 125.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4092])\n",
      "data: tensor([[ 26.5200,   0.0000, 117.6889, 110.1977,   0.4092,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.1044,  55.0989,  91.1689, 110.1977]])\n",
      "xywhn: tensor([[0.5910, 0.4592, 0.7473, 0.9183]])\n",
      "xyxy: tensor([[ 26.5200,   0.0000, 117.6889, 110.1977]])\n",
      "xyxyn: tensor([[0.2174, 0.0000, 0.9647, 0.9183]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 (no detections), 151.6ms\n",
      "Speed: 0.0ms preprocess, 151.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 -1, 160.1ms\n",
      "Speed: 7.5ms preprocess, 160.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 137.2ms\n",
      "Speed: 3.0ms preprocess, 137.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3299])\n",
      "data: tensor([[ 21.7142,   0.0000, 117.3542, 109.5616,   0.3299,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.5342,  54.7808,  95.6399, 109.5616]])\n",
      "xywhn: tensor([[0.5747, 0.4565, 0.7904, 0.9130]])\n",
      "xyxy: tensor([[ 21.7142,   0.0000, 117.3542, 109.5616]])\n",
      "xyxyn: tensor([[0.1795, 0.0000, 0.9699, 0.9130]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 (no detections), 139.7ms\n",
      "Speed: 0.0ms preprocess, 139.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 -1, 201.4ms\n",
      "Speed: 6.4ms preprocess, 201.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 136.7ms\n",
      "Speed: 0.5ms preprocess, 136.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3169])\n",
      "data: tensor([[ 21.1359,   0.0000, 117.2976, 109.7702,   0.3169,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.2167,  54.8851,  96.1617, 109.7702]])\n",
      "xywhn: tensor([[0.5720, 0.4574, 0.7947, 0.9148]])\n",
      "xyxy: tensor([[ 21.1359,   0.0000, 117.2976, 109.7702]])\n",
      "xyxyn: tensor([[0.1747, 0.0000, 0.9694, 0.9148]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 (no detections), 140.4ms\n",
      "Speed: 5.9ms preprocess, 140.4ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 -1, 194.7ms\n",
      "Speed: 6.4ms preprocess, 194.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 138.1ms\n",
      "Speed: 6.6ms preprocess, 138.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3219])\n",
      "data: tensor([[ 21.2744,   0.0000, 117.2342, 109.6714,   0.3219,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.2543,  54.8357,  95.9597, 109.6714]])\n",
      "xywhn: tensor([[0.5723, 0.4570, 0.7931, 0.9139]])\n",
      "xyxy: tensor([[ 21.2744,   0.0000, 117.2342, 109.6714]])\n",
      "xyxyn: tensor([[0.1758, 0.0000, 0.9689, 0.9139]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 1 -1, 132.2ms\n",
      "Speed: 5.1ms preprocess, 132.2ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3270])\n",
      "data: tensor([[ 0.0000,  0.0000, 38.4290, 77.6747,  0.3270,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (87, 53)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[19.2145, 38.8374, 38.4290, 77.6747]])\n",
      "xywhn: tensor([[0.3625, 0.4464, 0.7251, 0.8928]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 38.4290, 77.6747]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7251, 0.8928]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 199.7ms\n",
      "Speed: 6.1ms preprocess, 199.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 117.3ms\n",
      "Speed: 4.9ms preprocess, 117.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3231])\n",
      "data: tensor([[ 21.8628,   0.0000, 117.0488, 109.7057,   0.3231,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.4558,  54.8528,  95.1859, 109.7057]])\n",
      "xywhn: tensor([[0.5740, 0.4571, 0.7867, 0.9142]])\n",
      "xyxy: tensor([[ 21.8628,   0.0000, 117.0488, 109.7057]])\n",
      "xyxyn: tensor([[0.1807, 0.0000, 0.9673, 0.9142]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 1 -1, 112.6ms\n",
      "Speed: 0.0ms preprocess, 112.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 -1, 188.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6023])\n",
      "data: tensor([[ 0.0000,  0.0000, 37.8679, 78.5722,  0.6023,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (87, 54)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[18.9339, 39.2861, 37.8679, 78.5722]])\n",
      "xywhn: tensor([[0.3506, 0.4516, 0.7013, 0.9031]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 37.8679, 78.5722]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7013, 0.9031]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3391])\n",
      "data: tensor([[ 22.0241,   0.0000, 117.0287, 109.7101,   0.3391,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.5264,  54.8550,  95.0046, 109.7101]])\n",
      "xywhn: tensor([[0.5746, 0.4571, 0.7852, 0.9143]])\n",
      "xyxy: tensor([[ 22.0241,   0.0000, 117.0287, 109.7101]])\n",
      "xyxyn: tensor([[0.1820, 0.0000, 0.9672, 0.9143]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 17.6ms preprocess, 188.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 138.0ms\n",
      "Speed: 0.0ms preprocess, 138.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 1 -1, 139.1ms\n",
      "Speed: 6.2ms preprocess, 139.1ms inference, 6.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3577])\n",
      "data: tensor([[ 0.0000,  0.0000, 39.7638, 86.5561,  0.3577,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (88, 54)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[19.8819, 43.2780, 39.7638, 86.5561]])\n",
      "xywhn: tensor([[0.3682, 0.4918, 0.7364, 0.9836]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 39.7638, 86.5561]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7364, 0.9836]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 200.9ms\n",
      "Speed: 7.3ms preprocess, 200.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 124.6ms\n",
      "Speed: 6.6ms preprocess, 124.6ms inference, 7.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4226])\n",
      "data: tensor([[ 26.4839,   0.0000, 116.7864, 110.2481,   0.4226,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 71.6352,  55.1240,  90.3026, 110.2481]])\n",
      "xywhn: tensor([[0.5872, 0.4594, 0.7402, 0.9187]])\n",
      "xyxy: tensor([[ 26.4839,   0.0000, 116.7864, 110.2481]])\n",
      "xyxyn: tensor([[0.2171, 0.0000, 0.9573, 0.9187]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 1 -1, 139.9ms\n",
      "Speed: 5.9ms preprocess, 139.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4242])\n",
      "data: tensor([[ 0.0000,  0.0000, 37.4775, 78.9441,  0.4242,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (88, 55)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[18.7387, 39.4721, 37.4775, 78.9441]])\n",
      "xywhn: tensor([[0.3407, 0.4485, 0.6814, 0.8971]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 37.4775, 78.9441]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6814, 0.8971]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 194.0ms\n",
      "Speed: 0.0ms preprocess, 194.0ms inference, 6.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 125.4ms\n",
      "Speed: 6.7ms preprocess, 125.4ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4270])\n",
      "data: tensor([[ 25.7856,   0.0000, 117.0119, 110.0976,   0.4270,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 71.3988,  55.0488,  91.2263, 110.0976]])\n",
      "xywhn: tensor([[0.5852, 0.4587, 0.7478, 0.9175]])\n",
      "xyxy: tensor([[ 25.7856,   0.0000, 117.0119, 110.0976]])\n",
      "xyxyn: tensor([[0.2114, 0.0000, 0.9591, 0.9175]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x416 (no detections), 146.8ms\n",
      "Speed: 6.4ms preprocess, 146.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 1 -1, 186.0ms\n",
      "Speed: 1.5ms preprocess, 186.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 111.2ms\n",
      "Speed: 0.0ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3757])\n",
      "data: tensor([[ 26.9607,   0.0000, 117.6592, 109.8974,   0.3757,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.3100,  54.9487,  90.6985, 109.8974]])\n",
      "xywhn: tensor([[0.5927, 0.4541, 0.7434, 0.9082]])\n",
      "xyxy: tensor([[ 26.9607,   0.0000, 117.6592, 109.8974]])\n",
      "xyxyn: tensor([[0.2210, 0.0000, 0.9644, 0.9082]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x256 2 -1s, 71.9ms\n",
      "Speed: 2.1ms preprocess, 71.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5217])\n",
      "data: tensor([[  0.0000,  20.2110,  57.0525, 153.5125,   0.5217,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (155, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 28.5262,  86.8618,  57.0525, 133.3015]])\n",
      "xywhn: tensor([[0.4754, 0.5604, 0.9509, 0.8600]])\n",
      "xyxy: tensor([[  0.0000,  20.2110,  57.0525, 153.5125]])\n",
      "xyxyn: tensor([[0.0000, 0.1304, 0.9509, 0.9904]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4321])\n",
      "data: tensor([[  0.0000,   0.7164,  40.7736, 130.1425,   0.4321,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (155, 60)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 20.3868,  65.4294,  40.7736, 129.4261]])\n",
      "xywhn: tensor([[0.3398, 0.4221, 0.6796, 0.8350]])\n",
      "xyxy: tensor([[  0.0000,   0.7164,  40.7736, 130.1425]])\n",
      "xyxyn: tensor([[0.0000, 0.0046, 0.6796, 0.8396]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 167.3ms\n",
      "Speed: 9.8ms preprocess, 167.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 120.1ms\n",
      "Speed: 4.7ms preprocess, 120.1ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4433])\n",
      "data: tensor([[ 26.5469,   0.0000, 116.6346, 109.6086,   0.4433,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 71.5907,  54.8043,  90.0876, 109.6086]])\n",
      "xywhn: tensor([[0.5868, 0.4567, 0.7384, 0.9134]])\n",
      "xyxy: tensor([[ 26.5469,   0.0000, 116.6346, 109.6086]])\n",
      "xyxyn: tensor([[0.2176, 0.0000, 0.9560, 0.9134]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x256 2 -1s, 70.3ms\n",
      "Speed: 5.0ms preprocess, 70.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x640 1 -1, 167.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4226])\n",
      "data: tensor([[  0.0000,  11.3933,  63.0000, 157.0575,   0.4226,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (158, 63)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 31.5000,  84.2254,  63.0000, 145.6642]])\n",
      "xywhn: tensor([[0.5000, 0.5331, 1.0000, 0.9219]])\n",
      "xyxy: tensor([[  0.0000,  11.3933,  63.0000, 157.0575]])\n",
      "xyxyn: tensor([[0.0000, 0.0721, 1.0000, 0.9940]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3211])\n",
      "data: tensor([[  0.4821,   1.7910,  49.9338, 136.1054,   0.3211,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (158, 63)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 25.2079,  68.9482,  49.4516, 134.3144]])\n",
      "xywhn: tensor([[0.4001, 0.4364, 0.7849, 0.8501]])\n",
      "xyxy: tensor([[  0.4821,   1.7910,  49.9338, 136.1054]])\n",
      "xyxyn: tensor([[0.0077, 0.0113, 0.7926, 0.8614]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4413])\n",
      "data: tensor([[ 26.5497,   0.0000, 116.4936, 109.6943,   0.4413,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 71.5216,  54.8471,  89.9440, 109.6943]])\n",
      "xywhn: tensor([[0.5862, 0.4571, 0.7372, 0.9141]])\n",
      "xyxy: tensor([[ 26.5497,   0.0000, 116.4936, 109.6943]])\n",
      "xyxyn: tensor([[0.2176, 0.0000, 0.9549, 0.9141]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 6.9ms preprocess, 167.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 106.4ms\n",
      "Speed: 0.0ms preprocess, 106.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x288 3 -1s, 82.9ms\n",
      "Speed: 3.1ms preprocess, 82.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x640 1 -1, 166.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4926])\n",
      "data: tensor([[  1.8444,   1.9680,  60.8170, 154.5390,   0.4926,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (161, 66)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 31.3307,  78.2535,  58.9726, 152.5710]])\n",
      "xywhn: tensor([[0.4747, 0.4860, 0.8935, 0.9476]])\n",
      "xyxy: tensor([[  1.8444,   1.9680,  60.8170, 154.5390]])\n",
      "xyxyn: tensor([[0.0279, 0.0122, 0.9215, 0.9599]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3075])\n",
      "data: tensor([[  0.0000,  14.2942,  43.9093, 159.8838,   0.3075,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (161, 66)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 21.9547,  87.0890,  43.9093, 145.5897]])\n",
      "xywhn: tensor([[0.3326, 0.5409, 0.6653, 0.9043]])\n",
      "xyxy: tensor([[  0.0000,  14.2942,  43.9093, 159.8838]])\n",
      "xyxyn: tensor([[0.0000, 0.0888, 0.6653, 0.9931]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2769])\n",
      "data: tensor([[  0.5919,   2.4059,  45.4428, 120.0323,   0.2769,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (161, 66)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 23.0173,  61.2191,  44.8509, 117.6264]])\n",
      "xywhn: tensor([[0.3487, 0.3802, 0.6796, 0.7306]])\n",
      "xyxy: tensor([[  0.5919,   2.4059,  45.4428, 120.0323]])\n",
      "xyxyn: tensor([[0.0090, 0.0149, 0.6885, 0.7455]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4362])\n",
      "data: tensor([[ 26.1590,   0.0000, 116.6275, 109.5471,   0.4362,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 71.3932,  54.7735,  90.4685, 109.5471]])\n",
      "xywhn: tensor([[0.5852, 0.4564, 0.7415, 0.9129]])\n",
      "xyxy: tensor([[ 26.1590,   0.0000, 116.6275, 109.5471]])\n",
      "xyxyn: tensor([[0.2144, 0.0000, 0.9560, 0.9129]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 166.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 109.7ms\n",
      "Speed: 0.0ms preprocess, 109.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x288 2 -1s, 75.1ms\n",
      "Speed: 0.0ms preprocess, 75.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x640 1 -1, 152.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5532])\n",
      "data: tensor([[  8.2965,   3.3001,  69.0000, 145.7383,   0.5532,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (163, 69)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 38.6483,  74.5192,  60.7035, 142.4382]])\n",
      "xywhn: tensor([[0.5601, 0.4572, 0.8798, 0.8739]])\n",
      "xyxy: tensor([[  8.2965,   3.3001,  69.0000, 145.7383]])\n",
      "xyxyn: tensor([[0.1202, 0.0202, 1.0000, 0.8941]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2844])\n",
      "data: tensor([[  0.0000,  21.5469,  69.0000, 160.3712,   0.2844,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (163, 69)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 34.5000,  90.9590,  69.0000, 138.8243]])\n",
      "xywhn: tensor([[0.5000, 0.5580, 1.0000, 0.8517]])\n",
      "xyxy: tensor([[  0.0000,  21.5469,  69.0000, 160.3712]])\n",
      "xyxyn: tensor([[0.0000, 0.1322, 1.0000, 0.9839]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4388])\n",
      "data: tensor([[ 29.1193,   0.0000, 120.0944, 109.5366,   0.4388,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.6068,  54.7683,  90.9751, 109.5366]])\n",
      "xywhn: tensor([[0.6115, 0.4602, 0.7457, 0.9205]])\n",
      "xyxy: tensor([[ 29.1193,   0.0000, 120.0944, 109.5366]])\n",
      "xyxyn: tensor([[0.2387, 0.0000, 0.9844, 0.9205]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 6.6ms preprocess, 152.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 110.4ms\n",
      "Speed: 0.0ms preprocess, 110.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x288 1 -1, 83.6ms\n",
      "Speed: 7.1ms preprocess, 83.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x640 1 -1, 165.1ms\n",
      "Speed: 8.3ms preprocess, 165.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3513])\n",
      "data: tensor([[  3.7767,   3.9234,  72.0000, 158.3447,   0.3513,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (164, 72)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 37.8884,  81.1340,  68.2233, 154.4213]])\n",
      "xywhn: tensor([[0.5262, 0.4947, 0.9475, 0.9416]])\n",
      "xyxy: tensor([[  3.7767,   3.9234,  72.0000, 158.3447]])\n",
      "xyxyn: tensor([[0.0525, 0.0239, 1.0000, 0.9655]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4459])\n",
      "data: tensor([[ 29.1184,   0.0000, 120.0659, 109.6442,   0.4459,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.5922,  54.8221,  90.9476, 109.6442]])\n",
      "xywhn: tensor([[0.6114, 0.4607, 0.7455, 0.9214]])\n",
      "xyxy: tensor([[ 29.1184,   0.0000, 120.0659, 109.6442]])\n",
      "xyxyn: tensor([[0.2387, 0.0000, 0.9841, 0.9214]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 105.0ms\n",
      "Speed: 0.0ms preprocess, 105.0ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x288 1 -1, 87.8ms\n",
      "Speed: 3.4ms preprocess, 87.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x640 1 -1, 154.6ms\n",
      "Speed: 3.4ms preprocess, 154.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4973])\n",
      "data: tensor([[  0.0000,  10.4057,  73.0000, 164.2219,   0.4973,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (165, 73)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 36.5000,  87.3138,  73.0000, 153.8162]])\n",
      "xywhn: tensor([[0.5000, 0.5292, 1.0000, 0.9322]])\n",
      "xyxy: tensor([[  0.0000,  10.4057,  73.0000, 164.2219]])\n",
      "xyxyn: tensor([[0.0000, 0.0631, 1.0000, 0.9953]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4488])\n",
      "data: tensor([[ 28.7876,   0.0000, 120.1445, 109.4361,   0.4488,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.4660,  54.7181,  91.3569, 109.4361]])\n",
      "xywhn: tensor([[0.6104, 0.4598, 0.7488, 0.9196]])\n",
      "xyxy: tensor([[ 28.7876,   0.0000, 120.1445, 109.4361]])\n",
      "xyxyn: tensor([[0.2360, 0.0000, 0.9848, 0.9196]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 115.0ms\n",
      "Speed: 0.1ms preprocess, 115.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x288 2 -1s, 76.3ms\n",
      "Speed: 1.9ms preprocess, 76.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4581])\n",
      "data: tensor([[  0.0000,  15.7495,  53.5878, 165.3631,   0.4581,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (167, 73)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 26.7939,  90.5563,  53.5878, 149.6136]])\n",
      "xywhn: tensor([[0.3670, 0.5423, 0.7341, 0.8959]])\n",
      "xyxy: tensor([[  0.0000,  15.7495,  53.5878, 165.3631]])\n",
      "xyxyn: tensor([[0.0000, 0.0943, 0.7341, 0.9902]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2694])\n",
      "data: tensor([[  0.0000,  19.9164,  73.0000, 164.6026,   0.2694,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (167, 73)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 36.5000,  92.2595,  73.0000, 144.6862]])\n",
      "xywhn: tensor([[0.5000, 0.5525, 1.0000, 0.8664]])\n",
      "xyxy: tensor([[  0.0000,  19.9164,  73.0000, 164.6026]])\n",
      "xyxyn: tensor([[0.0000, 0.1193, 1.0000, 0.9856]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 206.4ms\n",
      "Speed: 6.1ms preprocess, 206.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 104.6ms\n",
      "Speed: 0.0ms preprocess, 104.6ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4808])\n",
      "data: tensor([[ 30.1019,   0.0000, 121.1636, 110.9043,   0.4808,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.6327,  55.4522,  91.0617, 110.9043]])\n",
      "xywhn: tensor([[0.6149, 0.4660, 0.7403, 0.9320]])\n",
      "xyxy: tensor([[ 30.1019,   0.0000, 121.1636, 110.9043]])\n",
      "xyxyn: tensor([[0.2447, 0.0000, 0.9851, 0.9320]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x288 1 -1, 81.9ms\n",
      "Speed: 2.5ms preprocess, 81.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x640 1 -1, 151.4ms\n",
      "Speed: 6.5ms preprocess, 151.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4365])\n",
      "data: tensor([[  0.0000,  11.5696,  74.0000, 168.1413,   0.4365,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (169, 74)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 37.0000,  89.8554,  74.0000, 156.5717]])\n",
      "xywhn: tensor([[0.5000, 0.5317, 1.0000, 0.9265]])\n",
      "xyxy: tensor([[  0.0000,  11.5696,  74.0000, 168.1413]])\n",
      "xyxyn: tensor([[0.0000, 0.0685, 1.0000, 0.9949]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4703])\n",
      "data: tensor([[ 30.0264,   0.0000, 121.1655, 110.9001,   0.4703,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.5959,  55.4501,  91.1390, 110.9001]])\n",
      "xywhn: tensor([[0.6146, 0.4660, 0.7410, 0.9319]])\n",
      "xyxy: tensor([[ 30.0264,   0.0000, 121.1655, 110.9001]])\n",
      "xyxyn: tensor([[0.2441, 0.0000, 0.9851, 0.9319]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 105.8ms\n",
      "Speed: 4.2ms preprocess, 105.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x288 1 -1, 73.2ms\n",
      "Speed: 6.7ms preprocess, 73.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x640 1 -1, 149.8ms\n",
      "Speed: 8.1ms preprocess, 149.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5125])\n",
      "data: tensor([[  0.4921,  14.3255,  75.0000, 168.5136,   0.5125,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (170, 75)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 37.7460,  91.4196,  74.5079, 154.1881]])\n",
      "xywhn: tensor([[0.5033, 0.5378, 0.9934, 0.9070]])\n",
      "xyxy: tensor([[  0.4921,  14.3255,  75.0000, 168.5136]])\n",
      "xyxyn: tensor([[0.0066, 0.0843, 1.0000, 0.9913]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4635])\n",
      "data: tensor([[ 30.1392,   0.0000, 121.1464, 110.7634,   0.4635,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.6428,  55.3817,  91.0072, 110.7634]])\n",
      "xywhn: tensor([[0.6150, 0.4654, 0.7399, 0.9308]])\n",
      "xyxy: tensor([[ 30.1392,   0.0000, 121.1464, 110.7634]])\n",
      "xyxyn: tensor([[0.2450, 0.0000, 0.9849, 0.9308]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 91.8ms\n",
      "Speed: 3.3ms preprocess, 91.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x288 1 -1, 77.2ms\n",
      "Speed: 2.7ms preprocess, 77.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x640 1 -1, 165.5ms\n",
      "Speed: 13.7ms preprocess, 165.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5407])\n",
      "data: tensor([[  0.0000,  28.8133,  75.0000, 170.8489,   0.5407,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (172, 75)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 37.5000,  99.8311,  75.0000, 142.0356]])\n",
      "xywhn: tensor([[0.5000, 0.5804, 1.0000, 0.8258]])\n",
      "xyxy: tensor([[  0.0000,  28.8133,  75.0000, 170.8489]])\n",
      "xyxyn: tensor([[0.0000, 0.1675, 1.0000, 0.9933]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4767])\n",
      "data: tensor([[ 29.7360,   0.0000, 121.1827, 110.1513,   0.4767,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.4594,  55.0757,  91.4467, 110.1513]])\n",
      "xywhn: tensor([[0.6135, 0.4628, 0.7435, 0.9256]])\n",
      "xyxy: tensor([[ 29.7360,   0.0000, 121.1827, 110.1513]])\n",
      "xyxyn: tensor([[0.2418, 0.0000, 0.9852, 0.9256]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 107.7ms\n",
      "Speed: 1.4ms preprocess, 107.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x288 1 -1, 76.8ms\n",
      "Speed: 3.3ms preprocess, 76.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3585])\n",
      "data: tensor([[  0.0000,  14.4873,  76.0000, 172.8208,   0.3585,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (174, 76)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 38.0000,  93.6540,  76.0000, 158.3336]])\n",
      "xywhn: tensor([[0.5000, 0.5382, 1.0000, 0.9100]])\n",
      "xyxy: tensor([[  0.0000,  14.4873,  76.0000, 172.8208]])\n",
      "xyxyn: tensor([[0.0000, 0.0833, 1.0000, 0.9932]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 170.6ms\n",
      "Speed: 17.1ms preprocess, 170.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 121.1ms\n",
      "Speed: 0.6ms preprocess, 121.1ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4732])\n",
      "data: tensor([[ 30.1798,   0.0000, 121.1982, 109.9816,   0.4732,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.6890,  54.9908,  91.0185, 109.9816]])\n",
      "xywhn: tensor([[0.6154, 0.4621, 0.7400, 0.9242]])\n",
      "xyxy: tensor([[ 30.1798,   0.0000, 121.1982, 109.9816]])\n",
      "xyxyn: tensor([[0.2454, 0.0000, 0.9854, 0.9242]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x288 1 -1, 81.6ms\n",
      "Speed: 1.8ms preprocess, 81.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x640 1 -1, 160.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4052])\n",
      "data: tensor([[  0.0000,   8.6786,  77.0000, 175.3271,   0.4052,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (176, 77)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 38.5000,  92.0029,  77.0000, 166.6485]])\n",
      "xywhn: tensor([[0.5000, 0.5227, 1.0000, 0.9469]])\n",
      "xyxy: tensor([[  0.0000,   8.6786,  77.0000, 175.3271]])\n",
      "xyxyn: tensor([[0.0000, 0.0493, 1.0000, 0.9962]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4555])\n",
      "data: tensor([[ 30.0499,   0.0000, 121.3779, 109.8568,   0.4555,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.7139,  54.9284,  91.3279, 109.8568]])\n",
      "xywhn: tensor([[0.6156, 0.4616, 0.7425, 0.9232]])\n",
      "xyxy: tensor([[ 30.0499,   0.0000, 121.3779, 109.8568]])\n",
      "xyxyn: tensor([[0.2443, 0.0000, 0.9868, 0.9232]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 5.0ms preprocess, 160.9ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 103.3ms\n",
      "Speed: 3.4ms preprocess, 103.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x288 1 -1, 90.5ms\n",
      "Speed: 0.9ms preprocess, 90.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x640 1 -1, 157.1ms\n",
      "Speed: 5.3ms preprocess, 157.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4309])\n",
      "data: tensor([[  0.0000,   0.0000,  78.0000, 171.9900,   0.4309,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (177, 78)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 39.0000,  85.9950,  78.0000, 171.9900]])\n",
      "xywhn: tensor([[0.5000, 0.4858, 1.0000, 0.9717]])\n",
      "xyxy: tensor([[  0.0000,   0.0000,  78.0000, 171.9900]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 1.0000, 0.9717]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4727])\n",
      "data: tensor([[ 30.0814,   0.0000, 121.3561, 109.7307,   0.4727,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.7188,  54.8654,  91.2747, 109.7307]])\n",
      "xywhn: tensor([[0.6156, 0.4611, 0.7421, 0.9221]])\n",
      "xyxy: tensor([[ 30.0814,   0.0000, 121.3561, 109.7307]])\n",
      "xyxyn: tensor([[0.2446, 0.0000, 0.9866, 0.9221]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 109.8ms\n",
      "Speed: 0.0ms preprocess, 109.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x320 1 -1, 69.9ms\n",
      "Speed: 2.8ms preprocess, 69.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x640 1 -1, 164.9ms\n",
      "Speed: 0.0ms preprocess, 164.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6570])\n",
      "data: tensor([[  0.0000,   9.4744,  81.0000, 178.0000,   0.6570,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (178, 81)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 40.5000,  93.7372,  81.0000, 168.5256]])\n",
      "xywhn: tensor([[0.5000, 0.5266, 1.0000, 0.9468]])\n",
      "xyxy: tensor([[  0.0000,   9.4744,  81.0000, 178.0000]])\n",
      "xyxyn: tensor([[0.0000, 0.0532, 1.0000, 1.0000]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4683])\n",
      "data: tensor([[ 29.6263,   0.0000, 121.2318, 109.9338,   0.4683,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.4291,  54.9669,  91.6055, 109.9338]])\n",
      "xywhn: tensor([[0.6132, 0.4619, 0.7448, 0.9238]])\n",
      "xyxy: tensor([[ 29.6263,   0.0000, 121.2318, 109.9338]])\n",
      "xyxyn: tensor([[0.2409, 0.0000, 0.9856, 0.9238]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 116.8ms\n",
      "Speed: 0.0ms preprocess, 116.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x320 1 -1, 81.2ms\n",
      "Speed: 0.0ms preprocess, 81.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x640 1 -1, 165.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5976])\n",
      "data: tensor([[  0.0000,   9.1038,  82.0000, 179.3316,   0.5976,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (180, 82)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 41.0000,  94.2177,  82.0000, 170.2279]])\n",
      "xywhn: tensor([[0.5000, 0.5234, 1.0000, 0.9457]])\n",
      "xyxy: tensor([[  0.0000,   9.1038,  82.0000, 179.3316]])\n",
      "xyxyn: tensor([[0.0000, 0.0506, 1.0000, 0.9963]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4702])\n",
      "data: tensor([[ 29.4704,   0.0000, 121.1541, 110.1883,   0.4702,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.3123,  55.0942,  91.6836, 110.1883]])\n",
      "xywhn: tensor([[0.6123, 0.4630, 0.7454, 0.9260]])\n",
      "xyxy: tensor([[ 29.4704,   0.0000, 121.1541, 110.1883]])\n",
      "xyxyn: tensor([[0.2396, 0.0000, 0.9850, 0.9260]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 4.5ms preprocess, 165.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 115.1ms\n",
      "Speed: 6.8ms preprocess, 115.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x320 1 -1, 101.4ms\n",
      "Speed: 0.0ms preprocess, 101.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x640 1 -1, 160.8ms\n",
      "Speed: 6.7ms preprocess, 160.8ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5814])\n",
      "data: tensor([[  0.0000,   8.9057,  85.0000, 180.8682,   0.5814,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (181, 85)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 42.5000,  94.8869,  85.0000, 171.9625]])\n",
      "xywhn: tensor([[0.5000, 0.5242, 1.0000, 0.9501]])\n",
      "xyxy: tensor([[  0.0000,   8.9057,  85.0000, 180.8682]])\n",
      "xyxyn: tensor([[0.0000, 0.0492, 1.0000, 0.9993]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4719])\n",
      "data: tensor([[ 29.7071,   0.0000, 121.1348, 109.9468,   0.4719,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.4209,  54.9734,  91.4277, 109.9468]])\n",
      "xywhn: tensor([[0.6132, 0.4620, 0.7433, 0.9239]])\n",
      "xyxy: tensor([[ 29.7071,   0.0000, 121.1348, 109.9468]])\n",
      "xyxyn: tensor([[0.2415, 0.0000, 0.9848, 0.9239]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 book, 112.9ms\n",
      "Speed: 0.1ms preprocess, 112.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x320 1 -1, 83.9ms\n",
      "Speed: 0.0ms preprocess, 83.9ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x640 2 -1s, 158.8ms\n",
      "Speed: 3.0ms preprocess, 158.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6046])\n",
      "data: tensor([[  0.0000,   3.0654,  87.0000, 181.9925,   0.6046,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (182, 87)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 43.5000,  92.5289,  87.0000, 178.9271]])\n",
      "xywhn: tensor([[0.5000, 0.5084, 1.0000, 0.9831]])\n",
      "xyxy: tensor([[  0.0000,   3.0654,  87.0000, 181.9925]])\n",
      "xyxyn: tensor([[0.0000, 0.0168, 1.0000, 1.0000]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4626])\n",
      "data: tensor([[ 29.3252,   0.0000, 122.2691, 110.4528,   0.4626,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.7972,  55.2264,  92.9439, 110.4528]])\n",
      "xywhn: tensor([[0.6113, 0.4641, 0.7495, 0.9282]])\n",
      "xyxy: tensor([[ 29.3252,   0.0000, 122.2691, 110.4528]])\n",
      "xyxyn: tensor([[0.2365, 0.0000, 0.9860, 0.9282]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2752])\n",
      "data: tensor([[ 23.0118,   2.4163,  84.7724, 111.6731,   0.2752,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 53.8921,  57.0447,  61.7606, 109.2568]])\n",
      "xywhn: tensor([[0.4346, 0.4794, 0.4981, 0.9181]])\n",
      "xyxy: tensor([[ 23.0118,   2.4163,  84.7724, 111.6731]])\n",
      "xyxyn: tensor([[0.1856, 0.0203, 0.6836, 0.9384]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 2 books, 118.2ms\n",
      "Speed: 0.0ms preprocess, 118.2ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x320 2 -1s, 92.6ms\n",
      "Speed: 2.0ms preprocess, 92.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x640 2 -1s, 151.4ms\n",
      "Speed: 6.5ms preprocess, 151.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5007])\n",
      "data: tensor([[  0.0000,   2.5491,  89.0000, 180.1426,   0.5007,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (181, 89)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 44.5000,  91.3459,  89.0000, 177.5934]])\n",
      "xywhn: tensor([[0.5000, 0.5047, 1.0000, 0.9812]])\n",
      "xyxy: tensor([[  0.0000,   2.5491,  89.0000, 180.1426]])\n",
      "xyxyn: tensor([[0.0000, 0.0141, 1.0000, 0.9953]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2540])\n",
      "data: tensor([[  0.0000,  23.9850,  65.3168, 178.2782,   0.2540,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (181, 89)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 32.6584, 101.1316,  65.3168, 154.2932]])\n",
      "xywhn: tensor([[0.3669, 0.5587, 0.7339, 0.8524]])\n",
      "xyxy: tensor([[  0.0000,  23.9850,  65.3168, 178.2782]])\n",
      "xyxyn: tensor([[0.0000, 0.1325, 0.7339, 0.9850]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4497])\n",
      "data: tensor([[ 28.6487,   0.0000, 122.1808, 110.4667,   0.4497,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.4147,  55.2333,  93.5322, 110.4667]])\n",
      "xywhn: tensor([[0.6082, 0.4641, 0.7543, 0.9283]])\n",
      "xyxy: tensor([[ 28.6487,   0.0000, 122.1808, 110.4667]])\n",
      "xyxyn: tensor([[0.2310, 0.0000, 0.9853, 0.9283]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3017])\n",
      "data: tensor([[ 23.1923,   2.3860,  84.7134, 111.6832,   0.3017,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 53.9529,  57.0346,  61.5211, 109.2972]])\n",
      "xywhn: tensor([[0.4351, 0.4793, 0.4961, 0.9185]])\n",
      "xyxy: tensor([[ 23.1923,   2.3860,  84.7134, 111.6832]])\n",
      "xyxyn: tensor([[0.1870, 0.0201, 0.6832, 0.9385]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 96.3ms\n",
      "Speed: 3.5ms preprocess, 96.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x352 1 -1, 90.9ms\n",
      "Speed: 2.0ms preprocess, 90.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x640 2 -1s, 155.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6621])\n",
      "data: tensor([[  0.0000,   9.4391,  91.0000, 177.4666,   0.6621,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (181, 91)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 45.5000,  93.4529,  91.0000, 168.0275]])\n",
      "xywhn: tensor([[0.5000, 0.5163, 1.0000, 0.9283]])\n",
      "xyxy: tensor([[  0.0000,   9.4391,  91.0000, 177.4666]])\n",
      "xyxyn: tensor([[0.0000, 0.0521, 1.0000, 0.9805]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4580])\n",
      "data: tensor([[ 22.6974,   1.9748,  85.9183, 111.9632,   0.4580,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 54.3078,  56.9690,  63.2209, 109.9883]])\n",
      "xywhn: tensor([[0.4345, 0.4787, 0.5058, 0.9243]])\n",
      "xyxy: tensor([[ 22.6974,   1.9748,  85.9183, 111.9632]])\n",
      "xyxyn: tensor([[0.1816, 0.0166, 0.6873, 0.9409]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4395])\n",
      "data: tensor([[ 29.3800,   0.0000, 123.3202, 111.1634,   0.4395,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.3501,  55.5817,  93.9402, 111.1634]])\n",
      "xywhn: tensor([[0.6108, 0.4671, 0.7515, 0.9341]])\n",
      "xyxy: tensor([[ 29.3800,   0.0000, 123.3202, 111.1634]])\n",
      "xyxyn: tensor([[0.2350, 0.0000, 0.9866, 0.9341]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 16.7ms preprocess, 155.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 2 books, 121.7ms\n",
      "Speed: 0.0ms preprocess, 121.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x352 1 -1, 91.5ms\n",
      "Speed: 3.5ms preprocess, 91.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x640 2 -1s, 149.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6288])\n",
      "data: tensor([[  0.0000,   3.5362,  91.0000, 181.0000,   0.6288,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (181, 91)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 45.5000,  92.2681,  91.0000, 177.4638]])\n",
      "xywhn: tensor([[0.5000, 0.5098, 1.0000, 0.9805]])\n",
      "xyxy: tensor([[  0.0000,   3.5362,  91.0000, 181.0000]])\n",
      "xyxyn: tensor([[0.0000, 0.0195, 1.0000, 1.0000]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5207])\n",
      "data: tensor([[ 22.6734,   1.9634,  85.4506, 111.9398,   0.5207,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 54.0620,  56.9516,  62.7772, 109.9764]])\n",
      "xywhn: tensor([[0.4325, 0.4786, 0.5022, 0.9242]])\n",
      "xyxy: tensor([[ 22.6734,   1.9634,  85.4506, 111.9398]])\n",
      "xyxyn: tensor([[0.1814, 0.0165, 0.6836, 0.9407]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4107])\n",
      "data: tensor([[ 28.9397,   0.0000, 123.2986, 111.1245,   0.4107,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.1191,  55.5623,  94.3589, 111.1245]])\n",
      "xywhn: tensor([[0.6090, 0.4669, 0.7549, 0.9338]])\n",
      "xyxy: tensor([[ 28.9397,   0.0000, 123.2986, 111.1245]])\n",
      "xyxyn: tensor([[0.2315, 0.0000, 0.9864, 0.9338]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 18.7ms preprocess, 149.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 106.0ms\n",
      "Speed: 0.0ms preprocess, 106.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x352 2 -1s, 92.4ms\n",
      "Speed: 3.8ms preprocess, 92.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x640 2 -1s, 158.3ms\n",
      "Speed: 0.0ms preprocess, 158.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6000])\n",
      "data: tensor([[  0.0000,   3.3839,  94.0000, 179.8618,   0.6000,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (181, 94)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 47.0000,  91.6229,  94.0000, 176.4778]])\n",
      "xywhn: tensor([[0.5000, 0.5062, 1.0000, 0.9750]])\n",
      "xyxy: tensor([[  0.0000,   3.3839,  94.0000, 179.8618]])\n",
      "xyxyn: tensor([[0.0000, 0.0187, 1.0000, 0.9937]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2820])\n",
      "data: tensor([[  0.0000,   5.0127,  58.7268, 179.9560,   0.2820,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (181, 94)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 29.3634,  92.4843,  58.7268, 174.9433]])\n",
      "xywhn: tensor([[0.3124, 0.5110, 0.6248, 0.9665]])\n",
      "xyxy: tensor([[  0.0000,   5.0127,  58.7268, 179.9560]])\n",
      "xyxyn: tensor([[0.0000, 0.0277, 0.6248, 0.9942]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4991])\n",
      "data: tensor([[ 23.1780,   1.8681,  85.6079, 111.8820,   0.4991,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 54.3930,  56.8750,  62.4300, 110.0139]])\n",
      "xywhn: tensor([[0.4351, 0.4779, 0.4994, 0.9245]])\n",
      "xyxy: tensor([[ 23.1780,   1.8681,  85.6079, 111.8820]])\n",
      "xyxyn: tensor([[0.1854, 0.0157, 0.6849, 0.9402]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4062])\n",
      "data: tensor([[ 29.3743,   0.0000, 123.2975, 111.1135,   0.4062,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.3359,  55.5568,  93.9232, 111.1135]])\n",
      "xywhn: tensor([[0.6107, 0.4669, 0.7514, 0.9337]])\n",
      "xyxy: tensor([[ 29.3743,   0.0000, 123.2975, 111.1135]])\n",
      "xyxyn: tensor([[0.2350, 0.0000, 0.9864, 0.9337]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 tie, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 99.1ms\n",
      "Speed: 4.0ms preprocess, 99.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x352 1 -1, 78.6ms\n",
      "Speed: 15.1ms preprocess, 78.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 608x640 2 -1s, 139.3ms\n",
      "Speed: 2.6ms preprocess, 139.3ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6459])\n",
      "data: tensor([[  0.0000,   3.4364,  95.0000, 180.5274,   0.6459,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (181, 95)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 47.5000,  91.9819,  95.0000, 177.0910]])\n",
      "xywhn: tensor([[0.5000, 0.5082, 1.0000, 0.9784]])\n",
      "xyxy: tensor([[  0.0000,   3.4364,  95.0000, 180.5274]])\n",
      "xyxyn: tensor([[0.0000, 0.0190, 1.0000, 0.9974]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5145])\n",
      "data: tensor([[ 27.9733,   0.0000,  83.7353, 112.2310,   0.5145,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 126)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 55.8543,  56.1155,  55.7620, 112.2310]])\n",
      "xywhn: tensor([[0.4433, 0.4716, 0.4426, 0.9431]])\n",
      "xyxy: tensor([[ 27.9733,   0.0000,  83.7353, 112.2310]])\n",
      "xyxyn: tensor([[0.2220, 0.0000, 0.6646, 0.9431]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3660])\n",
      "data: tensor([[ 27.6744,   0.0000, 124.9138, 110.1701,   0.3660,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 126)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.2941,  55.0851,  97.2394, 110.1701]])\n",
      "xywhn: tensor([[0.6055, 0.4629, 0.7717, 0.9258]])\n",
      "xyxy: tensor([[ 27.6744,   0.0000, 124.9138, 110.1701]])\n",
      "xyxyn: tensor([[0.2196, 0.0000, 0.9914, 0.9258]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 tie, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 113.4ms\n",
      "Speed: 0.0ms preprocess, 113.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x352 1 -1, 78.0ms\n",
      "Speed: 0.8ms preprocess, 78.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x640 3 -1s, 149.8ms\n",
      "Speed: 8.4ms preprocess, 149.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5974])\n",
      "data: tensor([[  0.0000,   3.1986,  96.0000, 178.9752,   0.5974,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (181, 96)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 48.0000,  91.0869,  96.0000, 175.7765]])\n",
      "xywhn: tensor([[0.5000, 0.5032, 1.0000, 0.9711]])\n",
      "xyxy: tensor([[  0.0000,   3.1986,  96.0000, 178.9752]])\n",
      "xyxyn: tensor([[0.0000, 0.0177, 1.0000, 0.9888]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4321])\n",
      "data: tensor([[ 29.0056,   0.0000, 122.0845, 111.0205,   0.4321,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.5451,  55.5102,  93.0789, 111.0205]])\n",
      "xywhn: tensor([[0.6092, 0.4665, 0.7506, 0.9329]])\n",
      "xyxy: tensor([[ 29.0056,   0.0000, 122.0845, 111.0205]])\n",
      "xyxyn: tensor([[0.2339, 0.0000, 0.9846, 0.9329]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4099])\n",
      "data: tensor([[ 23.3260,   2.2870,  86.1230, 112.2771,   0.4099,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 54.7245,  57.2820,  62.7970, 109.9901]])\n",
      "xywhn: tensor([[0.4413, 0.4814, 0.5064, 0.9243]])\n",
      "xyxy: tensor([[ 23.3260,   2.2870,  86.1230, 112.2771]])\n",
      "xyxyn: tensor([[0.1881, 0.0192, 0.6945, 0.9435]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3026])\n",
      "data: tensor([[  0.5877,   0.0000,  26.4985, 110.7409,   0.3026,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 13.5431,  55.3704,  25.9108, 110.7409]])\n",
      "xywhn: tensor([[0.1092, 0.4653, 0.2090, 0.9306]])\n",
      "xyxy: tensor([[  0.5877,   0.0000,  26.4985, 110.7409]])\n",
      "xyxyn: tensor([[0.0047, 0.0000, 0.2137, 0.9306]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 tie, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 3 books, 91.9ms\n",
      "Speed: 0.0ms preprocess, 91.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x352 1 -1, 78.0ms\n",
      "Speed: 5.3ms preprocess, 78.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x640 2 -1s, 141.1ms\n",
      "Speed: 3.4ms preprocess, 141.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5676])\n",
      "data: tensor([[  0.0000,   3.3622,  97.0000, 177.5034,   0.5676,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (180, 97)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 48.5000,  90.4328,  97.0000, 174.1412]])\n",
      "xywhn: tensor([[0.5000, 0.5024, 1.0000, 0.9675]])\n",
      "xyxy: tensor([[  0.0000,   3.3622,  97.0000, 177.5034]])\n",
      "xyxyn: tensor([[0.0000, 0.0187, 1.0000, 0.9861]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4321])\n",
      "data: tensor([[ 27.6567,   0.0000, 120.9040, 110.3070,   0.4321,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.2803,  55.1535,  93.2473, 110.3070]])\n",
      "xywhn: tensor([[0.6039, 0.4635, 0.7581, 0.9269]])\n",
      "xyxy: tensor([[ 27.6567,   0.0000, 120.9040, 110.3070]])\n",
      "xyxyn: tensor([[0.2249, 0.0000, 0.9830, 0.9269]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4186])\n",
      "data: tensor([[ 20.5269,   2.6430,  84.2247, 111.7639,   0.4186,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 52.3758,  57.2035,  63.6978, 109.1209]])\n",
      "xywhn: tensor([[0.4258, 0.4807, 0.5179, 0.9170]])\n",
      "xyxy: tensor([[ 20.5269,   2.6430,  84.2247, 111.7639]])\n",
      "xyxyn: tensor([[0.1669, 0.0222, 0.6848, 0.9392]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 3 books, 107.7ms\n",
      "Speed: 4.4ms preprocess, 107.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x352 1 -1, 93.6ms\n",
      "Speed: 1.2ms preprocess, 93.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x640 2 -1s, 145.3ms\n",
      "Speed: 5.6ms preprocess, 145.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6601])\n",
      "data: tensor([[  0.0000,   4.3074,  96.0000, 177.9668,   0.6601,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (180, 96)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 48.0000,  91.1371,  96.0000, 173.6594]])\n",
      "xywhn: tensor([[0.5000, 0.5063, 1.0000, 0.9648]])\n",
      "xyxy: tensor([[  0.0000,   4.3074,  96.0000, 177.9668]])\n",
      "xyxyn: tensor([[0.0000, 0.0239, 1.0000, 0.9887]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4088])\n",
      "data: tensor([[ 27.4491,   0.0000, 120.1341, 110.8014,   0.4088,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.7916,  55.4007,  92.6850, 110.8014]])\n",
      "xywhn: tensor([[0.6048, 0.4656, 0.7597, 0.9311]])\n",
      "xyxy: tensor([[ 27.4491,   0.0000, 120.1341, 110.8014]])\n",
      "xyxyn: tensor([[0.2250, 0.0000, 0.9847, 0.9311]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2788])\n",
      "data: tensor([[ 18.3524,   1.0702,  85.5301, 111.3788,   0.2788,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 51.9412,  56.2245,  67.1777, 110.3086]])\n",
      "xywhn: tensor([[0.4257, 0.4725, 0.5506, 0.9270]])\n",
      "xyxy: tensor([[ 18.3524,   1.0702,  85.5301, 111.3788]])\n",
      "xyxyn: tensor([[0.1504, 0.0090, 0.7011, 0.9360]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 3 books, 93.7ms\n",
      "Speed: 0.0ms preprocess, 93.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x352 1 -1, 75.3ms\n",
      "Speed: 4.2ms preprocess, 75.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x640 2 -1s, 154.2ms\n",
      "Speed: 2.0ms preprocess, 154.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6132])\n",
      "data: tensor([[  0.0000,   4.2228,  96.0000, 178.7857,   0.6132,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (181, 96)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 48.0000,  91.5043,  96.0000, 174.5629]])\n",
      "xywhn: tensor([[0.5000, 0.5055, 1.0000, 0.9644]])\n",
      "xyxy: tensor([[  0.0000,   4.2228,  96.0000, 178.7857]])\n",
      "xyxyn: tensor([[0.0000, 0.0233, 1.0000, 0.9878]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4051])\n",
      "data: tensor([[ 26.9219,   0.0000, 119.0142, 110.7489,   0.4051,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (118, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.9680,  55.3744,  92.0923, 110.7489]])\n",
      "xywhn: tensor([[0.6030, 0.4693, 0.7611, 0.9385]])\n",
      "xyxy: tensor([[ 26.9219,   0.0000, 119.0142, 110.7489]])\n",
      "xyxyn: tensor([[0.2225, 0.0000, 0.9836, 0.9385]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2690])\n",
      "data: tensor([[ 19.8697,   1.6980,  85.0855, 111.5971,   0.2690,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (118, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 52.4776,  56.6475,  65.2158, 109.8991]])\n",
      "xywhn: tensor([[0.4337, 0.4801, 0.5390, 0.9313]])\n",
      "xyxy: tensor([[ 19.8697,   1.6980,  85.0855, 111.5971]])\n",
      "xyxyn: tensor([[0.1642, 0.0144, 0.7032, 0.9457]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 cell phone, 3 books, 92.7ms\n",
      "Speed: 0.0ms preprocess, 92.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x352 1 -1, 77.7ms\n",
      "Speed: 1.9ms preprocess, 77.7ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x640 2 -1s, 171.4ms\n",
      "Speed: 2.2ms preprocess, 171.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7560])\n",
      "data: tensor([[  0.0000,   4.6752,  97.0000, 172.6404,   0.7560,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (180, 97)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 48.5000,  88.6578,  97.0000, 167.9652]])\n",
      "xywhn: tensor([[0.5000, 0.4925, 1.0000, 0.9331]])\n",
      "xyxy: tensor([[  0.0000,   4.6752,  97.0000, 172.6404]])\n",
      "xyxyn: tensor([[0.0000, 0.0260, 1.0000, 0.9591]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4284])\n",
      "data: tensor([[ 27.1847,   0.0000, 120.8418, 110.6931,   0.4284,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.0132,  55.3466,  93.6570, 110.6931]])\n",
      "xywhn: tensor([[0.6017, 0.4651, 0.7614, 0.9302]])\n",
      "xyxy: tensor([[ 27.1847,   0.0000, 120.8418, 110.6931]])\n",
      "xyxyn: tensor([[0.2210, 0.0000, 0.9825, 0.9302]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3930])\n",
      "data: tensor([[  0.3022,   0.0000,  27.2483, 111.3083,   0.3930,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 13.7753,  55.6541,  26.9460, 111.3083]])\n",
      "xywhn: tensor([[0.1120, 0.4677, 0.2191, 0.9354]])\n",
      "xyxy: tensor([[  0.3022,   0.0000,  27.2483, 111.3083]])\n",
      "xyxyn: tensor([[0.0025, 0.0000, 0.2215, 0.9354]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 3 books, 100.6ms\n",
      "Speed: 4.0ms preprocess, 100.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x384 1 -1, 92.9ms\n",
      "Speed: 7.0ms preprocess, 92.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x640 2 -1s, 142.5ms\n",
      "Speed: 8.3ms preprocess, 142.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6760])\n",
      "data: tensor([[  0.0000,   5.6778, 102.0000, 175.9123,   0.6760,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (180, 102)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 51.0000,  90.7950, 102.0000, 170.2345]])\n",
      "xywhn: tensor([[0.5000, 0.5044, 1.0000, 0.9457]])\n",
      "xyxy: tensor([[  0.0000,   5.6778, 102.0000, 175.9123]])\n",
      "xyxyn: tensor([[0.0000, 0.0315, 1.0000, 0.9773]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3990])\n",
      "data: tensor([[ 27.7340,   0.0000, 120.9935, 111.0579,   0.3990,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.3637,  55.5290,  93.2595, 111.0579]])\n",
      "xywhn: tensor([[0.6046, 0.4627, 0.7582, 0.9255]])\n",
      "xyxy: tensor([[ 27.7340,   0.0000, 120.9935, 111.0579]])\n",
      "xyxyn: tensor([[0.2255, 0.0000, 0.9837, 0.9255]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2648])\n",
      "data: tensor([[  0.4129,   0.0000,  26.3834, 111.5842,   0.2648,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 13.3982,  55.7921,  25.9706, 111.5842]])\n",
      "xywhn: tensor([[0.1089, 0.4649, 0.2111, 0.9299]])\n",
      "xyxy: tensor([[  0.4129,   0.0000,  26.3834, 111.5842]])\n",
      "xyxyn: tensor([[0.0034, 0.0000, 0.2145, 0.9299]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 3 books, 100.3ms\n",
      "Speed: 0.0ms preprocess, 100.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x384 1 -1, 100.3ms\n",
      "Speed: 2.6ms preprocess, 100.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x640 1 -1, 138.2ms\n",
      "Speed: 2.5ms preprocess, 138.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6711])\n",
      "data: tensor([[  0.0000,   5.0820, 102.0000, 137.1334,   0.6711,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (178, 102)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 51.0000,  71.1077, 102.0000, 132.0515]])\n",
      "xywhn: tensor([[0.5000, 0.3995, 1.0000, 0.7419]])\n",
      "xyxy: tensor([[  0.0000,   5.0820, 102.0000, 137.1334]])\n",
      "xyxyn: tensor([[0.0000, 0.0286, 1.0000, 0.7704]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4084])\n",
      "data: tensor([[ 27.9815,   0.0000, 120.9476, 111.4658,   0.4084,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.4646,  55.7329,  92.9661, 111.4658]])\n",
      "xywhn: tensor([[0.6054, 0.4644, 0.7558, 0.9289]])\n",
      "xyxy: tensor([[ 27.9815,   0.0000, 120.9476, 111.4658]])\n",
      "xyxyn: tensor([[0.2275, 0.0000, 0.9833, 0.9289]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 110.7ms\n",
      "Speed: 0.0ms preprocess, 110.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x416 1 -1, 102.7ms\n",
      "Speed: 0.0ms preprocess, 102.7ms inference, 15.7ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x640 3 -1s, 143.2ms\n",
      "Speed: 10.5ms preprocess, 143.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5971])\n",
      "data: tensor([[  0.0000,   6.0772, 105.5404, 146.2733,   0.5971,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (175, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 52.7702,  76.1752, 105.5404, 140.1961]])\n",
      "xywhn: tensor([[0.4886, 0.4353, 0.9772, 0.8011]])\n",
      "xyxy: tensor([[  0.0000,   6.0772, 105.5404, 146.2733]])\n",
      "xyxyn: tensor([[0.0000, 0.0347, 0.9772, 0.8358]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4220])\n",
      "data: tensor([[ 27.1068,   0.0000, 120.8122, 111.5543,   0.4220,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.9595,  55.7771,  93.7054, 111.5543]])\n",
      "xywhn: tensor([[0.6013, 0.4648, 0.7618, 0.9296]])\n",
      "xyxy: tensor([[ 27.1068,   0.0000, 120.8122, 111.5543]])\n",
      "xyxyn: tensor([[0.2204, 0.0000, 0.9822, 0.9296]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3314])\n",
      "data: tensor([[ 21.0643,   1.3384,  86.5993, 112.6155,   0.3314,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 53.8318,  56.9769,  65.5350, 111.2772]])\n",
      "xywhn: tensor([[0.4377, 0.4748, 0.5328, 0.9273]])\n",
      "xyxy: tensor([[ 21.0643,   1.3384,  86.5993, 112.6155]])\n",
      "xyxyn: tensor([[0.1713, 0.0112, 0.7041, 0.9385]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2656])\n",
      "data: tensor([[  0.4130,   2.9553,  26.2930, 111.2511,   0.2656,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 13.3530,  57.1032,  25.8800, 108.2958]])\n",
      "xywhn: tensor([[0.1086, 0.4759, 0.2104, 0.9025]])\n",
      "xyxy: tensor([[  0.4130,   2.9553,  26.2930, 111.2511]])\n",
      "xyxyn: tensor([[0.0034, 0.0246, 0.2138, 0.9271]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 tie, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 90.8ms\n",
      "Speed: 4.2ms preprocess, 90.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 -1, 105.7ms\n",
      "Speed: 2.5ms preprocess, 105.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 3 -1s, 154.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4657])\n",
      "data: tensor([[  0.0000,   5.9657, 112.6749, 167.6217,   0.4657,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (170, 114)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 56.3374,  86.7937, 112.6749, 161.6560]])\n",
      "xywhn: tensor([[0.4942, 0.5106, 0.9884, 0.9509]])\n",
      "xyxy: tensor([[  0.0000,   5.9657, 112.6749, 167.6217]])\n",
      "xyxyn: tensor([[0.0000, 0.0351, 0.9884, 0.9860]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4207])\n",
      "data: tensor([[ 27.5454,   0.0000, 120.7767, 110.5207,   0.4207,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.1610,  55.2603,  93.2313, 110.5207]])\n",
      "xywhn: tensor([[0.6029, 0.4605, 0.7580, 0.9210]])\n",
      "xyxy: tensor([[ 27.5454,   0.0000, 120.7767, 110.5207]])\n",
      "xyxyn: tensor([[0.2239, 0.0000, 0.9819, 0.9210]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3978])\n",
      "data: tensor([[ 19.1767,   1.6831,  86.6430, 112.1660,   0.3978,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 52.9098,  56.9245,  67.4663, 110.4829]])\n",
      "xywhn: tensor([[0.4302, 0.4744, 0.5485, 0.9207]])\n",
      "xyxy: tensor([[ 19.1767,   1.6831,  86.6430, 112.1660]])\n",
      "xyxyn: tensor([[0.1559, 0.0140, 0.7044, 0.9347]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3190])\n",
      "data: tensor([[  0.1313,   0.0000,  26.1166, 113.0398,   0.3190,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 13.1240,  56.5199,  25.9853, 113.0398]])\n",
      "xywhn: tensor([[0.1067, 0.4710, 0.2113, 0.9420]])\n",
      "xyxy: tensor([[  0.1313,   0.0000,  26.1166, 113.0398]])\n",
      "xyxyn: tensor([[0.0011, 0.0000, 0.2123, 0.9420]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 4.2ms preprocess, 154.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 book, 97.6ms\n",
      "Speed: 2.3ms preprocess, 97.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x480 1 -1, 108.4ms\n",
      "Speed: 3.0ms preprocess, 108.4ms inference, 15.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 3 -1s, 157.4ms\n",
      "Speed: 10.4ms preprocess, 157.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5654])\n",
      "data: tensor([[  2.1555,   6.6370, 103.5451, 161.7484,   0.5654,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (165, 119)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 52.8503,  84.1927, 101.3896, 155.1113]])\n",
      "xywhn: tensor([[0.4441, 0.5103, 0.8520, 0.9401]])\n",
      "xyxy: tensor([[  2.1555,   6.6370, 103.5451, 161.7484]])\n",
      "xyxyn: tensor([[0.0181, 0.0402, 0.8701, 0.9803]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4227])\n",
      "data: tensor([[ 28.2657,   0.0000, 120.5807, 111.3538,   0.4227,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.4232,  55.6769,  92.3150, 111.3538]])\n",
      "xywhn: tensor([[0.6051, 0.4640, 0.7505, 0.9279]])\n",
      "xyxy: tensor([[ 28.2657,   0.0000, 120.5807, 111.3538]])\n",
      "xyxyn: tensor([[0.2298, 0.0000, 0.9803, 0.9279]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3522])\n",
      "data: tensor([[ 18.6961,   0.0000,  88.1596, 112.2896,   0.3522,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 53.4278,  56.1448,  69.4634, 112.2896]])\n",
      "xywhn: tensor([[0.4344, 0.4679, 0.5647, 0.9357]])\n",
      "xyxy: tensor([[ 18.6961,   0.0000,  88.1596, 112.2896]])\n",
      "xyxyn: tensor([[0.1520, 0.0000, 0.7167, 0.9357]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3316])\n",
      "data: tensor([[  0.1190,   0.0000,  25.9606, 112.9627,   0.3316,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 13.0398,  56.4813,  25.8416, 112.9627]])\n",
      "xywhn: tensor([[0.1060, 0.4707, 0.2101, 0.9414]])\n",
      "xyxy: tensor([[  0.1190,   0.0000,  25.9606, 112.9627]])\n",
      "xyxyn: tensor([[0.0010, 0.0000, 0.2111, 0.9414]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 book, 106.3ms\n",
      "Speed: 4.2ms preprocess, 106.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x512 1 -1, 124.9ms\n",
      "Speed: 2.5ms preprocess, 124.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x640 2 -1s, 152.5ms\n",
      "Speed: 6.3ms preprocess, 152.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4054])\n",
      "data: tensor([[  0.0000,   0.6118,  96.6378, 155.5199,   0.4054,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (160, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 48.3189,  78.0658,  96.6378, 154.9081]])\n",
      "xywhn: tensor([[0.3961, 0.4879, 0.7921, 0.9682]])\n",
      "xyxy: tensor([[  0.0000,   0.6118,  96.6378, 155.5199]])\n",
      "xyxyn: tensor([[0.0000, 0.0038, 0.7921, 0.9720]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4679])\n",
      "data: tensor([[ 16.4464,   0.8355,  87.8197, 112.5726,   0.4679,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 52.1330,  56.7040,  71.3733, 111.7371]])\n",
      "xywhn: tensor([[0.4204, 0.4648, 0.5756, 0.9159]])\n",
      "xyxy: tensor([[ 16.4464,   0.8355,  87.8197, 112.5726]])\n",
      "xyxyn: tensor([[0.1326, 0.0068, 0.7082, 0.9227]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4274])\n",
      "data: tensor([[ 28.1587,   0.0000, 121.7441, 112.4458,   0.4274,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.9514,  56.2229,  93.5854, 112.4458]])\n",
      "xywhn: tensor([[0.6044, 0.4608, 0.7547, 0.9217]])\n",
      "xyxy: tensor([[ 28.1587,   0.0000, 121.7441, 112.4458]])\n",
      "xyxyn: tensor([[0.2271, 0.0000, 0.9818, 0.9217]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 book, 95.7ms\n",
      "Speed: 2.8ms preprocess, 95.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x512 1 -1, 124.9ms\n",
      "Speed: 2.3ms preprocess, 124.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x640 2 -1s, 166.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2892])\n",
      "data: tensor([[  0.0000,   5.0393,  94.9850, 154.7617,   0.2892,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (156, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 47.4925,  79.9005,  94.9850, 149.7223]])\n",
      "xywhn: tensor([[0.3925, 0.5122, 0.7850, 0.9598]])\n",
      "xyxy: tensor([[  0.0000,   5.0393,  94.9850, 154.7617]])\n",
      "xyxyn: tensor([[0.0000, 0.0323, 0.7850, 0.9921]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3970])\n",
      "data: tensor([[ 28.1346,   0.0000, 122.5913, 114.7147,   0.3970,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.3629,  57.3574,  94.4567, 114.7147]])\n",
      "xywhn: tensor([[0.6029, 0.4663, 0.7557, 0.9326]])\n",
      "xyxy: tensor([[ 28.1346,   0.0000, 122.5913, 114.7147]])\n",
      "xyxyn: tensor([[0.2251, 0.0000, 0.9807, 0.9326]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3344])\n",
      "data: tensor([[ 14.2785,   1.2230,  89.4307, 113.5976,   0.3344,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 125)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 51.8546,  57.4103,  75.1522, 112.3746]])\n",
      "xywhn: tensor([[0.4148, 0.4668, 0.6012, 0.9136]])\n",
      "xyxy: tensor([[ 14.2785,   1.2230,  89.4307, 113.5976]])\n",
      "xyxyn: tensor([[0.1142, 0.0099, 0.7154, 0.9236]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.9ms preprocess, 166.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 103.7ms\n",
      "Speed: 0.0ms preprocess, 103.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x512 1 -1, 138.6ms\n",
      "Speed: 2.0ms preprocess, 138.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x640 2 -1s, 156.4ms\n",
      "Speed: 10.5ms preprocess, 156.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4448])\n",
      "data: tensor([[  1.3881,   9.9178, 102.4036, 147.4630,   0.4448,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (151, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 51.8958,  78.6904, 101.0155, 137.5452]])\n",
      "xywhn: tensor([[0.4474, 0.5211, 0.8708, 0.9109]])\n",
      "xyxy: tensor([[  1.3881,   9.9178, 102.4036, 147.4630]])\n",
      "xyxyn: tensor([[0.0120, 0.0657, 0.8828, 0.9766]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4294])\n",
      "data: tensor([[ 20.5612,   0.0000, 113.9186, 113.7533,   0.4294,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 67.2399,  56.8767,  93.3574, 113.7533]])\n",
      "xywhn: tensor([[0.5467, 0.4701, 0.7590, 0.9401]])\n",
      "xyxy: tensor([[ 20.5612,   0.0000, 113.9186, 113.7533]])\n",
      "xyxyn: tensor([[0.1672, 0.0000, 0.9262, 0.9401]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2706])\n",
      "data: tensor([[ 18.2882,   0.0000,  87.7850, 115.3046,   0.2706,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 53.0366,  57.6523,  69.4968, 115.3046]])\n",
      "xywhn: tensor([[0.4312, 0.4765, 0.5650, 0.9529]])\n",
      "xyxy: tensor([[ 18.2882,   0.0000,  87.7850, 115.3046]])\n",
      "xyxyn: tensor([[0.1487, 0.0000, 0.7137, 0.9529]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 115.0ms\n",
      "Speed: 3.5ms preprocess, 115.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x512 1 -1, 130.6ms\n",
      "Speed: 4.0ms preprocess, 130.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x640 1 -1, 139.5ms\n",
      "Speed: 19.1ms preprocess, 139.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2924])\n",
      "data: tensor([[  0.0000,   7.8912,  90.2449, 143.3546,   0.2924,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (147, 112)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 45.1224,  75.6229,  90.2449, 135.4634]])\n",
      "xywhn: tensor([[0.4029, 0.5144, 0.8058, 0.9215]])\n",
      "xyxy: tensor([[  0.0000,   7.8912,  90.2449, 143.3546]])\n",
      "xyxyn: tensor([[0.0000, 0.0537, 0.8058, 0.9752]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3086])\n",
      "data: tensor([[ 13.9598,   0.2841, 106.1137, 118.3872,   0.3086,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 60.0367,  59.3356,  92.1539, 118.1031]])\n",
      "xywhn: tensor([[0.4962, 0.4864, 0.7616, 0.9681]])\n",
      "xyxy: tensor([[ 13.9598,   0.2841, 106.1137, 118.3872]])\n",
      "xyxyn: tensor([[0.1154, 0.0023, 0.8770, 0.9704]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 cat, 1 dog, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 2 books, 99.6ms\n",
      "Speed: 4.1ms preprocess, 99.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 -1, 136.0ms\n",
      "Speed: 15.5ms preprocess, 136.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cat, 1 dog, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 book, 97.2ms\n",
      "Speed: 0.0ms preprocess, 97.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3363])\n",
      "data: tensor([[ 24.2131,   0.1432, 111.9894, 119.5558,   0.3363,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 68.1013,  59.8495,  87.7763, 119.4126]])\n",
      "xywhn: tensor([[0.5675, 0.4906, 0.7315, 0.9788]])\n",
      "xyxy: tensor([[ 24.2131,   0.1432, 111.9894, 119.5558]])\n",
      "xyxyn: tensor([[0.2018, 0.0012, 0.9332, 0.9800]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 139.1ms\n",
      "Speed: 4.6ms preprocess, 139.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cat, 1 dog, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 1 book, 94.9ms\n",
      "Speed: 8.1ms preprocess, 94.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3905])\n",
      "data: tensor([[ 25.7002,   0.0000, 120.9894, 119.5421,   0.3905,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.3448,  59.7711,  95.2891, 119.5421]])\n",
      "xywhn: tensor([[0.5963, 0.4782, 0.7747, 0.9563]])\n",
      "xyxy: tensor([[ 25.7002,   0.0000, 120.9894, 119.5421]])\n",
      "xyxyn: tensor([[0.2089, 0.0000, 0.9837, 0.9563]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2843])\n",
      "data: tensor([[  3.5154,   7.3483,  90.0600, 119.4994,   0.2843,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 46.7877,  63.4238,  86.5446, 112.1511]])\n",
      "xywhn: tensor([[0.3804, 0.5074, 0.7036, 0.8972]])\n",
      "xyxy: tensor([[  3.5154,   7.3483,  90.0600, 119.4994]])\n",
      "xyxyn: tensor([[0.0286, 0.0588, 0.7322, 0.9560]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 151.9ms\n",
      "Speed: 6.2ms preprocess, 151.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 dog, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 90.8ms\n",
      "Speed: 3.5ms preprocess, 90.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3740])\n",
      "data: tensor([[ 36.4796,   0.2445, 124.0000, 122.6995,   0.3740,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (126, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 80.2398,  61.4720,  87.5204, 122.4551]])\n",
      "xywhn: tensor([[0.6471, 0.4879, 0.7058, 0.9719]])\n",
      "xyxy: tensor([[ 36.4796,   0.2445, 124.0000, 122.6995]])\n",
      "xyxyn: tensor([[0.2942, 0.0019, 1.0000, 0.9738]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3090])\n",
      "data: tensor([[  3.0230,   8.0056,  92.2303, 121.4635,   0.3090,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (126, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 47.6267,  64.7346,  89.2072, 113.4579]])\n",
      "xywhn: tensor([[0.3841, 0.5138, 0.7194, 0.9005]])\n",
      "xyxy: tensor([[  3.0230,   8.0056,  92.2303, 121.4635]])\n",
      "xyxyn: tensor([[0.0244, 0.0635, 0.7438, 0.9640]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 158.1ms\n",
      "Speed: 2.1ms preprocess, 158.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 dog, 2 dining tables, 1 laptop, 1 mouse, 108.9ms\n",
      "Speed: 0.0ms preprocess, 108.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4649])\n",
      "data: tensor([[3.3362e+01, 3.9798e-02, 1.2300e+02, 1.2527e+02, 4.6486e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (129, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 78.1812,  62.6554,  89.6376, 125.2311]])\n",
      "xywhn: tensor([[0.6356, 0.4857, 0.7288, 0.9708]])\n",
      "xyxy: tensor([[3.3362e+01, 3.9798e-02, 1.2300e+02, 1.2527e+02]])\n",
      "xyxyn: tensor([[2.7124e-01, 3.0851e-04, 1.0000e+00, 9.7109e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2888])\n",
      "data: tensor([[  5.3032,   2.1869,  97.9104, 122.8908,   0.2888,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (129, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 51.6068,  62.5389,  92.6072, 120.7038]])\n",
      "xywhn: tensor([[0.4196, 0.4848, 0.7529, 0.9357]])\n",
      "xyxy: tensor([[  5.3032,   2.1869,  97.9104, 122.8908]])\n",
      "xyxyn: tensor([[0.0431, 0.0170, 0.7960, 0.9526]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 1 1, 136.9ms\n",
      "Speed: 3.6ms preprocess, 136.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cat, 2 dining tables, 1 laptop, 1 mouse, 1 book, 97.2ms\n",
      "Speed: 0.0ms preprocess, 97.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4430])\n",
      "data: tensor([[ 32.2912,   0.6242, 122.0000, 123.8860,   0.4430,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 77.1456,  62.2551,  89.7088, 123.2618]])\n",
      "xywhn: tensor([[0.6323, 0.4864, 0.7353, 0.9630]])\n",
      "xyxy: tensor([[ 32.2912,   0.6242, 122.0000, 123.8860]])\n",
      "xyxyn: tensor([[0.2647, 0.0049, 1.0000, 0.9679]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3159])\n",
      "data: tensor([[ 0.0501,  0.3578, 18.6214, 28.7607,  0.3159,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 9.3358, 14.5593, 18.5713, 28.4029]])\n",
      "xywhn: tensor([[0.0765, 0.1137, 0.1522, 0.2219]])\n",
      "xyxy: tensor([[ 0.0501,  0.3578, 18.6214, 28.7607]])\n",
      "xyxyn: tensor([[0.0004, 0.0028, 0.1526, 0.2247]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2829])\n",
      "data: tensor([[  8.3688,   1.9863, 103.3583, 123.4425,   0.2829,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 55.8636,  62.7144,  94.9895, 121.4562]])\n",
      "xywhn: tensor([[0.4579, 0.4900, 0.7786, 0.9489]])\n",
      "xyxy: tensor([[  8.3688,   1.9863, 103.3583, 123.4425]])\n",
      "xyxyn: tensor([[0.0686, 0.0155, 0.8472, 0.9644]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 138.1ms\n",
      "Speed: 4.1ms preprocess, 138.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cat, 1 dog, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 1 book, 94.9ms\n",
      "Speed: 1.2ms preprocess, 94.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4127])\n",
      "data: tensor([[ 31.3226,   1.0076, 122.0000, 125.5872,   0.4127,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.6613,  63.2974,  90.6774, 124.5797]])\n",
      "xywhn: tensor([[0.6284, 0.4945, 0.7433, 0.9733]])\n",
      "xyxy: tensor([[ 31.3226,   1.0076, 122.0000, 125.5872]])\n",
      "xyxyn: tensor([[0.2567, 0.0079, 1.0000, 0.9812]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2963])\n",
      "data: tensor([[  9.1868,   2.5413, 103.5291, 124.9281,   0.2963,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 56.3580,  63.7347,  94.3423, 122.3868]])\n",
      "xywhn: tensor([[0.4620, 0.4979, 0.7733, 0.9561]])\n",
      "xyxy: tensor([[  9.1868,   2.5413, 103.5291, 124.9281]])\n",
      "xyxyn: tensor([[0.0753, 0.0199, 0.8486, 0.9760]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 3 -1s, 139.2ms\n",
      "Speed: 5.4ms preprocess, 139.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 1 person, 1 dog, 1 cup, 3 dining tables, 1 laptop, 1 mouse, 1 remote, 1 book, 95.8ms\n",
      "Speed: 0.0ms preprocess, 95.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4053])\n",
      "data: tensor([[  7.8402,   1.9186,  96.7446, 118.5332,   0.4053,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 119)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 52.2924,  60.2259,  88.9044, 116.6145]])\n",
      "xywhn: tensor([[0.4394, 0.4705, 0.7471, 0.9111]])\n",
      "xyxy: tensor([[  7.8402,   1.9186,  96.7446, 118.5332]])\n",
      "xyxyn: tensor([[0.0659, 0.0150, 0.8130, 0.9260]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3073])\n",
      "data: tensor([[ 39.9203,   0.2536, 119.0000, 125.4075,   0.3073,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 119)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 79.4601,  62.8305,  79.0797, 125.1539]])\n",
      "xywhn: tensor([[0.6677, 0.4909, 0.6645, 0.9778]])\n",
      "xyxy: tensor([[ 39.9203,   0.2536, 119.0000, 125.4075]])\n",
      "xyxyn: tensor([[0.3355, 0.0020, 1.0000, 0.9797]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2679])\n",
      "data: tensor([[ 29.0974,   0.0000,  90.6372, 120.2487,   0.2679,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 119)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 59.8673,  60.1243,  61.5398, 120.2487]])\n",
      "xywhn: tensor([[0.5031, 0.4697, 0.5171, 0.9394]])\n",
      "xyxy: tensor([[ 29.0974,   0.0000,  90.6372, 120.2487]])\n",
      "xyxyn: tensor([[0.2445, 0.0000, 0.7617, 0.9394]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 1 -1, 143.2ms\n",
      "Speed: 4.4ms preprocess, 143.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 1 person, 1 dog, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 101.1ms\n",
      "Speed: 0.5ms preprocess, 101.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3688])\n",
      "data: tensor([[  4.4051,   2.2739,  96.2375, 119.0564,   0.3688,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (127, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 50.3213,  60.6651,  91.8324, 116.7825]])\n",
      "xywhn: tensor([[0.4265, 0.4777, 0.7782, 0.9195]])\n",
      "xyxy: tensor([[  4.4051,   2.2739,  96.2375, 119.0564]])\n",
      "xyxyn: tensor([[0.0373, 0.0179, 0.8156, 0.9375]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 1 -1, 1 1, 119.6ms\n",
      "Speed: 3.6ms preprocess, 119.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 2 persons, 1 dog, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 92.1ms\n",
      "Speed: 1.6ms preprocess, 92.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4744])\n",
      "data: tensor([[  3.4350,   0.0000,  89.0088, 116.6823,   0.4744,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (129, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 46.2219,  58.3411,  85.5738, 116.6823]])\n",
      "xywhn: tensor([[0.3985, 0.4523, 0.7377, 0.9045]])\n",
      "xyxy: tensor([[  3.4350,   0.0000,  89.0088, 116.6823]])\n",
      "xyxyn: tensor([[0.0296, 0.0000, 0.7673, 0.9045]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.2527])\n",
      "data: tensor([[  0.1369,  98.2718,  78.9301, 128.9434,   0.2527,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (129, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 39.5335, 113.6076,  78.7933,  30.6716]])\n",
      "xywhn: tensor([[0.3408, 0.8807, 0.6793, 0.2378]])\n",
      "xyxy: tensor([[  0.1369,  98.2718,  78.9301, 128.9434]])\n",
      "xyxyn: tensor([[0.0012, 0.7618, 0.6804, 0.9996]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 2 -1s, 1 1, 142.0ms\n",
      "Speed: 3.5ms preprocess, 142.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 1 person, 1 dog, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 94.6ms\n",
      "Speed: 1.9ms preprocess, 94.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5178])\n",
      "data: tensor([[  2.4794,   0.0000,  87.5476, 114.7854,   0.5178,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 115)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 45.0135,  57.3927,  85.0681, 114.7854]])\n",
      "xywhn: tensor([[0.3914, 0.4484, 0.7397, 0.8968]])\n",
      "xyxy: tensor([[  2.4794,   0.0000,  87.5476, 114.7854]])\n",
      "xyxyn: tensor([[0.0216, 0.0000, 0.7613, 0.8968]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3161])\n",
      "data: tensor([[1.0806e-01, 9.7770e+01, 7.8222e+01, 1.2793e+02, 3.1614e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 115)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 39.1652, 112.8510,  78.1143,  30.1624]])\n",
      "xywhn: tensor([[0.3406, 0.8816, 0.6793, 0.2356]])\n",
      "xyxy: tensor([[1.0806e-01, 9.7770e+01, 7.8222e+01, 1.2793e+02]])\n",
      "xyxyn: tensor([[9.3962e-04, 7.6383e-01, 6.8019e-01, 9.9947e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2695])\n",
      "data: tensor([[ 19.7925,   0.0000, 115.0000, 113.9432,   0.2695,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 115)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 67.3962,  56.9716,  95.2075, 113.9432]])\n",
      "xywhn: tensor([[0.5861, 0.4451, 0.8279, 0.8902]])\n",
      "xyxy: tensor([[ 19.7925,   0.0000, 115.0000, 113.9432]])\n",
      "xyxyn: tensor([[0.1721, 0.0000, 1.0000, 0.8902]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 2 -1s, 1 1, 135.7ms\n",
      "Speed: 2.8ms preprocess, 135.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 1 person, 1 dog, 1 chair, 1 dining table, 1 laptop, 1 mouse, 92.7ms\n",
      "Speed: 0.7ms preprocess, 92.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5980])\n",
      "data: tensor([[  0.0000,   0.0000,  82.5438, 113.3098,   0.5980,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (127, 114)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 41.2719,  56.6549,  82.5438, 113.3098]])\n",
      "xywhn: tensor([[0.3620, 0.4461, 0.7241, 0.8922]])\n",
      "xyxy: tensor([[  0.0000,   0.0000,  82.5438, 113.3098]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.7241, 0.8922]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3187])\n",
      "data: tensor([[  0.0000,  97.3063,  77.2679, 126.9332,   0.3187,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (127, 114)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 38.6340, 112.1198,  77.2679,  29.6269]])\n",
      "xywhn: tensor([[0.3389, 0.8828, 0.6778, 0.2333]])\n",
      "xyxy: tensor([[  0.0000,  97.3063,  77.2679, 126.9332]])\n",
      "xyxyn: tensor([[0.0000, 0.7662, 0.6778, 0.9995]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2544])\n",
      "data: tensor([[ 17.6814,   0.0000, 114.0000, 112.3137,   0.2544,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (127, 114)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 65.8407,  56.1568,  96.3186, 112.3137]])\n",
      "xywhn: tensor([[0.5775, 0.4422, 0.8449, 0.8844]])\n",
      "xyxy: tensor([[ 17.6814,   0.0000, 114.0000, 112.3137]])\n",
      "xyxyn: tensor([[0.1551, 0.0000, 1.0000, 0.8844]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 2 -1s, 132.8ms\n",
      "Speed: 4.2ms preprocess, 132.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 2 persons, 1 dog, 1 cup, 1 chair, 2 dining tables, 2 laptops, 1 mouse, 109.4ms\n",
      "Speed: 0.0ms preprocess, 109.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5216])\n",
      "data: tensor([[  1.3780,   0.0000,  79.9335, 114.1105,   0.5216,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (127, 115)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 40.6558,  57.0553,  78.5555, 114.1105]])\n",
      "xywhn: tensor([[0.3535, 0.4493, 0.6831, 0.8985]])\n",
      "xyxy: tensor([[  1.3780,   0.0000,  79.9335, 114.1105]])\n",
      "xyxyn: tensor([[0.0120, 0.0000, 0.6951, 0.8985]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2998])\n",
      "data: tensor([[ 28.3062,   0.0000, 115.0000, 112.9161,   0.2998,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (127, 115)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 71.6531,  56.4580,  86.6938, 112.9161]])\n",
      "xywhn: tensor([[0.6231, 0.4446, 0.7539, 0.8891]])\n",
      "xyxy: tensor([[ 28.3062,   0.0000, 115.0000, 112.9161]])\n",
      "xyxyn: tensor([[0.2461, 0.0000, 1.0000, 0.8891]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 1 -1, 139.7ms\n",
      "Speed: 2.1ms preprocess, 139.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 2 persons, 1 dog, 1 cup, 1 chair, 2 dining tables, 2 laptops, 1 mouse, 110.6ms\n",
      "Speed: 0.0ms preprocess, 110.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3748])\n",
      "data: tensor([[  0.0000,   1.3362,  79.4897, 116.3943,   0.3748,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (124, 112)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 39.7448,  58.8652,  79.4897, 115.0581]])\n",
      "xywhn: tensor([[0.3549, 0.4747, 0.7097, 0.9279]])\n",
      "xyxy: tensor([[  0.0000,   1.3362,  79.4897, 116.3943]])\n",
      "xyxyn: tensor([[0.0000, 0.0108, 0.7097, 0.9387]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 1 -1, 137.8ms\n",
      "Speed: 2.9ms preprocess, 137.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 2 persons, 1 dog, 1 cup, 1 chair, 2 dining tables, 2 laptops, 1 mouse, 1 remote, 100.4ms\n",
      "Speed: 0.0ms preprocess, 100.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4758])\n",
      "data: tensor([[  0.6290,   0.2964,  61.5580, 114.7310,   0.4758,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 109)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 31.0935,  57.5137,  60.9290, 114.4346]])\n",
      "xywhn: tensor([[0.2853, 0.4714, 0.5590, 0.9380]])\n",
      "xyxy: tensor([[  0.6290,   0.2964,  61.5580, 114.7310]])\n",
      "xyxyn: tensor([[0.0058, 0.0024, 0.5648, 0.9404]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 1 -1, 129.3ms\n",
      "Speed: 5.8ms preprocess, 129.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 2 persons, 1 dog, 1 cup, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 111.3ms\n",
      "Speed: 7.8ms preprocess, 111.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5376])\n",
      "data: tensor([[  1.1515,   0.0000,  59.1849, 113.5312,   0.5376,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (120, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 30.1682,  56.7656,  58.0334, 113.5312]])\n",
      "xywhn: tensor([[0.2793, 0.4730, 0.5373, 0.9461]])\n",
      "xyxy: tensor([[  1.1515,   0.0000,  59.1849, 113.5312]])\n",
      "xyxyn: tensor([[0.0107, 0.0000, 0.5480, 0.9461]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 1 -1, 137.6ms\n",
      "Speed: 4.0ms preprocess, 137.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 3 persons, 1 dog, 1 cup, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 99.0ms\n",
      "Speed: 4.5ms preprocess, 99.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3436])\n",
      "data: tensor([[  8.0859,   1.6352,  60.1065, 104.8657,   0.3436,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 109)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 34.0962,  53.2505,  52.0206, 103.2305]])\n",
      "xywhn: tensor([[0.3128, 0.4475, 0.4773, 0.8675]])\n",
      "xyxy: tensor([[  8.0859,   1.6352,  60.1065, 104.8657]])\n",
      "xyxyn: tensor([[0.0742, 0.0137, 0.5514, 0.8812]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 (no detections), 138.9ms\n",
      "Speed: 2.6ms preprocess, 138.9ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 dog, 1 cup, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 97.6ms\n",
      "Speed: 0.5ms preprocess, 97.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 140.5ms\n",
      "Speed: 13.9ms preprocess, 140.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 101.9ms\n",
      "Speed: 0.0ms preprocess, 101.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2536])\n",
      "data: tensor([[ 16.9590,   1.3017,  62.1372, 104.9801,   0.2536,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (115, 113)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 39.5481,  53.1409,  45.1782, 103.6784]])\n",
      "xywhn: tensor([[0.3500, 0.4621, 0.3998, 0.9016]])\n",
      "xyxy: tensor([[ 16.9590,   1.3017,  62.1372, 104.9801]])\n",
      "xyxyn: tensor([[0.1501, 0.0113, 0.5499, 0.9129]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 141.7ms\n",
      "Speed: 3.1ms preprocess, 141.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bird, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 111.8ms\n",
      "Speed: 7.2ms preprocess, 111.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4694])\n",
      "data: tensor([[ 23.4834,   2.4482,  68.9923, 106.3202,   0.4694,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (114, 117)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 46.2379,  54.3842,  45.5089, 103.8720]])\n",
      "xywhn: tensor([[0.3952, 0.4771, 0.3890, 0.9112]])\n",
      "xyxy: tensor([[ 23.4834,   2.4482,  68.9923, 106.3202]])\n",
      "xyxyn: tensor([[0.2007, 0.0215, 0.5897, 0.9326]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2519])\n",
      "data: tensor([[ 22.1772,   0.0000, 113.9956, 104.9945,   0.2519,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (114, 117)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 68.0864,  52.4973,  91.8184, 104.9945]])\n",
      "xywhn: tensor([[0.5819, 0.4605, 0.7848, 0.9210]])\n",
      "xyxy: tensor([[ 22.1772,   0.0000, 113.9956, 104.9945]])\n",
      "xyxyn: tensor([[0.1895, 0.0000, 0.9743, 0.9210]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 139.7ms\n",
      "Speed: 2.7ms preprocess, 139.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bird, 1 dog, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 110.5ms\n",
      "Speed: 0.0ms preprocess, 110.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5666])\n",
      "data: tensor([[  0.0000,   1.7807,  72.9813, 108.1808,   0.5666,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (112, 117)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 36.4907,  54.9808,  72.9813, 106.4001]])\n",
      "xywhn: tensor([[0.3119, 0.4909, 0.6238, 0.9500]])\n",
      "xyxy: tensor([[  0.0000,   1.7807,  72.9813, 108.1808]])\n",
      "xyxyn: tensor([[0.0000, 0.0159, 0.6238, 0.9659]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2916])\n",
      "data: tensor([[ 22.0753,   0.0000, 114.3028, 104.4105,   0.2916,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (112, 117)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 68.1890,  52.2052,  92.2275, 104.4105]])\n",
      "xywhn: tensor([[0.5828, 0.4661, 0.7883, 0.9322]])\n",
      "xyxy: tensor([[ 22.0753,   0.0000, 114.3028, 104.4105]])\n",
      "xyxyn: tensor([[0.1887, 0.0000, 0.9769, 0.9322]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 608x640 2 -1s, 138.2ms\n",
      "Speed: 4.2ms preprocess, 138.2ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bird, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 93.5ms\n",
      "Speed: 1.8ms preprocess, 93.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5595])\n",
      "data: tensor([[  0.0000,   0.4591,  69.6321, 101.9190,   0.5595,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (110, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 34.8161,  51.1890,  69.6321, 101.4599]])\n",
      "xywhn: tensor([[0.2951, 0.4654, 0.5901, 0.9224]])\n",
      "xyxy: tensor([[  0.0000,   0.4591,  69.6321, 101.9190]])\n",
      "xyxyn: tensor([[0.0000, 0.0042, 0.5901, 0.9265]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2648])\n",
      "data: tensor([[ 22.6217,   0.0000, 116.6405,  99.3850,   0.2648,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (110, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[69.6311, 49.6925, 94.0189, 99.3850]])\n",
      "xywhn: tensor([[0.5901, 0.4518, 0.7968, 0.9035]])\n",
      "xyxy: tensor([[ 22.6217,   0.0000, 116.6405,  99.3850]])\n",
      "xyxyn: tensor([[0.1917, 0.0000, 0.9885, 0.9035]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 608x640 3 -1s, 1 1, 143.2ms\n",
      "Speed: 6.8ms preprocess, 143.2ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bird, 1 dog, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 1 book, 88.9ms\n",
      "Speed: 6.3ms preprocess, 88.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5668])\n",
      "data: tensor([[  0.0000,   0.0000,  69.7927, 100.7213,   0.5668,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (110, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 34.8964,  50.3607,  69.7927, 100.7213]])\n",
      "xywhn: tensor([[0.2957, 0.4578, 0.5915, 0.9156]])\n",
      "xyxy: tensor([[  0.0000,   0.0000,  69.7927, 100.7213]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.5915, 0.9156]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3025])\n",
      "data: tensor([[ 27.9309,   0.0000, 115.8635,  99.2700,   0.3025,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (110, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[71.8972, 49.6350, 87.9326, 99.2700]])\n",
      "xywhn: tensor([[0.6093, 0.4512, 0.7452, 0.9025]])\n",
      "xyxy: tensor([[ 27.9309,   0.0000, 115.8635,  99.2700]])\n",
      "xyxyn: tensor([[0.2367, 0.0000, 0.9819, 0.9025]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2734])\n",
      "data: tensor([[ 0.0000, 35.4065, 48.7771, 87.6372,  0.2734,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (110, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[24.3885, 61.5218, 48.7771, 52.2307]])\n",
      "xywhn: tensor([[0.2067, 0.5593, 0.4134, 0.4748]])\n",
      "xyxy: tensor([[ 0.0000, 35.4065, 48.7771, 87.6372]])\n",
      "xyxyn: tensor([[0.0000, 0.3219, 0.4134, 0.7967]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.2731])\n",
      "data: tensor([[ 71.0185, 103.0952, 103.7241, 109.5129,   0.2731,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (110, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 87.3713, 106.3041,  32.7056,   6.4177]])\n",
      "xywhn: tensor([[0.7404, 0.9664, 0.2772, 0.0583]])\n",
      "xyxy: tensor([[ 71.0185, 103.0952, 103.7241, 109.5129]])\n",
      "xyxyn: tensor([[0.6019, 0.9372, 0.8790, 0.9956]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 576x640 4 -1s, 1 1, 169.8ms\n",
      "Speed: 2.4ms preprocess, 169.8ms inference, 0.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bird, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 1 book, 107.4ms\n",
      "Speed: 3.8ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5492])\n",
      "data: tensor([[0.0000e+00, 5.3644e-02, 6.8057e+01, 8.7519e+01, 5.4920e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (108, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[34.0287, 43.7861, 68.0573, 87.4649]])\n",
      "xywhn: tensor([[0.2836, 0.4054, 0.5671, 0.8099]])\n",
      "xyxy: tensor([[0.0000e+00, 5.3644e-02, 6.8057e+01, 8.7519e+01]])\n",
      "xyxyn: tensor([[0.0000e+00, 4.9671e-04, 5.6714e-01, 8.1036e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3951])\n",
      "data: tensor([[  0.0000,  79.6414,  88.5344, 107.9581,   0.3951,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (108, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[44.2672, 93.7998, 88.5344, 28.3167]])\n",
      "xywhn: tensor([[0.3689, 0.8685, 0.7378, 0.2622]])\n",
      "xyxy: tensor([[  0.0000,  79.6414,  88.5344, 107.9581]])\n",
      "xyxyn: tensor([[0.0000, 0.7374, 0.7378, 0.9996]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3547])\n",
      "data: tensor([[ 0.1021, 33.7174, 52.7809, 86.9597,  0.3547,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (108, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[26.4415, 60.3385, 52.6788, 53.2422]])\n",
      "xywhn: tensor([[0.2203, 0.5587, 0.4390, 0.4930]])\n",
      "xyxy: tensor([[ 0.1021, 33.7174, 52.7809, 86.9597]])\n",
      "xyxyn: tensor([[0.0009, 0.3122, 0.4398, 0.8052]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3273])\n",
      "data: tensor([[30.2805, 37.3598, 52.1348, 88.9919,  0.3273,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (108, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.2077, 63.1758, 21.8543, 51.6322]])\n",
      "xywhn: tensor([[0.3434, 0.5850, 0.1821, 0.4781]])\n",
      "xyxy: tensor([[30.2805, 37.3598, 52.1348, 88.9919]])\n",
      "xyxyn: tensor([[0.2523, 0.3459, 0.4345, 0.8240]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2822])\n",
      "data: tensor([[ 26.0508,   0.0000, 118.0602,  99.1086,   0.2822,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (108, 120)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[72.0555, 49.5543, 92.0094, 99.1086]])\n",
      "xywhn: tensor([[0.6005, 0.4588, 0.7667, 0.9177]])\n",
      "xyxy: tensor([[ 26.0508,   0.0000, 118.0602,  99.1086]])\n",
      "xyxyn: tensor([[0.2171, 0.0000, 0.9838, 0.9177]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 416x640 (no detections), 141.8ms\n",
      "Speed: 0.0ms preprocess, 141.8ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 544x640 2 -1s, 157.5ms\n",
      "Speed: 15.4ms preprocess, 157.5ms inference, 15.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 116.9ms\n",
      "Speed: 0.0ms preprocess, 116.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4172])\n",
      "data: tensor([[ 0.0000,  0.0000, 82.7846, 93.6238,  0.4172,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (106, 126)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.3923, 46.8119, 82.7846, 93.6238]])\n",
      "xywhn: tensor([[0.3285, 0.4416, 0.6570, 0.8832]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 82.7846, 93.6238]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6570, 0.8832]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2896])\n",
      "data: tensor([[ 31.4488,   0.0000, 126.0000, 100.2420,   0.2896,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (106, 126)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 78.7244,  50.1210,  94.5512, 100.2420]])\n",
      "xywhn: tensor([[0.6248, 0.4728, 0.7504, 0.9457]])\n",
      "xyxy: tensor([[ 31.4488,   0.0000, 126.0000, 100.2420]])\n",
      "xyxyn: tensor([[0.2496, 0.0000, 1.0000, 0.9457]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 416x640 1 -1, 104.3ms\n",
      "Speed: 2.9ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 512x640 1 -1, 139.7ms\n",
      "Speed: 2.2ms preprocess, 139.7ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4724])\n",
      "data: tensor([[ 0.0000,  0.0000, 36.9966, 61.6053,  0.4724,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (64, 106)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[18.4983, 30.8026, 36.9966, 61.6053]])\n",
      "xywhn: tensor([[0.1745, 0.4813, 0.3490, 0.9626]])\n",
      "xyxy: tensor([[ 0.0000,  0.0000, 36.9966, 61.6053]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.3490, 0.9626]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4122])\n",
      "data: tensor([[ 0.3269,  0.0000, 91.5171, 96.8096,  0.4122,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (106, 134)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[45.9220, 48.4048, 91.1902, 96.8096]])\n",
      "xywhn: tensor([[0.3427, 0.4566, 0.6805, 0.9133]])\n",
      "xyxy: tensor([[ 0.3269,  0.0000, 91.5171, 96.8096]])\n",
      "xyxyn: tensor([[0.0024, 0.0000, 0.6830, 0.9133]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 90.2ms\n",
      "Speed: 2.6ms preprocess, 90.2ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 -1s, 84.7ms\n",
      "Speed: 2.0ms preprocess, 84.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 -1, 128.6ms\n",
      "Speed: 4.5ms preprocess, 128.6ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5123])\n",
      "data: tensor([[ 0.0000,  0.1570, 36.9951, 60.2359,  0.5123,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (62, 107)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[18.4975, 30.1964, 36.9951, 60.0789]])\n",
      "xywhn: tensor([[0.1729, 0.4870, 0.3457, 0.9690]])\n",
      "xyxy: tensor([[ 0.0000,  0.1570, 36.9951, 60.2359]])\n",
      "xyxyn: tensor([[0.0000, 0.0025, 0.3457, 0.9715]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4236])\n",
      "data: tensor([[ 42.4890,  22.8326, 107.0000,  61.4931,   0.4236,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (62, 107)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[74.7445, 42.1629, 64.5110, 38.6605]])\n",
      "xywhn: tensor([[0.6985, 0.6800, 0.6029, 0.6236]])\n",
      "xyxy: tensor([[ 42.4890,  22.8326, 107.0000,  61.4931]])\n",
      "xyxyn: tensor([[0.3971, 0.3683, 1.0000, 0.9918]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4732])\n",
      "data: tensor([[ 0.2005,  0.0000, 98.5155, 91.5872,  0.4732,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (107, 140)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[49.3580, 45.7936, 98.3149, 91.5872]])\n",
      "xywhn: tensor([[0.3526, 0.4280, 0.7022, 0.8560]])\n",
      "xyxy: tensor([[ 0.2005,  0.0000, 98.5155, 91.5872]])\n",
      "xyxyn: tensor([[0.0014, 0.0000, 0.7037, 0.8560]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 91.6ms\n",
      "Speed: 2.9ms preprocess, 91.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 -1s, 91.3ms\n",
      "Speed: 1.8ms preprocess, 91.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 -1, 122.8ms\n",
      "Speed: 2.7ms preprocess, 122.8ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5360])\n",
      "data: tensor([[5.3082e-02, 1.6925e-01, 3.5855e+01, 5.8615e+01, 5.3599e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (61, 105)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[17.9540, 29.3923, 35.8019, 58.4461]])\n",
      "xywhn: tensor([[0.1710, 0.4818, 0.3410, 0.9581]])\n",
      "xyxy: tensor([[5.3082e-02, 1.6925e-01, 3.5855e+01, 5.8615e+01]])\n",
      "xyxyn: tensor([[5.0554e-04, 2.7746e-03, 3.4148e-01, 9.6091e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2590])\n",
      "data: tensor([[ 39.2318,  17.2802, 105.0000,  60.3015,   0.2590,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (61, 105)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[72.1159, 38.7909, 65.7682, 43.0213]])\n",
      "xywhn: tensor([[0.6868, 0.6359, 0.6264, 0.7053]])\n",
      "xyxy: tensor([[ 39.2318,  17.2802, 105.0000,  60.3015]])\n",
      "xyxyn: tensor([[0.3736, 0.2833, 1.0000, 0.9885]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5242])\n",
      "data: tensor([[1.9810e-02, 0.0000e+00, 9.9361e+01, 9.1501e+01, 5.2422e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (109, 144)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[49.6904, 45.7505, 99.3412, 91.5010]])\n",
      "xywhn: tensor([[0.3451, 0.4197, 0.6899, 0.8395]])\n",
      "xyxy: tensor([[1.9810e-02, 0.0000e+00, 9.9361e+01, 9.1501e+01]])\n",
      "xyxyn: tensor([[1.3757e-04, 0.0000e+00, 6.9001e-01, 8.3946e-01]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 92.1ms\n",
      "Speed: 1.8ms preprocess, 92.1ms inference, 9.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 1 -1, 155.5ms\n",
      "Speed: 0.0ms preprocess, 155.5ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 384x640 1 -1, 93.4ms\n",
      "Speed: 2.0ms preprocess, 93.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2841])\n",
      "data: tensor([[ 24.5712,   0.0000,  89.5800, 106.7087,   0.2841,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (111, 93)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 57.0756,  53.3543,  65.0088, 106.7087]])\n",
      "xywhn: tensor([[0.6137, 0.4807, 0.6990, 0.9613]])\n",
      "xyxy: tensor([[ 24.5712,   0.0000,  89.5800, 106.7087]])\n",
      "xyxyn: tensor([[0.2642, 0.0000, 0.9632, 0.9613]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2573])\n",
      "data: tensor([[ 35.6295,  15.3686, 105.0000,  58.5017,   0.2573,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (59, 105)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[70.3148, 36.9351, 69.3705, 43.1331]])\n",
      "xywhn: tensor([[0.6697, 0.6260, 0.6607, 0.7311]])\n",
      "xyxy: tensor([[ 35.6295,  15.3686, 105.0000,  58.5017]])\n",
      "xyxyn: tensor([[0.3393, 0.2605, 1.0000, 0.9916]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 544x640 2 -1s, 140.5ms\n",
      "Speed: 1.4ms preprocess, 140.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 95.5ms\n",
      "Speed: 0.0ms preprocess, 95.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6410])\n",
      "data: tensor([[ 0.0000,  0.4346, 81.9099, 98.5939,  0.6410,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (110, 137)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[40.9550, 49.5142, 81.9099, 98.1593]])\n",
      "xywhn: tensor([[0.2989, 0.4501, 0.5979, 0.8924]])\n",
      "xyxy: tensor([[ 0.0000,  0.4346, 81.9099, 98.5939]])\n",
      "xyxyn: tensor([[0.0000, 0.0040, 0.5979, 0.8963]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2540])\n",
      "data: tensor([[ 32.1378,  18.7425, 137.0000, 103.9836,   0.2540,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (110, 137)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 84.5689,  61.3631, 104.8622,  85.2411]])\n",
      "xywhn: tensor([[0.6173, 0.5578, 0.7654, 0.7749]])\n",
      "xyxy: tensor([[ 32.1378,  18.7425, 137.0000, 103.9836]])\n",
      "xyxyn: tensor([[0.2346, 0.1704, 1.0000, 0.9453]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 1 -1, 136.1ms\n",
      "Speed: 6.7ms preprocess, 136.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 (no detections), 102.8ms\n",
      "Speed: 0.0ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3074])\n",
      "data: tensor([[ 25.0927,   0.0000,  93.0000, 103.1012,   0.3074,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (109, 93)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 59.0463,  51.5506,  67.9073, 103.1012]])\n",
      "xywhn: tensor([[0.6349, 0.4729, 0.7302, 0.9459]])\n",
      "xyxy: tensor([[ 25.0927,   0.0000,  93.0000, 103.1012]])\n",
      "xyxyn: tensor([[0.2698, 0.0000, 1.0000, 0.9459]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 544x640 2 -1s, 140.8ms\n",
      "Speed: 2.9ms preprocess, 140.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 2 dining tables, 1 laptop, 1 mouse, 106.3ms\n",
      "Speed: 3.0ms preprocess, 106.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5314])\n",
      "data: tensor([[  0.0000,   0.0000,  80.2393, 102.3526,   0.5314,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (112, 132)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 40.1197,  51.1763,  80.2393, 102.3526]])\n",
      "xywhn: tensor([[0.3039, 0.4569, 0.6079, 0.9139]])\n",
      "xyxy: tensor([[  0.0000,   0.0000,  80.2393, 102.3526]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6079, 0.9139]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2923])\n",
      "data: tensor([[ 22.4358,   3.8909, 129.1732, 105.8966,   0.2923,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (112, 132)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.8045,  54.8937, 106.7374, 102.0058]])\n",
      "xywhn: tensor([[0.5743, 0.4901, 0.8086, 0.9108]])\n",
      "xyxy: tensor([[ 22.4358,   3.8909, 129.1732, 105.8966]])\n",
      "xyxyn: tensor([[0.1700, 0.0347, 0.9786, 0.9455]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 (no detections), 137.9ms\n",
      "Speed: 4.2ms preprocess, 137.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 (no detections), 94.3ms\n",
      "Speed: 15.6ms preprocess, 94.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 576x640 3 -1s, 139.5ms\n",
      "Speed: 12.8ms preprocess, 139.5ms inference, 0.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 110.9ms\n",
      "Speed: 1.8ms preprocess, 110.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5522])\n",
      "data: tensor([[  0.0000,   0.0000,  80.1772, 100.4486,   0.5522,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (113, 128)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 40.0886,  50.2243,  80.1772, 100.4486]])\n",
      "xywhn: tensor([[0.3132, 0.4445, 0.6264, 0.8889]])\n",
      "xyxy: tensor([[  0.0000,   0.0000,  80.1772, 100.4486]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6264, 0.8889]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3767])\n",
      "data: tensor([[ 22.8364,  22.6073, 124.5179, 103.3204,   0.3767,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (113, 128)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.6771,  62.9638, 101.6815,  80.7131]])\n",
      "xywhn: tensor([[0.5756, 0.5572, 0.7944, 0.7143]])\n",
      "xyxy: tensor([[ 22.8364,  22.6073, 124.5179, 103.3204]])\n",
      "xyxyn: tensor([[0.1784, 0.2001, 0.9728, 0.9143]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2775])\n",
      "data: tensor([[ 32.1942,   0.0000, 128.0000, 105.8933,   0.2775,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (113, 128)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 80.0971,  52.9466,  95.8058, 105.8933]])\n",
      "xywhn: tensor([[0.6258, 0.4686, 0.7485, 0.9371]])\n",
      "xyxy: tensor([[ 32.1942,   0.0000, 128.0000, 105.8933]])\n",
      "xyxyn: tensor([[0.2515, 0.0000, 1.0000, 0.9371]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 -1, 115.3ms\n",
      "Speed: 0.0ms preprocess, 115.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 608x640 2 -1s, 148.5ms\n",
      "Speed: 5.8ms preprocess, 148.5ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4259])\n",
      "data: tensor([[ 56.1826,  13.2078, 101.7212,  56.8852,   0.4259,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (58, 102)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[78.9519, 35.0465, 45.5387, 43.6775]])\n",
      "xywhn: tensor([[0.7740, 0.6042, 0.4465, 0.7531]])\n",
      "xyxy: tensor([[ 56.1826,  13.2078, 101.7212,  56.8852]])\n",
      "xyxyn: tensor([[0.5508, 0.2277, 0.9973, 0.9808]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6573])\n",
      "data: tensor([[0.0000e+00, 4.1456e-03, 7.5515e+01, 9.9568e+01, 6.5731e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (115, 126)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[37.7577, 49.7859, 75.5154, 99.5634]])\n",
      "xywhn: tensor([[0.2997, 0.4329, 0.5993, 0.8658]])\n",
      "xyxy: tensor([[0.0000e+00, 4.1456e-03, 7.5515e+01, 9.9568e+01]])\n",
      "xyxyn: tensor([[0.0000e+00, 3.6049e-05, 5.9933e-01, 8.6580e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4048])\n",
      "data: tensor([[ 18.6725,   0.0000, 120.9882, 104.1038,   0.4048,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (115, 126)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.8304,  52.0519, 102.3157, 104.1038]])\n",
      "xywhn: tensor([[0.5542, 0.4526, 0.8120, 0.9053]])\n",
      "xyxy: tensor([[ 18.6725,   0.0000, 120.9882, 104.1038]])\n",
      "xyxyn: tensor([[0.1482, 0.0000, 0.9602, 0.9053]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 1 cup, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 109.6ms\n",
      "Speed: 3.5ms preprocess, 109.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 608x640 2 -1s, 1 1, 142.5ms\n",
      "Speed: 9.2ms preprocess, 142.5ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 97.9ms\n",
      "Speed: 2.0ms preprocess, 97.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5360])\n",
      "data: tensor([[  0.0000,   0.0000,  78.5113, 103.7865,   0.5360,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (117, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 39.2557,  51.8932,  78.5113, 103.7865]])\n",
      "xywhn: tensor([[0.3166, 0.4435, 0.6332, 0.8871]])\n",
      "xyxy: tensor([[  0.0000,   0.0000,  78.5113, 103.7865]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6332, 0.8871]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3439])\n",
      "data: tensor([[2.6460e-02, 4.5988e-01, 2.3788e+01, 2.9630e+01, 3.4393e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (117, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[11.9073, 15.0449, 23.7616, 29.1701]])\n",
      "xywhn: tensor([[0.0960, 0.1286, 0.1916, 0.2493]])\n",
      "xyxy: tensor([[2.6460e-02, 4.5988e-01, 2.3788e+01, 2.9630e+01]])\n",
      "xyxyn: tensor([[2.1338e-04, 3.9306e-03, 1.9184e-01, 2.5325e-01]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3262])\n",
      "data: tensor([[ 17.8238,   2.2864, 120.3237, 109.4964,   0.3262,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (117, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.0737,  55.8914, 102.4998, 107.2100]])\n",
      "xywhn: tensor([[0.5570, 0.4777, 0.8266, 0.9163]])\n",
      "xyxy: tensor([[ 17.8238,   2.2864, 120.3237, 109.4964]])\n",
      "xyxyn: tensor([[0.1437, 0.0195, 0.9704, 0.9359]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 1 1, 153.9ms\n",
      "Speed: 2.2ms preprocess, 153.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 91.3ms\n",
      "Speed: 2.0ms preprocess, 91.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.4067])\n",
      "data: tensor([[ 0.0956,  1.1120, 23.6617, 29.7323,  0.4067,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[11.8786, 15.4221, 23.5661, 28.6203]])\n",
      "xywhn: tensor([[0.0966, 0.1296, 0.1916, 0.2405]])\n",
      "xyxy: tensor([[ 0.0956,  1.1120, 23.6617, 29.7323]])\n",
      "xyxyn: tensor([[0.0008, 0.0093, 0.1924, 0.2499]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3826])\n",
      "data: tensor([[ 24.0838,   0.0000, 120.3123, 110.5279,   0.3826,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.1981,  55.2640,  96.2285, 110.5279]])\n",
      "xywhn: tensor([[0.5870, 0.4644, 0.7823, 0.9288]])\n",
      "xyxy: tensor([[ 24.0838,   0.0000, 120.3123, 110.5279]])\n",
      "xyxyn: tensor([[0.1958, 0.0000, 0.9781, 0.9288]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 3 -1s, 133.3ms\n",
      "Speed: 5.7ms preprocess, 133.3ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 121.5ms\n",
      "Speed: 1.8ms preprocess, 121.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3816])\n",
      "data: tensor([[  0.0000,   0.0000,  28.9150, 100.1291,   0.3816,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 14.4575,  50.0646,  28.9150, 100.1291]])\n",
      "xywhn: tensor([[0.1195, 0.4138, 0.2390, 0.8275]])\n",
      "xyxy: tensor([[  0.0000,   0.0000,  28.9150, 100.1291]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.2390, 0.8275]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2839])\n",
      "data: tensor([[ 21.9943,   0.0000, 117.8610, 118.4016,   0.2839,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.9276,  59.2008,  95.8666, 118.4016]])\n",
      "xywhn: tensor([[0.5779, 0.4893, 0.7923, 0.9785]])\n",
      "xyxy: tensor([[ 21.9943,   0.0000, 117.8610, 118.4016]])\n",
      "xyxyn: tensor([[0.1818, 0.0000, 0.9741, 0.9785]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2570])\n",
      "data: tensor([[ 10.7291,   0.0000,  82.2935, 107.5290,   0.2570,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (121, 121)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 46.5113,  53.7645,  71.5645, 107.5290]])\n",
      "xywhn: tensor([[0.3844, 0.4443, 0.5914, 0.8887]])\n",
      "xyxy: tensor([[ 10.7291,   0.0000,  82.2935, 107.5290]])\n",
      "xyxyn: tensor([[0.0887, 0.0000, 0.6801, 0.8887]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 3 -1s, 152.3ms\n",
      "Speed: 4.1ms preprocess, 152.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 110.2ms\n",
      "Speed: 13.2ms preprocess, 110.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3567])\n",
      "data: tensor([[  2.5720,   0.9698,  87.4322, 101.7610,   0.3567,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 45.0021,  51.3654,  84.8602, 100.7912]])\n",
      "xywhn: tensor([[0.3629, 0.4210, 0.6844, 0.8262]])\n",
      "xyxy: tensor([[  2.5720,   0.9698,  87.4322, 101.7610]])\n",
      "xyxyn: tensor([[0.0207, 0.0079, 0.7051, 0.8341]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3361])\n",
      "data: tensor([[ 25.6015,   0.0000, 121.8578, 114.9390,   0.3361,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 73.7296,  57.4695,  96.2563, 114.9390]])\n",
      "xywhn: tensor([[0.5946, 0.4711, 0.7763, 0.9421]])\n",
      "xyxy: tensor([[ 25.6015,   0.0000, 121.8578, 114.9390]])\n",
      "xyxyn: tensor([[0.2065, 0.0000, 0.9827, 0.9421]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2709])\n",
      "data: tensor([[  0.0000,   1.3314,  37.0780, 104.5860,   0.2709,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (122, 124)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 18.5390,  52.9587,  37.0780, 103.2546]])\n",
      "xywhn: tensor([[0.1495, 0.4341, 0.2990, 0.8463]])\n",
      "xyxy: tensor([[  0.0000,   1.3314,  37.0780, 104.5860]])\n",
      "xyxyn: tensor([[0.0000, 0.0109, 0.2990, 0.8573]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 151.5ms\n",
      "Speed: 5.7ms preprocess, 151.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 90.6ms\n",
      "Speed: 3.8ms preprocess, 90.6ms inference, 10.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2876])\n",
      "data: tensor([[ 18.1528,   1.3801, 116.6012, 121.9190,   0.2876,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (123, 122)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 67.3770,  61.6495,  98.4484, 120.5389]])\n",
      "xywhn: tensor([[0.5523, 0.5012, 0.8070, 0.9800]])\n",
      "xyxy: tensor([[ 18.1528,   1.3801, 116.6012, 121.9190]])\n",
      "xyxyn: tensor([[0.1488, 0.0112, 0.9557, 0.9912]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 152.8ms\n",
      "Speed: 4.0ms preprocess, 152.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 92.5ms\n",
      "Speed: 3.3ms preprocess, 92.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3342])\n",
      "data: tensor([[ 21.5832,   1.4401, 117.9852, 124.1946,   0.3342,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.7842,  62.8174,  96.4020, 122.7545]])\n",
      "xywhn: tensor([[0.5674, 0.5025, 0.7838, 0.9820]])\n",
      "xyxy: tensor([[ 21.5832,   1.4401, 117.9852, 124.1946]])\n",
      "xyxyn: tensor([[0.1755, 0.0115, 0.9592, 0.9936]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 2 -1s, 162.7ms\n",
      "Speed: 3.9ms preprocess, 162.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 95.3ms\n",
      "Speed: 0.0ms preprocess, 95.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3335])\n",
      "data: tensor([[ 29.0607,   0.0000, 123.0000, 123.7631,   0.3335,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.0304,  61.8815,  93.9393, 123.7631]])\n",
      "xywhn: tensor([[0.6181, 0.4951, 0.7637, 0.9901]])\n",
      "xyxy: tensor([[ 29.0607,   0.0000, 123.0000, 123.7631]])\n",
      "xyxyn: tensor([[0.2363, 0.0000, 1.0000, 0.9901]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2741])\n",
      "data: tensor([[  1.6819,   0.2741,  88.7533, 105.6650,   0.2741,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (125, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 45.2176,  52.9695,  87.0713, 105.3909]])\n",
      "xywhn: tensor([[0.3676, 0.4238, 0.7079, 0.8431]])\n",
      "xyxy: tensor([[  1.6819,   0.2741,  88.7533, 105.6650]])\n",
      "xyxyn: tensor([[0.0137, 0.0022, 0.7216, 0.8453]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 -1, 147.9ms\n",
      "Speed: 4.3ms preprocess, 147.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 95.6ms\n",
      "Speed: 3.3ms preprocess, 95.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3561])\n",
      "data: tensor([[ 23.9433,   5.2482, 121.7411, 122.4293,   0.3561,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (126, 123)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.8422,  63.8387,  97.7978, 117.1811]])\n",
      "xywhn: tensor([[0.5922, 0.5067, 0.7951, 0.9300]])\n",
      "xyxy: tensor([[ 23.9433,   5.2482, 121.7411, 122.4293]])\n",
      "xyxyn: tensor([[0.1947, 0.0417, 0.9898, 0.9717]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 1 -1, 149.4ms\n",
      "Speed: 7.5ms preprocess, 149.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 2 persons, 2 dining tables, 1 laptop, 1 mouse, 90.5ms\n",
      "Speed: 2.8ms preprocess, 90.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3739])\n",
      "data: tensor([[ 21.8326,   3.7896, 119.0000, 122.8950,   0.3739,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (127, 119)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 70.4163,  63.3423,  97.1674, 119.1054]])\n",
      "xywhn: tensor([[0.5917, 0.4988, 0.8165, 0.9378]])\n",
      "xyxy: tensor([[ 21.8326,   3.7896, 119.0000, 122.8950]])\n",
      "xyxyn: tensor([[0.1835, 0.0298, 1.0000, 0.9677]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 1 -1, 144.8ms\n",
      "Speed: 4.0ms preprocess, 144.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 2 dining tables, 1 laptop, 1 mouse, 1 book, 110.2ms\n",
      "Speed: 0.0ms preprocess, 110.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3842])\n",
      "data: tensor([[ 24.2736,   4.3570, 118.0000, 122.9598,   0.3842,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (127, 118)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 71.1368,  63.6584,  93.7264, 118.6027]])\n",
      "xywhn: tensor([[0.6029, 0.5012, 0.7943, 0.9339]])\n",
      "xyxy: tensor([[ 24.2736,   4.3570, 118.0000, 122.9598]])\n",
      "xyxyn: tensor([[0.2057, 0.0343, 1.0000, 0.9682]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 1 -1, 138.2ms\n",
      "Speed: 4.1ms preprocess, 138.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 2 dining tables, 1 laptop, 1 mouse, 1 book, 118.1ms\n",
      "Speed: 15.9ms preprocess, 118.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3308])\n",
      "data: tensor([[ 14.4803,   1.4413, 114.8477, 125.8159,   0.3308,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (127, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 64.6640,  63.6286, 100.3674, 124.3747]])\n",
      "xywhn: tensor([[0.5574, 0.5010, 0.8652, 0.9793]])\n",
      "xyxy: tensor([[ 14.4803,   1.4413, 114.8477, 125.8159]])\n",
      "xyxyn: tensor([[0.1248, 0.0113, 0.9901, 0.9907]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 2 -1s, 129.8ms\n",
      "Speed: 5.6ms preprocess, 129.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 2 dining tables, 1 laptop, 1 mouse, 1 book, 97.8ms\n",
      "Speed: 9.7ms preprocess, 97.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4018])\n",
      "data: tensor([[ 20.0437,   4.6209, 115.0000, 124.3418,   0.4018,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 115)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 67.5218,  64.4814,  94.9563, 119.7210]])\n",
      "xywhn: tensor([[0.5871, 0.5038, 0.8257, 0.9353]])\n",
      "xyxy: tensor([[ 20.0437,   4.6209, 115.0000, 124.3418]])\n",
      "xyxyn: tensor([[0.1743, 0.0361, 1.0000, 0.9714]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2513])\n",
      "data: tensor([[  6.4140,   2.3847,  87.8594, 110.4269,   0.2513,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 115)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 47.1367,  56.4058,  81.4454, 108.0422]])\n",
      "xywhn: tensor([[0.4099, 0.4407, 0.7082, 0.8441]])\n",
      "xyxy: tensor([[  6.4140,   2.3847,  87.8594, 110.4269]])\n",
      "xyxyn: tensor([[0.0558, 0.0186, 0.7640, 0.8627]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x608 2 -1s, 140.4ms\n",
      "Speed: 12.7ms preprocess, 140.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 384x640 1 person, 1 bird, 2 chairs, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 117.3ms\n",
      "Speed: 1.3ms preprocess, 117.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3358])\n",
      "data: tensor([[ 10.9930,   1.9446,  89.3855, 108.5086,   0.3358,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 50.1892,  55.2266,  78.3925, 106.5640]])\n",
      "xywhn: tensor([[0.4327, 0.4315, 0.6758, 0.8325]])\n",
      "xyxy: tensor([[ 10.9930,   1.9446,  89.3855, 108.5086]])\n",
      "xyxyn: tensor([[0.0948, 0.0152, 0.7706, 0.8477]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2873])\n",
      "data: tensor([[ 14.0058,   5.4451, 115.7228, 124.0313,   0.2873,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (128, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 64.8643,  64.7382, 101.7169, 118.5862]])\n",
      "xywhn: tensor([[0.5592, 0.5058, 0.8769, 0.9265]])\n",
      "xyxy: tensor([[ 14.0058,   5.4451, 115.7228, 124.0313]])\n",
      "xyxyn: tensor([[0.1207, 0.0425, 0.9976, 0.9690]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 2 -1s, 152.7ms\n",
      "Speed: 2.8ms preprocess, 152.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 2 persons, 1 bird, 2 chairs, 2 dining tables, 1 laptop, 1 mouse, 1 cell phone, 1 book, 92.8ms\n",
      "Speed: 5.2ms preprocess, 92.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3782])\n",
      "data: tensor([[ 28.8572,   0.0000, 116.0000, 127.6012,   0.3782,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (130, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.4286,  63.8006,  87.1428, 127.6012]])\n",
      "xywhn: tensor([[0.6244, 0.4908, 0.7512, 0.9815]])\n",
      "xyxy: tensor([[ 28.8572,   0.0000, 116.0000, 127.6012]])\n",
      "xyxyn: tensor([[0.2488, 0.0000, 1.0000, 0.9815]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2568])\n",
      "data: tensor([[  4.5984,   0.4684,  91.6641, 107.3466,   0.2568,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (130, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 48.1313,  53.9075,  87.0657, 106.8782]])\n",
      "xywhn: tensor([[0.4149, 0.4147, 0.7506, 0.8221]])\n",
      "xyxy: tensor([[  4.5984,   0.4684,  91.6641, 107.3466]])\n",
      "xyxyn: tensor([[0.0396, 0.0036, 0.7902, 0.8257]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 2 -1s, 136.2ms\n",
      "Speed: 6.2ms preprocess, 136.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 90.8ms\n",
      "Speed: 3.0ms preprocess, 90.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3045])\n",
      "data: tensor([[ 28.6445,   0.0000, 116.0000, 127.6048,   0.3045,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (131, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 72.3222,  63.8024,  87.3555, 127.6048]])\n",
      "xywhn: tensor([[0.6235, 0.4870, 0.7531, 0.9741]])\n",
      "xyxy: tensor([[ 28.6445,   0.0000, 116.0000, 127.6048]])\n",
      "xyxyn: tensor([[0.2469, 0.0000, 1.0000, 0.9741]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2691])\n",
      "data: tensor([[  2.8919,   0.6555,  91.5875, 106.3538,   0.2691,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (131, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 47.2397,  53.5046,  88.6956, 105.6983]])\n",
      "xywhn: tensor([[0.4072, 0.4084, 0.7646, 0.8069]])\n",
      "xyxy: tensor([[  2.8919,   0.6555,  91.5875, 106.3538]])\n",
      "xyxyn: tensor([[0.0249, 0.0050, 0.7895, 0.8119]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 1 -1, 141.5ms\n",
      "Speed: 2.7ms preprocess, 141.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 3 persons, 1 bird, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 94.3ms\n",
      "Speed: 3.4ms preprocess, 94.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3209])\n",
      "data: tensor([[ 16.3433,   0.0000, 116.0000, 127.2791,   0.3209,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (131, 116)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 66.1716,  63.6396,  99.6567, 127.2791]])\n",
      "xywhn: tensor([[0.5704, 0.4858, 0.8591, 0.9716]])\n",
      "xyxy: tensor([[ 16.3433,   0.0000, 116.0000, 127.2791]])\n",
      "xyxyn: tensor([[0.1409, 0.0000, 1.0000, 0.9716]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 1 -1, 145.7ms\n",
      "Speed: 6.0ms preprocess, 145.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 95.1ms\n",
      "Speed: 0.0ms preprocess, 95.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2666])\n",
      "data: tensor([[  9.5072,   3.2735, 103.0908, 119.1691,   0.2666,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (131, 114)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 56.2990,  61.2213,  93.5836, 115.8957]])\n",
      "xywhn: tensor([[0.4939, 0.4673, 0.8209, 0.8847]])\n",
      "xyxy: tensor([[  9.5072,   3.2735, 103.0908, 119.1691]])\n",
      "xyxyn: tensor([[0.0834, 0.0250, 0.9043, 0.9097]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x576 1 -1, 140.5ms\n",
      "Speed: 3.7ms preprocess, 140.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 2 dining tables, 1 laptop, 1 mouse, 1 book, 96.5ms\n",
      "Speed: 2.8ms preprocess, 96.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3040])\n",
      "data: tensor([[  9.1448,   3.0633, 104.9590, 121.8577,   0.3040,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (133, 115)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 57.0519,  62.4605,  95.8142, 118.7944]])\n",
      "xywhn: tensor([[0.4961, 0.4696, 0.8332, 0.8932]])\n",
      "xyxy: tensor([[  9.1448,   3.0633, 104.9590, 121.8577]])\n",
      "xyxyn: tensor([[0.0795, 0.0230, 0.9127, 0.9162]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x544 1 -1, 141.6ms\n",
      "Speed: 3.9ms preprocess, 141.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 2 dining tables, 1 laptop, 1 mouse, 1 book, 93.9ms\n",
      "Speed: 0.0ms preprocess, 93.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3335])\n",
      "data: tensor([[ 20.8375,   0.0000, 114.0000, 126.8324,   0.3335,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (134, 114)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 67.4188,  63.4162,  93.1625, 126.8324]])\n",
      "xywhn: tensor([[0.5914, 0.4733, 0.8172, 0.9465]])\n",
      "xyxy: tensor([[ 20.8375,   0.0000, 114.0000, 126.8324]])\n",
      "xyxyn: tensor([[0.1828, 0.0000, 1.0000, 0.9465]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 608x640 (no detections), 138.6ms\n",
      "Speed: 3.4ms preprocess, 138.6ms inference, 8.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x576 1 -1, 150.3ms\n",
      "Speed: 7.9ms preprocess, 150.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 2 dining tables, 2 laptops, 1 mouse, 1 book, 97.5ms\n",
      "Speed: 3.5ms preprocess, 97.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3486])\n",
      "data: tensor([[ 11.8746,   1.2314, 114.4046, 124.9931,   0.3486,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (134, 115)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 63.1396,  63.1123, 102.5301, 123.7617]])\n",
      "xywhn: tensor([[0.5490, 0.4710, 0.8916, 0.9236]])\n",
      "xyxy: tensor([[ 11.8746,   1.2314, 114.4046, 124.9931]])\n",
      "xyxyn: tensor([[0.1033, 0.0092, 0.9948, 0.9328]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 608x640 (no detections), 135.8ms\n",
      "Speed: 6.3ms preprocess, 135.8ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x544 1 -1, 125.7ms\n",
      "Speed: 16.5ms preprocess, 125.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 384x640 4 persons, 2 chairs, 2 dining tables, 1 laptop, 1 mouse, 1 book, 110.9ms\n",
      "Speed: 0.0ms preprocess, 110.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3273])\n",
      "data: tensor([[1.1459e+01, 5.9936e-02, 1.1400e+02, 1.2589e+02, 3.2726e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (134, 114)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 62.7295,  62.9744, 102.5411, 125.8290]])\n",
      "xywhn: tensor([[0.5503, 0.4700, 0.8995, 0.9390]])\n",
      "xyxy: tensor([[1.1459e+01, 5.9936e-02, 1.1400e+02, 1.2589e+02]])\n",
      "xyxyn: tensor([[1.0052e-01, 4.4728e-04, 1.0000e+00, 9.3947e-01]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 576x640 1 -1, 142.0ms\n",
      "Speed: 3.2ms preprocess, 142.0ms inference, 0.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x544 1 -1, 141.6ms\n",
      "Speed: 14.8ms preprocess, 141.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2780])\n",
      "data: tensor([[ 21.8550,   0.0000, 128.7767, 116.6488,   0.2780,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (119, 134)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 75.3158,  58.3244, 106.9217, 116.6488]])\n",
      "xywhn: tensor([[0.5621, 0.4901, 0.7979, 0.9802]])\n",
      "xyxy: tensor([[ 21.8550,   0.0000, 128.7767, 116.6488]])\n",
      "xyxyn: tensor([[0.1631, 0.0000, 0.9610, 0.9802]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3215])\n",
      "data: tensor([[1.2287e+01, 3.8909e-02, 1.1400e+02, 1.2539e+02, 3.2149e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (134, 114)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 63.1434,  62.7136, 101.7132, 125.3494]])\n",
      "xywhn: tensor([[0.5539, 0.4680, 0.8922, 0.9354]])\n",
      "xyxy: tensor([[1.2287e+01, 3.8909e-02, 1.1400e+02, 1.2539e+02]])\n",
      "xyxyn: tensor([[1.0778e-01, 2.9036e-04, 1.0000e+00, 9.3573e-01]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 3 chairs, 2 dining tables, 1 laptop, 1 mouse, 1 book, 98.9ms\n",
      "Speed: 4.4ms preprocess, 98.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 576x640 2 -1s, 137.9ms\n",
      "Speed: 4.5ms preprocess, 137.9ms inference, 0.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x544 1 -1, 141.6ms\n",
      "Speed: 9.1ms preprocess, 141.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3005])\n",
      "data: tensor([[  1.5820,   0.0000,  91.2385, 115.1826,   0.3005,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (117, 135)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 46.4102,  57.5913,  89.6565, 115.1826]])\n",
      "xywhn: tensor([[0.3438, 0.4922, 0.6641, 0.9845]])\n",
      "xyxy: tensor([[  1.5820,   0.0000,  91.2385, 115.1826]])\n",
      "xyxyn: tensor([[0.0117, 0.0000, 0.6758, 0.9845]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2531])\n",
      "data: tensor([[ 20.9079,   0.0000, 128.7268, 117.0000,   0.2531,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (117, 135)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 74.8173,  58.5000, 107.8190, 117.0000]])\n",
      "xywhn: tensor([[0.5542, 0.5000, 0.7987, 1.0000]])\n",
      "xyxy: tensor([[ 20.9079,   0.0000, 128.7268, 117.0000]])\n",
      "xyxyn: tensor([[0.1549, 0.0000, 0.9535, 1.0000]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3493])\n",
      "data: tensor([[1.1324e+01, 7.7314e-02, 1.1300e+02, 1.2581e+02, 3.4931e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (134, 113)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 62.1620,  62.9414, 101.6761, 125.7282]])\n",
      "xywhn: tensor([[0.5501, 0.4697, 0.8998, 0.9383]])\n",
      "xyxy: tensor([[1.1324e+01, 7.7314e-02, 1.1300e+02, 1.2581e+02]])\n",
      "xyxyn: tensor([[1.0021e-01, 5.7697e-04, 1.0000e+00, 9.3885e-01]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 remote, 1 book, 99.1ms\n",
      "Speed: 4.2ms preprocess, 99.1ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 544x640 1 -1, 155.1ms\n",
      "Speed: 10.1ms preprocess, 155.1ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x544 1 -1, 132.5ms\n",
      "Speed: 2.5ms preprocess, 132.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3388])\n",
      "data: tensor([[  2.9754,   0.0000, 101.7083, 111.2382,   0.3388,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (115, 136)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 52.3419,  55.6191,  98.7329, 111.2382]])\n",
      "xywhn: tensor([[0.3849, 0.4836, 0.7260, 0.9673]])\n",
      "xyxy: tensor([[  2.9754,   0.0000, 101.7083, 111.2382]])\n",
      "xyxyn: tensor([[0.0219, 0.0000, 0.7479, 0.9673]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3261])\n",
      "data: tensor([[ 11.9930,   4.5486, 109.1867, 121.1566,   0.3261,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (135, 113)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 60.5899,  62.8526,  97.1936, 116.6080]])\n",
      "xywhn: tensor([[0.5362, 0.4656, 0.8601, 0.8638]])\n",
      "xyxy: tensor([[ 11.9930,   4.5486, 109.1867, 121.1566]])\n",
      "xyxyn: tensor([[0.1061, 0.0337, 0.9663, 0.8975]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 92.8ms\n",
      "Speed: 0.0ms preprocess, 92.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 544x640 2 -1s, 146.2ms\n",
      "Speed: 8.7ms preprocess, 146.2ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x544 1 -1, 142.5ms\n",
      "Speed: 2.2ms preprocess, 142.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3832])\n",
      "data: tensor([[  0.4133,   0.0000, 101.1995, 109.8490,   0.3832,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (114, 137)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 50.8064,  54.9245, 100.7862, 109.8490]])\n",
      "xywhn: tensor([[0.3708, 0.4818, 0.7357, 0.9636]])\n",
      "xyxy: tensor([[  0.4133,   0.0000, 101.1995, 109.8490]])\n",
      "xyxyn: tensor([[0.0030, 0.0000, 0.7387, 0.9636]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3751])\n",
      "data: tensor([[ 23.8330,   0.0000, 129.9029, 111.4492,   0.3751,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (114, 137)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 76.8680,  55.7246, 106.0698, 111.4492]])\n",
      "xywhn: tensor([[0.5611, 0.4888, 0.7742, 0.9776]])\n",
      "xyxy: tensor([[ 23.8330,   0.0000, 129.9029, 111.4492]])\n",
      "xyxyn: tensor([[0.1740, 0.0000, 0.9482, 0.9776]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3009])\n",
      "data: tensor([[  5.7641,   2.6860, 111.6581, 126.6482,   0.3009,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (135, 112)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 58.7111,  64.6671, 105.8940, 123.9622]])\n",
      "xywhn: tensor([[0.5242, 0.4790, 0.9455, 0.9182]])\n",
      "xyxy: tensor([[  5.7641,   2.6860, 111.6581, 126.6482]])\n",
      "xyxyn: tensor([[0.0515, 0.0199, 0.9969, 0.9381]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 97.1ms\n",
      "Speed: 3.8ms preprocess, 97.1ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 544x640 2 -1s, 126.8ms\n",
      "Speed: 4.1ms preprocess, 126.8ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x544 1 -1, 124.5ms\n",
      "Speed: 11.0ms preprocess, 124.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3574])\n",
      "data: tensor([[ 27.2280,   0.0000, 133.8680, 108.4192,   0.3574,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (112, 138)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 80.5480,  54.2096, 106.6400, 108.4192]])\n",
      "xywhn: tensor([[0.5837, 0.4840, 0.7728, 0.9680]])\n",
      "xyxy: tensor([[ 27.2280,   0.0000, 133.8680, 108.4192]])\n",
      "xyxyn: tensor([[0.1973, 0.0000, 0.9701, 0.9680]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3430])\n",
      "data: tensor([[  3.1542,   0.0000, 108.1218, 108.9823,   0.3430,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (112, 138)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 55.6380,  54.4912, 104.9676, 108.9823]])\n",
      "xywhn: tensor([[0.4032, 0.4865, 0.7606, 0.9731]])\n",
      "xyxy: tensor([[  3.1542,   0.0000, 108.1218, 108.9823]])\n",
      "xyxyn: tensor([[0.0229, 0.0000, 0.7835, 0.9731]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3426])\n",
      "data: tensor([[ 10.0902,   0.2962, 112.0000, 125.6114,   0.3426,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (135, 112)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 61.0451,  62.9538, 101.9098, 125.3153]])\n",
      "xywhn: tensor([[0.5450, 0.4663, 0.9099, 0.9283]])\n",
      "xyxy: tensor([[ 10.0902,   0.2962, 112.0000, 125.6114]])\n",
      "xyxyn: tensor([[0.0901, 0.0022, 1.0000, 0.9305]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 94.8ms\n",
      "Speed: 3.1ms preprocess, 94.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 (no detections), 123.4ms\n",
      "Speed: 3.0ms preprocess, 123.4ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 -1, 147.3ms\n",
      "Speed: 14.7ms preprocess, 147.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 92.0ms\n",
      "Speed: 2.5ms preprocess, 92.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3319])\n",
      "data: tensor([[  9.4950,   1.2852, 110.4464, 118.4476,   0.3319,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (133, 111)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 59.9707,  59.8664, 100.9514, 117.1625]])\n",
      "xywhn: tensor([[0.5403, 0.4501, 0.9095, 0.8809]])\n",
      "xyxy: tensor([[  9.4950,   1.2852, 110.4464, 118.4476]])\n",
      "xyxyn: tensor([[0.0855, 0.0097, 0.9950, 0.8906]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 512x640 1 -1, 138.8ms\n",
      "Speed: 2.6ms preprocess, 138.8ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 -1, 139.1ms\n",
      "Speed: 0.0ms preprocess, 139.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2769])\n",
      "data: tensor([[  6.7385,   0.0000, 112.3856, 105.5778,   0.2769,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (107, 139)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 59.5620,  52.7889, 105.6471, 105.5778]])\n",
      "xywhn: tensor([[0.4285, 0.4934, 0.7601, 0.9867]])\n",
      "xyxy: tensor([[  6.7385,   0.0000, 112.3856, 105.5778]])\n",
      "xyxyn: tensor([[0.0485, 0.0000, 0.8085, 0.9867]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3481])\n",
      "data: tensor([[ 11.5158,   1.8766, 109.9477, 123.0686,   0.3481,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (133, 111)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 60.7317,  62.4726,  98.4320, 121.1919]])\n",
      "xywhn: tensor([[0.5471, 0.4697, 0.8868, 0.9112]])\n",
      "xyxy: tensor([[ 11.5158,   1.8766, 109.9477, 123.0686]])\n",
      "xyxyn: tensor([[0.1037, 0.0141, 0.9905, 0.9253]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 93.3ms\n",
      "Speed: 0.0ms preprocess, 93.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 -1, 136.2ms\n",
      "Speed: 4.2ms preprocess, 136.2ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 -1, 125.4ms\n",
      "Speed: 7.1ms preprocess, 125.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3106])\n",
      "data: tensor([[ 11.4463,   0.0000, 110.4843, 104.7984,   0.3106,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (107, 141)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 60.9653,  52.3992,  99.0381, 104.7984]])\n",
      "xywhn: tensor([[0.4324, 0.4897, 0.7024, 0.9794]])\n",
      "xyxy: tensor([[ 11.4463,   0.0000, 110.4843, 104.7984]])\n",
      "xyxyn: tensor([[0.0812, 0.0000, 0.7836, 0.9794]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3509])\n",
      "data: tensor([[ 12.1195,   1.8800, 109.9369, 122.8607,   0.3509,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (133, 111)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 61.0282,  62.3704,  97.8175, 120.9807]])\n",
      "xywhn: tensor([[0.5498, 0.4690, 0.8812, 0.9096]])\n",
      "xyxy: tensor([[ 12.1195,   1.8800, 109.9369, 122.8607]])\n",
      "xyxyn: tensor([[0.1092, 0.0141, 0.9904, 0.9238]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 100.1ms\n",
      "Speed: 1.9ms preprocess, 100.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 -1, 123.9ms\n",
      "Speed: 2.7ms preprocess, 123.9ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 -1, 141.3ms\n",
      "Speed: 0.9ms preprocess, 141.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3905])\n",
      "data: tensor([[  3.0241,   0.0000, 109.7371, 105.1007,   0.3905,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (106, 140)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 56.3806,  52.5504, 106.7131, 105.1007]])\n",
      "xywhn: tensor([[0.4027, 0.4958, 0.7622, 0.9915]])\n",
      "xyxy: tensor([[  3.0241,   0.0000, 109.7371, 105.1007]])\n",
      "xyxyn: tensor([[0.0216, 0.0000, 0.7838, 0.9915]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2727])\n",
      "data: tensor([[ 16.5289,   0.3767, 110.0000, 129.1480,   0.2727,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (132, 110)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 63.2644,  64.7623,  93.4711, 128.7713]])\n",
      "xywhn: tensor([[0.5751, 0.4906, 0.8497, 0.9755]])\n",
      "xyxy: tensor([[ 16.5289,   0.3767, 110.0000, 129.1480]])\n",
      "xyxyn: tensor([[0.1503, 0.0029, 1.0000, 0.9784]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 111.0ms\n",
      "Speed: 0.0ms preprocess, 111.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 -1, 121.1ms\n",
      "Speed: 5.5ms preprocess, 121.1ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 -1, 133.9ms\n",
      "Speed: 13.5ms preprocess, 133.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3257])\n",
      "data: tensor([[ 11.6943,   0.0000, 116.9275, 103.5155,   0.3257,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (106, 140)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 64.3109,  51.7578, 105.2332, 103.5155]])\n",
      "xywhn: tensor([[0.4594, 0.4883, 0.7517, 0.9766]])\n",
      "xyxy: tensor([[ 11.6943,   0.0000, 116.9275, 103.5155]])\n",
      "xyxyn: tensor([[0.0835, 0.0000, 0.8352, 0.9766]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2948])\n",
      "data: tensor([[ 10.0273,   0.7896, 110.0000, 126.8268,   0.2948,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (131, 110)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 60.0137,  63.8082,  99.9727, 126.0372]])\n",
      "xywhn: tensor([[0.5456, 0.4871, 0.9088, 0.9621]])\n",
      "xyxy: tensor([[ 10.0273,   0.7896, 110.0000, 126.8268]])\n",
      "xyxyn: tensor([[0.0912, 0.0060, 1.0000, 0.9681]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 92.3ms\n",
      "Speed: 3.0ms preprocess, 92.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 -1, 132.0ms\n",
      "Speed: 11.6ms preprocess, 132.0ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 -1, 131.2ms\n",
      "Speed: 2.1ms preprocess, 131.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3267])\n",
      "data: tensor([[  9.6548,   0.0000, 117.7013, 102.8554,   0.3267,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (107, 141)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 63.6781,  51.4277, 108.0465, 102.8554]])\n",
      "xywhn: tensor([[0.4516, 0.4806, 0.7663, 0.9613]])\n",
      "xyxy: tensor([[  9.6548,   0.0000, 117.7013, 102.8554]])\n",
      "xyxyn: tensor([[0.0685, 0.0000, 0.8348, 0.9613]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2844])\n",
      "data: tensor([[  9.8675,   0.7361, 109.0000, 126.2128,   0.2844,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (130, 109)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 59.4338,  63.4745,  99.1325, 125.4767]])\n",
      "xywhn: tensor([[0.5453, 0.4883, 0.9095, 0.9652]])\n",
      "xyxy: tensor([[  9.8675,   0.7361, 109.0000, 126.2128]])\n",
      "xyxyn: tensor([[0.0905, 0.0057, 1.0000, 0.9709]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 122.5ms\n",
      "Speed: 0.0ms preprocess, 122.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 -1, 125.0ms\n",
      "Speed: 2.3ms preprocess, 125.0ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 -1, 139.4ms\n",
      "Speed: 3.0ms preprocess, 139.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2603])\n",
      "data: tensor([[ 12.1524,   0.0000, 115.0695, 103.1367,   0.2603,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (106, 139)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 63.6109,  51.5684, 102.9171, 103.1367]])\n",
      "xywhn: tensor([[0.4576, 0.4865, 0.7404, 0.9730]])\n",
      "xyxy: tensor([[ 12.1524,   0.0000, 115.0695, 103.1367]])\n",
      "xyxyn: tensor([[0.0874, 0.0000, 0.8278, 0.9730]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2773])\n",
      "data: tensor([[ 17.4758,   0.0000, 106.5878, 105.2600,   0.2773,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (130, 109)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 62.0318,  52.6300,  89.1120, 105.2600]])\n",
      "xywhn: tensor([[0.5691, 0.4048, 0.8175, 0.8097]])\n",
      "xyxy: tensor([[ 17.4758,   0.0000, 106.5878, 105.2600]])\n",
      "xyxyn: tensor([[0.1603, 0.0000, 0.9779, 0.8097]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 118.0ms\n",
      "Speed: 3.5ms preprocess, 118.0ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 1, 135.4ms\n",
      "Speed: 10.3ms preprocess, 135.4ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 -1, 145.3ms\n",
      "Speed: 6.6ms preprocess, 145.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.2722])\n",
      "data: tensor([[31.3590,  0.6640, 68.9349, 25.0345,  0.2722,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (105, 138)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[50.1470, 12.8492, 37.5760, 24.3705]])\n",
      "xywhn: tensor([[0.3634, 0.1224, 0.2723, 0.2321]])\n",
      "xyxy: tensor([[31.3590,  0.6640, 68.9349, 25.0345]])\n",
      "xyxyn: tensor([[0.2272, 0.0063, 0.4995, 0.2384]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2841])\n",
      "data: tensor([[ 16.6189,   0.0000, 106.4419, 105.4006,   0.2841,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (130, 109)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 61.5304,  52.7003,  89.8229, 105.4006]])\n",
      "xywhn: tensor([[0.5645, 0.4054, 0.8241, 0.8108]])\n",
      "xyxy: tensor([[ 16.6189,   0.0000, 106.4419, 105.4006]])\n",
      "xyxyn: tensor([[0.1525, 0.0000, 0.9765, 0.8108]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 112.0ms\n",
      "Speed: 3.0ms preprocess, 112.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 (no detections), 131.1ms\n",
      "Speed: 3.9ms preprocess, 131.1ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 -1, 130.4ms\n",
      "Speed: 4.8ms preprocess, 130.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 105.2ms\n",
      "Speed: 5.9ms preprocess, 105.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2992])\n",
      "data: tensor([[ 16.3840,   0.4313, 108.0000, 127.5805,   0.2992,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (130, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 62.1920,  64.0059,  91.6160, 127.1492]])\n",
      "xywhn: tensor([[0.5759, 0.4924, 0.8483, 0.9781]])\n",
      "xyxy: tensor([[ 16.3840,   0.4313, 108.0000, 127.5805]])\n",
      "xyxyn: tensor([[0.1517, 0.0033, 1.0000, 0.9814]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 512x640 1 -1, 124.6ms\n",
      "Speed: 2.4ms preprocess, 124.6ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 -1, 142.9ms\n",
      "Speed: 5.3ms preprocess, 142.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3342])\n",
      "data: tensor([[  3.1093,   0.0000,  83.7601, 100.1733,   0.3342,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (104, 136)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 43.4347,  50.0866,  80.6508, 100.1733]])\n",
      "xywhn: tensor([[0.3194, 0.4816, 0.5930, 0.9632]])\n",
      "xyxy: tensor([[  3.1093,   0.0000,  83.7601, 100.1733]])\n",
      "xyxyn: tensor([[0.0229, 0.0000, 0.6159, 0.9632]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2844])\n",
      "data: tensor([[ 14.4799,   0.0000, 105.9905, 105.3114,   0.2844,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (129, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 60.2352,  52.6557,  91.5106, 105.3114]])\n",
      "xywhn: tensor([[0.5577, 0.4082, 0.8473, 0.8164]])\n",
      "xyxy: tensor([[ 14.4799,   0.0000, 105.9905, 105.3114]])\n",
      "xyxyn: tensor([[0.1341, 0.0000, 0.9814, 0.8164]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 102.8ms\n",
      "Speed: 0.0ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 -1, 1 1, 142.4ms\n",
      "Speed: 6.5ms preprocess, 142.4ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 1 -1, 138.0ms\n",
      "Speed: 4.0ms preprocess, 138.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.3795])\n",
      "data: tensor([[30.1644,  0.0000, 66.2345, 24.7772,  0.3795,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (104, 134)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[48.1994, 12.3886, 36.0701, 24.7772]])\n",
      "xywhn: tensor([[0.3597, 0.1191, 0.2692, 0.2382]])\n",
      "xyxy: tensor([[30.1644,  0.0000, 66.2345, 24.7772]])\n",
      "xyxyn: tensor([[0.2251, 0.0000, 0.4943, 0.2382]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3531])\n",
      "data: tensor([[ 3.6380,  0.0000, 88.1426, 99.5940,  0.3531,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (104, 134)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[45.8903, 49.7970, 84.5046, 99.5940]])\n",
      "xywhn: tensor([[0.3425, 0.4788, 0.6306, 0.9576]])\n",
      "xyxy: tensor([[ 3.6380,  0.0000, 88.1426, 99.5940]])\n",
      "xyxyn: tensor([[0.0271, 0.0000, 0.6578, 0.9576]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2957])\n",
      "data: tensor([[ 14.6188,   0.0000, 106.1751, 104.7371,   0.2957,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (129, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 60.3969,  52.3686,  91.5563, 104.7371]])\n",
      "xywhn: tensor([[0.5592, 0.4060, 0.8477, 0.8119]])\n",
      "xyxy: tensor([[ 14.6188,   0.0000, 106.1751, 104.7371]])\n",
      "xyxyn: tensor([[0.1354, 0.0000, 0.9831, 0.8119]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 107.4ms\n",
      "Speed: 0.0ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 -1, 1 1, 124.2ms\n",
      "Speed: 12.6ms preprocess, 124.2ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 2 -1s, 130.8ms\n",
      "Speed: 3.9ms preprocess, 130.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.4223])\n",
      "data: tensor([[31.2670,  0.0000, 66.3956, 24.7361,  0.4223,  1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (104, 134)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[48.8313, 12.3681, 35.1286, 24.7361]])\n",
      "xywhn: tensor([[0.3644, 0.1189, 0.2622, 0.2378]])\n",
      "xyxy: tensor([[31.2670,  0.0000, 66.3956, 24.7361]])\n",
      "xyxyn: tensor([[0.2333, 0.0000, 0.4955, 0.2378]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3012])\n",
      "data: tensor([[  4.1909,   0.0000, 108.0910, 100.5009,   0.3012,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (104, 134)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 56.1409,  50.2505, 103.9001, 100.5009]])\n",
      "xywhn: tensor([[0.4190, 0.4832, 0.7754, 0.9664]])\n",
      "xyxy: tensor([[  4.1909,   0.0000, 108.0910, 100.5009]])\n",
      "xyxyn: tensor([[0.0313, 0.0000, 0.8066, 0.9664]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2756])\n",
      "data: tensor([[  9.7611,   0.9534, 108.0000, 125.6719,   0.2756,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (129, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 58.8805,  63.3127,  98.2389, 124.7185]])\n",
      "xywhn: tensor([[0.5452, 0.4908, 0.9096, 0.9668]])\n",
      "xyxy: tensor([[  9.7611,   0.9534, 108.0000, 125.6719]])\n",
      "xyxyn: tensor([[0.0904, 0.0074, 1.0000, 0.9742]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2630])\n",
      "data: tensor([[ 13.8264,   0.3548,  98.3781, 104.9933,   0.2630,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (129, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 56.1022,  52.6740,  84.5517, 104.6386]])\n",
      "xywhn: tensor([[0.5195, 0.4083, 0.7829, 0.8112]])\n",
      "xyxy: tensor([[ 13.8264,   0.3548,  98.3781, 104.9933]])\n",
      "xyxyn: tensor([[0.1280, 0.0028, 0.9109, 0.8139]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 94.7ms\n",
      "Speed: 4.4ms preprocess, 94.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 1 -1, 139.2ms\n",
      "Speed: 6.8ms preprocess, 139.2ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 2 -1s, 141.5ms\n",
      "Speed: 0.7ms preprocess, 141.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4206])\n",
      "data: tensor([[  0.8839,   0.0000,  90.0837, 101.6383,   0.4206,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (104, 132)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 45.4838,  50.8192,  89.1998, 101.6383]])\n",
      "xywhn: tensor([[0.3446, 0.4886, 0.6758, 0.9773]])\n",
      "xyxy: tensor([[  0.8839,   0.0000,  90.0837, 101.6383]])\n",
      "xyxyn: tensor([[0.0067, 0.0000, 0.6825, 0.9773]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3055])\n",
      "data: tensor([[ 15.1906,   0.2965,  96.8994, 105.3636,   0.3055,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (130, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 56.0450,  52.8300,  81.7088, 105.0671]])\n",
      "xywhn: tensor([[0.5189, 0.4064, 0.7566, 0.8082]])\n",
      "xyxy: tensor([[ 15.1906,   0.2965,  96.8994, 105.3636]])\n",
      "xyxyn: tensor([[0.1407, 0.0023, 0.8972, 0.8105]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2805])\n",
      "data: tensor([[ 10.4862,   0.9221, 108.0000, 126.0959,   0.2805,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (130, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 59.2431,  63.5090,  97.5138, 125.1738]])\n",
      "xywhn: tensor([[0.5485, 0.4885, 0.9029, 0.9629]])\n",
      "xyxy: tensor([[ 10.4862,   0.9221, 108.0000, 126.0959]])\n",
      "xyxyn: tensor([[0.0971, 0.0071, 1.0000, 0.9700]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 97.1ms\n",
      "Speed: 0.0ms preprocess, 97.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 512x640 2 -1s, 123.8ms\n",
      "Speed: 3.0ms preprocess, 123.8ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x544 2 -1s, 139.5ms\n",
      "Speed: 2.7ms preprocess, 139.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3894])\n",
      "data: tensor([[  0.2654,   0.0000,  88.2889, 103.1273,   0.3894,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (104, 130)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 44.2771,  51.5637,  88.0235, 103.1273]])\n",
      "xywhn: tensor([[0.3406, 0.4958, 0.6771, 0.9916]])\n",
      "xyxy: tensor([[  0.2654,   0.0000,  88.2889, 103.1273]])\n",
      "xyxyn: tensor([[0.0020, 0.0000, 0.6791, 0.9916]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2530])\n",
      "data: tensor([[ 30.0825,   0.0000, 128.2018,  62.5378,   0.2530,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (104, 130)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[79.1421, 31.2689, 98.1194, 62.5378]])\n",
      "xywhn: tensor([[0.6088, 0.3007, 0.7548, 0.6013]])\n",
      "xyxy: tensor([[ 30.0825,   0.0000, 128.2018,  62.5378]])\n",
      "xyxyn: tensor([[0.2314, 0.0000, 0.9862, 0.6013]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3145])\n",
      "data: tensor([[ 10.5197,   0.9548, 108.0000, 125.7661,   0.3145,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (130, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 59.2599,  63.3604,  97.4803, 124.8113]])\n",
      "xywhn: tensor([[0.5487, 0.4874, 0.9026, 0.9601]])\n",
      "xyxy: tensor([[ 10.5197,   0.9548, 108.0000, 125.7661]])\n",
      "xyxyn: tensor([[0.0974, 0.0073, 1.0000, 0.9674]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2714])\n",
      "data: tensor([[ 14.4134,   0.3614,  98.0241, 105.6390,   0.2714,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (130, 108)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 56.2187,  53.0002,  83.6107, 105.2776]])\n",
      "xywhn: tensor([[0.5205, 0.4077, 0.7742, 0.8098]])\n",
      "xyxy: tensor([[ 14.4134,   0.3614,  98.0241, 105.6390]])\n",
      "xyxyn: tensor([[0.1335, 0.0028, 0.9076, 0.8126]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 99.0ms\n",
      "Speed: 2.0ms preprocess, 99.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 544x640 3 -1s, 131.5ms\n",
      "Speed: 10.5ms preprocess, 131.5ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x544 1 -1, 162.5ms\n",
      "Speed: 4.3ms preprocess, 162.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3995])\n",
      "data: tensor([[  0.0000,   0.0000,  82.5278, 100.2398,   0.3995,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (104, 129)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 41.2639,  50.1199,  82.5278, 100.2398]])\n",
      "xywhn: tensor([[0.3199, 0.4819, 0.6398, 0.9638]])\n",
      "xyxy: tensor([[  0.0000,   0.0000,  82.5278, 100.2398]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.6398, 0.9638]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3583])\n",
      "data: tensor([[ 39.8811,   0.0000, 127.9781, 102.4410,   0.3583,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (104, 129)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 83.9296,  51.2205,  88.0970, 102.4410]])\n",
      "xywhn: tensor([[0.6506, 0.4925, 0.6829, 0.9850]])\n",
      "xyxy: tensor([[ 39.8811,   0.0000, 127.9781, 102.4410]])\n",
      "xyxyn: tensor([[0.3092, 0.0000, 0.9921, 0.9850]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2709])\n",
      "data: tensor([[ 19.5290,   0.0000, 118.6062, 101.1839,   0.2709,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (104, 129)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 69.0676,  50.5919,  99.0772, 101.1839]])\n",
      "xywhn: tensor([[0.5354, 0.4865, 0.7680, 0.9729]])\n",
      "xyxy: tensor([[ 19.5290,   0.0000, 118.6062, 101.1839]])\n",
      "xyxyn: tensor([[0.1514, 0.0000, 0.9194, 0.9729]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2852])\n",
      "data: tensor([[ 10.8996,   5.4026, 109.0000, 124.2583,   0.2852,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (130, 109)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 59.9498,  64.8304,  98.1004, 118.8557]])\n",
      "xywhn: tensor([[0.5500, 0.4987, 0.9000, 0.9143]])\n",
      "xyxy: tensor([[ 10.8996,   5.4026, 109.0000, 124.2583]])\n",
      "xyxyn: tensor([[0.1000, 0.0416, 1.0000, 0.9558]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 110.2ms\n",
      "Speed: 0.0ms preprocess, 110.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 544x640 2 -1s, 142.9ms\n",
      "Speed: 4.0ms preprocess, 142.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x544 1 -1, 141.6ms\n",
      "Speed: 0.5ms preprocess, 141.6ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3949])\n",
      "data: tensor([[ 0.0000,  0.3425, 82.1273, 99.8229,  0.3949,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (103, 128)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[41.0637, 50.0827, 82.1273, 99.4804]])\n",
      "xywhn: tensor([[0.3208, 0.4862, 0.6416, 0.9658]])\n",
      "xyxy: tensor([[ 0.0000,  0.3425, 82.1273, 99.8229]])\n",
      "xyxyn: tensor([[0.0000, 0.0033, 0.6416, 0.9692]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3661])\n",
      "data: tensor([[ 38.8976,   0.0000, 125.8557, 101.2504,   0.3661,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (103, 128)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 82.3767,  50.6252,  86.9581, 101.2504]])\n",
      "xywhn: tensor([[0.6436, 0.4915, 0.6794, 0.9830]])\n",
      "xyxy: tensor([[ 38.8976,   0.0000, 125.8557, 101.2504]])\n",
      "xyxyn: tensor([[0.3039, 0.0000, 0.9832, 0.9830]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3397])\n",
      "data: tensor([[ 10.7127,   0.7750, 110.0000, 124.9388,   0.3397,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (132, 110)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 60.3563,  62.8569,  99.2874, 124.1637]])\n",
      "xywhn: tensor([[0.5487, 0.4762, 0.9026, 0.9406]])\n",
      "xyxy: tensor([[ 10.7127,   0.7750, 110.0000, 124.9388]])\n",
      "xyxyn: tensor([[0.0974, 0.0059, 1.0000, 0.9465]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 114.5ms\n",
      "Speed: 3.1ms preprocess, 114.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 544x640 3 -1s, 139.9ms\n",
      "Speed: 12.8ms preprocess, 139.9ms inference, 0.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x544 1 -1, 137.7ms\n",
      "Speed: 3.2ms preprocess, 137.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.4902])\n",
      "data: tensor([[ 0.0000,  0.5149, 81.7738, 99.6518,  0.4902,  0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (103, 126)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[40.8869, 50.0833, 81.7738, 99.1369]])\n",
      "xywhn: tensor([[0.3245, 0.4862, 0.6490, 0.9625]])\n",
      "xyxy: tensor([[ 0.0000,  0.5149, 81.7738, 99.6518]])\n",
      "xyxyn: tensor([[0.0000, 0.0050, 0.6490, 0.9675]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2720])\n",
      "data: tensor([[ 20.1674,   0.0000, 115.3373,  99.9162,   0.2720,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (103, 126)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[67.7524, 49.9581, 95.1699, 99.9162]])\n",
      "xywhn: tensor([[0.5377, 0.4850, 0.7553, 0.9701]])\n",
      "xyxy: tensor([[ 20.1674,   0.0000, 115.3373,  99.9162]])\n",
      "xyxyn: tensor([[0.1601, 0.0000, 0.9154, 0.9701]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.2501])\n",
      "data: tensor([[ 39.1399,   0.0000, 124.1337,  63.4668,   0.2501,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (103, 126)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[81.6368, 31.7334, 84.9938, 63.4668]])\n",
      "xywhn: tensor([[0.6479, 0.3081, 0.6746, 0.6162]])\n",
      "xyxy: tensor([[ 39.1399,   0.0000, 124.1337,  63.4668]])\n",
      "xyxyn: tensor([[0.3106, 0.0000, 0.9852, 0.6162]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3459])\n",
      "data: tensor([[  4.9576,   2.7841, 109.8862, 124.0773,   0.3459,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (133, 110)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 57.4219,  63.4307, 104.9286, 121.2932]])\n",
      "xywhn: tensor([[0.5220, 0.4769, 0.9539, 0.9120]])\n",
      "xyxy: tensor([[  4.9576,   2.7841, 109.8862, 124.0773]])\n",
      "xyxyn: tensor([[0.0451, 0.0209, 0.9990, 0.9329]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 dining tables, 1 laptop, 1 mouse, 1 book, 107.5ms\n",
      "Speed: 0.7ms preprocess, 107.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x544 1 -1, 121.4ms\n",
      "Speed: 3.4ms preprocess, 121.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 544)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3950])\n",
      "data: tensor([[  6.8246,   2.6408, 107.3028, 124.9528,   0.3950,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (134, 110)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[ 57.0637,  63.7968, 100.4782, 122.3120]])\n",
      "xywhn: tensor([[0.5188, 0.4761, 0.9134, 0.9128]])\n",
      "xyxy: tensor([[  6.8246,   2.6408, 107.3028, 124.9528]])\n",
      "xyxyn: tensor([[0.0620, 0.0197, 0.9755, 0.9325]])\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# video = cv2.VideoCapture(\"/Users/gdiva/Downloads/walking.mp4\")\n",
    "video = cv2.VideoCapture(\"/Users/gdiva/Downloads/gettyimages-1286237053-640_adpp.mp4\")\n",
    "while True:\n",
    "\n",
    "    # Read the next frame from the video sequence\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    # If the frame is empty, break out of the loop\n",
    "    if not ret:\n",
    "        print(1)\n",
    "        break\n",
    "\n",
    "    result = model2(frame, stream=True)\n",
    "    detections = np.empty((0,5))\n",
    "    for r in result:\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            class_id = r.names[box.cls[0].item()]\n",
    "            if (class_id == 'person') :\n",
    "                conf = math.ceil(box.conf[0]*100)/100\n",
    "                if (conf>0.4):\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                    current_array = np.array([x1,y1,x2,y2,conf])\n",
    "                    detections = np.vstack((detections, current_array))\n",
    "\n",
    "    resultTracker = mot_tracker.update(detections)\n",
    "    class_labels = []\n",
    "    for r in resultTracker:\n",
    "        x1, y1, x2, y2, id = r\n",
    "        x1, y1, x2, y2, id = int(x1), int(y1), int(x2), int(y2), int(id)\n",
    "        image = np.copy(frame[y1:y2, x1:x2])\n",
    "        cv2.rectangle(frame, (x1,y1), (x2,y2), [0,0,255], 2)\n",
    "        predictions = model3(image, stream=True)\n",
    "        for pred in predictions:\n",
    "            boxes = pred.boxes\n",
    "            for box in boxes:\n",
    "                if (box.conf < 0.3):\n",
    "                    class_labels.append('fall')\n",
    "                else:\n",
    "                    class_labels.append('no fall')\n",
    "                print(box)\n",
    "\n",
    "    # result = model3(frame, stream=True)\n",
    "    # detections = np.empty((0,5))\n",
    "    # class_labels = []\n",
    "    # for r in result:\n",
    "    #     boxes = r.boxes\n",
    "    #     for box in boxes:\n",
    "    #         if (box.conf > 0.3):\n",
    "    #             class_labels.append('no fall')\n",
    "    #         else:\n",
    "    #             class_labels.append('fall')\n",
    "    #         print(box)\n",
    "        \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "    class_labels = np.array(class_labels)\n",
    "    if np.any(class_labels == 'fall'):\n",
    "        frame = cv2.putText(frame, \"Fall\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    else:\n",
    "        frame = cv2.putText(frame, \"No Fall\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Detected Faces', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "video.release()\n",
    "\n",
    "# Destroy all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbf80ae6-3c7f-4e11-bba2-93accbc2eca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 212.9ms\n",
      "Speed: 49.2ms preprocess, 212.9ms inference, 27.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 477ms/step\n",
      "[0.7250289]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 122.8ms\n",
      "Speed: 11.2ms preprocess, 122.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 99.9ms\n",
      "Speed: 4.8ms preprocess, 99.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75627345]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 106.2ms\n",
      "Speed: 4.5ms preprocess, 106.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76091456]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.6815833]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6815833, 0.74074346]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 105.2ms\n",
      "Speed: 0.0ms preprocess, 105.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[0.68316805]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68316805, 0.7378991]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.7ms\n",
      "Speed: 5.0ms preprocess, 102.7ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.6948345]\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6948345, 0.73007715]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 114.2ms\n",
      "Speed: 0.0ms preprocess, 114.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.68097204]\n",
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68097204, 0.7194254]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 100.3ms\n",
      "Speed: 4.2ms preprocess, 100.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.6792271]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6792271, 0.7154341]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 100.2ms\n",
      "Speed: 0.0ms preprocess, 100.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "[0.675878]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.675878, 0.7180451]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.3ms\n",
      "Speed: 1.5ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "[0.6862114]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6862114, 0.718106]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 100.6ms\n",
      "Speed: 2.9ms preprocess, 100.6ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.69974303]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69974303, 0.7185917]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 101.6ms\n",
      "Speed: 0.0ms preprocess, 101.6ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n",
      "[0.71098036]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71098036, 0.718601]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 115.2ms\n",
      "Speed: 0.0ms preprocess, 115.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.72561777]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72561777, 0.72473943]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 105.0ms\n",
      "Speed: 0.0ms preprocess, 105.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7295874]\n",
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7295874, 0.72830135]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 107.5ms\n",
      "Speed: 3.4ms preprocess, 107.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "[0.7382629]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7382629, 0.73272127]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 106.3ms\n",
      "Speed: 7.0ms preprocess, 106.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.75009096]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75009096, 0.7439083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 110.4ms\n",
      "Speed: 0.0ms preprocess, 110.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.75171393]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75171393, 0.7447493]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 110.7ms\n",
      "Speed: 0.0ms preprocess, 110.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7553528]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7553528, 0.74981964]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 106.3ms\n",
      "Speed: 3.9ms preprocess, 106.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "[0.7548907]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7548907, 0.7453369]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 104.8ms\n",
      "Speed: 0.0ms preprocess, 104.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7586239]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7586239, 0.73961747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 104.0ms\n",
      "Speed: 0.0ms preprocess, 104.0ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7642406]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7642406, 0.74049866]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 102.7ms\n",
      "Speed: 0.0ms preprocess, 102.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.76941586]\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76941586, 0.73993033]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 100.0ms\n",
      "Speed: 0.3ms preprocess, 100.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.76443356]\n",
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 handbag, 106.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76443356, 0.7279346]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 106.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7599674]\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7599674, 0.729747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 111.6ms\n",
      "Speed: 0.0ms preprocess, 111.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7625668]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7625668, 0.7292634]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 108.0ms\n",
      "Speed: 0.0ms preprocess, 108.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7641791]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7641791, 0.73063797]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 103.9ms\n",
      "Speed: 4.5ms preprocess, 103.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.76858485]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76858485, 0.73266876]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.7ms\n",
      "Speed: 0.0ms preprocess, 111.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.76452667]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76452667, 0.73649]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.6ms\n",
      "Speed: 0.0ms preprocess, 110.6ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.7637555]\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7637555, 0.73698014]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 106.9ms\n",
      "Speed: 0.0ms preprocess, 106.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7491515]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7491515, 0.73199004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 112.5ms\n",
      "Speed: 0.0ms preprocess, 112.5ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.74413466]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74413466, 0.7264601]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 96.5ms\n",
      "Speed: 0.0ms preprocess, 96.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "[0.7308234]\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7308234, 0.72560936]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.0ms\n",
      "Speed: 0.0ms preprocess, 111.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7260974]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7260974, 0.726843]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.7ms\n",
      "Speed: 0.0ms preprocess, 109.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7240431]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7240431, 0.72504634]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.6ms\n",
      "Speed: 3.8ms preprocess, 109.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7240082]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7240082, 0.71613926]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.2ms\n",
      "Speed: 0.0ms preprocess, 109.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7243817]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7243817, 0.7160856]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 99.4ms\n",
      "Speed: 0.0ms preprocess, 99.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7212706]\n",
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7212706, 0.7121856]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.1ms\n",
      "Speed: 3.3ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.71775377]\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71775377, 0.7127951]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.0ms\n",
      "Speed: 3.7ms preprocess, 102.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[0.72307163]\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72307163, 0.7102944]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.0ms\n",
      "Speed: 0.0ms preprocess, 111.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7213263]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7213263, 0.71016604]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 101.9ms\n",
      "Speed: 3.7ms preprocess, 101.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7192935]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7192935, 0.71320975]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 92.5ms\n",
      "Speed: 4.7ms preprocess, 92.5ms inference, 14.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "[0.72504747]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72504747, 0.71289384]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 115.6ms\n",
      "Speed: 4.2ms preprocess, 115.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7221155]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7221155, 0.71425265]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 106.6ms\n",
      "Speed: 0.0ms preprocess, 106.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7271585]\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7271585, 0.71069974]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 100.4ms\n",
      "Speed: 0.0ms preprocess, 100.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.73008335]\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73008335, 0.7087031]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 106.7ms\n",
      "Speed: 3.9ms preprocess, 106.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.72828877]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72828877, 0.70765805]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 95.3ms\n",
      "Speed: 0.0ms preprocess, 95.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.74418825]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74418825, 0.70656866]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 100.5ms\n",
      "Speed: 4.2ms preprocess, 100.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7494957]\n",
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7494957, 0.7074297]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 106.2ms\n",
      "Speed: 4.5ms preprocess, 106.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7495781]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7495781, 0.7091311]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.5ms\n",
      "Speed: 0.0ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.7577211]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7577211, 0.7127022]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.9ms\n",
      "Speed: 0.0ms preprocess, 110.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.76275885]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76275885, 0.714869]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.6ms\n",
      "Speed: 0.0ms preprocess, 102.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.76606697]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76606697, 0.7188562]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 101.5ms\n",
      "Speed: 4.3ms preprocess, 101.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7643021]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7643021, 0.7175034]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 103.8ms\n",
      "Speed: 0.0ms preprocess, 103.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.76424336]\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 84.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76424336, 0.7189712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 84.2ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7604198]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7604198, 0.7195641]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.1ms\n",
      "Speed: 3.8ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7508181]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7508181, 0.7248726]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 94.8ms\n",
      "Speed: 3.0ms preprocess, 94.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.74374884]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74374884, 0.7299448]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 98.8ms\n",
      "Speed: 4.6ms preprocess, 98.8ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7392103]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 95.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7392103, 0.7316487]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 95.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73149055]\n",
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73149055, 0.73257]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.4ms\n",
      "Speed: 3.7ms preprocess, 110.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.7331799]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7331799, 0.7345303]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 107.5ms\n",
      "Speed: 4.0ms preprocess, 107.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.736537]\n",
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.736537, 0.7384996]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.9ms\n",
      "Speed: 0.0ms preprocess, 111.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73790056]\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73790056, 0.7423198]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 108.1ms\n",
      "Speed: 2.8ms preprocess, 108.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7410942]\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7410942, 0.74554366]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 110.1ms\n",
      "Speed: 0.0ms preprocess, 110.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.74916345]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74916345, 0.74315804]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 102.8ms\n",
      "Speed: 4.1ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.74563396]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74563396, 0.7447155]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 105.9ms\n",
      "Speed: 0.0ms preprocess, 105.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "[0.7538595]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7538595, 0.7358386]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 86.3ms\n",
      "Speed: 4.4ms preprocess, 86.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7498504]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7498504, 0.7389338]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 95.3ms\n",
      "Speed: 2.1ms preprocess, 95.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.750008]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.750008, 0.7364131]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 107.2ms\n",
      "Speed: 3.0ms preprocess, 107.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "[0.7478661]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7478661, 0.73750275]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 105.0ms\n",
      "Speed: 0.0ms preprocess, 105.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7472229]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7472229, 0.73471004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 111.6ms\n",
      "Speed: 4.2ms preprocess, 111.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.74539924]\n",
      "1/1 [==============================] - 0s 55ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74539924, 0.7373505]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 102.2ms\n",
      "Speed: 2.5ms preprocess, 102.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.74928105]\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74928105, 0.73604834]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 110.4ms\n",
      "Speed: 0.0ms preprocess, 110.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.7574416]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7574416, 0.73874974]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 112.3ms\n",
      "Speed: 0.0ms preprocess, 112.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.76235634]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76235634, 0.74064505]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 108.6ms\n",
      "Speed: 2.4ms preprocess, 108.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.762853]\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.762853, 0.74493897]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 103.1ms\n",
      "Speed: 0.0ms preprocess, 103.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7706621]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7706621, 0.7527251]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 102.0ms\n",
      "Speed: 3.6ms preprocess, 102.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7664967]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7664967, 0.7541853]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 106.0ms\n",
      "Speed: 3.1ms preprocess, 106.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7624323]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7624323, 0.7568855]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 95.6ms\n",
      "Speed: 4.8ms preprocess, 95.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[0.75869817]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75869817, 0.75562686]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 110.8ms\n",
      "Speed: 3.6ms preprocess, 110.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.75247246]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75247246, 0.7608763]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 106.5ms\n",
      "Speed: 4.3ms preprocess, 106.5ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.749526]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.749526, 0.76414585]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 103.9ms\n",
      "Speed: 3.5ms preprocess, 103.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.74887943]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74887943, 0.7616534]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 108.9ms\n",
      "Speed: 0.0ms preprocess, 108.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.74939257]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74939257, 0.76153284]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 2 dogs, 1 handbag, 103.2ms\n",
      "Speed: 3.6ms preprocess, 103.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n",
      "[0.74932486]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74932486, 0.76105386]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 110.9ms\n",
      "Speed: 3.6ms preprocess, 110.9ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "[0.7505854]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7505854, 0.75094765]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 118.2ms\n",
      "Speed: 4.9ms preprocess, 118.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.75737554]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75737554, 0.744715]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 102.3ms\n",
      "Speed: 0.0ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7605093]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7605093, 0.74563676]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 105.5ms\n",
      "Speed: 3.8ms preprocess, 105.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7658332]\n",
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7658332, 0.7470524]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 120.4ms\n",
      "Speed: 4.7ms preprocess, 120.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7624116]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7624116, 0.75301313]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 87.9ms\n",
      "Speed: 2.5ms preprocess, 87.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7570316]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7570316, 0.74840033]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 2 dogs, 107.5ms\n",
      "Speed: 4.4ms preprocess, 107.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "[0.75593174]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75593174, 0.74212325]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 87.4ms\n",
      "Speed: 0.0ms preprocess, 87.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.75099957]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75099957, 0.73639435]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 111.5ms\n",
      "Speed: 0.0ms preprocess, 111.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.75501597]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75501597, 0.7325986]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 113.5ms\n",
      "Speed: 3.1ms preprocess, 113.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7542064]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7542064, 0.724312]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 109.4ms\n",
      "Speed: 0.0ms preprocess, 109.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "[0.7527088]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7527088, 0.72883207]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 108.2ms\n",
      "Speed: 0.0ms preprocess, 108.2ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.750924]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.750924, 0.7274606]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 111.0ms\n",
      "Speed: 0.0ms preprocess, 111.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7561188]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7561188, 0.72876954]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 104.4ms\n",
      "Speed: 2.9ms preprocess, 104.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7523956]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7523956, 0.730655]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 101.8ms\n",
      "Speed: 0.0ms preprocess, 101.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7534247]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7534247, 0.7354983]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 1 bear, 110.9ms\n",
      "Speed: 3.8ms preprocess, 110.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step\n",
      "[0.75587577]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75587577, 0.7334431]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 103.8ms\n",
      "Speed: 4.1ms preprocess, 103.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7537059]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7537059, 0.7359243]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 97.8ms\n",
      "Speed: 0.0ms preprocess, 97.8ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.763749]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.763749, 0.7310175]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 104.1ms\n",
      "Speed: 2.0ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.76622987]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76622987, 0.72250473]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 90.4ms\n",
      "Speed: 0.0ms preprocess, 90.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.76530254]\n",
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76530254, 0.71635985]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 111.9ms\n",
      "Speed: 0.0ms preprocess, 111.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "[0.77018]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77018, 0.72225803]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 102.9ms\n",
      "Speed: 0.0ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.76377416]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76377416, 0.7277024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 105.4ms\n",
      "Speed: 0.0ms preprocess, 105.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.77397]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77397, 0.7393566]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 105.6ms\n",
      "Speed: 5.1ms preprocess, 105.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7797715]\n",
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7797715, 0.74076515]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 109.8ms\n",
      "Speed: 0.0ms preprocess, 109.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "[0.775084]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.775084, 0.7456877]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 108.0ms\n",
      "Speed: 3.1ms preprocess, 108.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.77333516]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77333516, 0.7415017]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 105.8ms\n",
      "Speed: 0.0ms preprocess, 105.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7770814]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7770814, 0.7433348]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 104.8ms\n",
      "Speed: 0.0ms preprocess, 104.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.77314734]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77314734, 0.7405583]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 104.6ms\n",
      "Speed: 0.0ms preprocess, 104.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.76927465]\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76927465, 0.7398321]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 95.4ms\n",
      "Speed: 2.9ms preprocess, 95.4ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7667665]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7667665, 0.73497707]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 100.8ms\n",
      "Speed: 4.0ms preprocess, 100.8ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "[0.7628913]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 dog, 1 cow, 94.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7628913, 0.7305322]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 94.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.76172924]\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76172924, 0.7335577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 85.7ms\n",
      "Speed: 4.2ms preprocess, 85.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "[0.7632094]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7632094, 0.72891736]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 91.2ms\n",
      "Speed: 0.0ms preprocess, 91.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "[0.76160324]\n",
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76160324, 0.72668207]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bird, 104.3ms\n",
      "Speed: 0.0ms preprocess, 104.3ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.75209665]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75209665, 0.7233068]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.5ms\n",
      "Speed: 0.0ms preprocess, 104.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7445763]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7445763, 0.7287558]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 98.2ms\n",
      "Speed: 4.4ms preprocess, 98.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.72933054]\n",
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72933054, 0.7328516]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 97.0ms\n",
      "Speed: 0.0ms preprocess, 97.0ms inference, 12.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73143405]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73143405, 0.737726]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 104.4ms\n",
      "Speed: 0.0ms preprocess, 104.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7351689]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7351689, 0.7387963]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 98.7ms\n",
      "Speed: 4.5ms preprocess, 98.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[0.74218905]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74218905, 0.7363797]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 103.5ms\n",
      "Speed: 4.4ms preprocess, 103.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73718625]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73718625, 0.7367375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 106.0ms\n",
      "Speed: 0.0ms preprocess, 106.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.74125284]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74125284, 0.7395089]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 105.7ms\n",
      "Speed: 0.0ms preprocess, 105.7ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.74452806]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74452806, 0.74522036]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 102.8ms\n",
      "Speed: 2.4ms preprocess, 102.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n",
      "[0.74992895]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74992895, 0.7377323]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 98.6ms\n",
      "Speed: 1.4ms preprocess, 98.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.75895417]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75895417, 0.7358609]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 106.0ms\n",
      "Speed: 2.7ms preprocess, 106.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.75001514]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75001514, 0.72926116]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.5ms\n",
      "Speed: 4.0ms preprocess, 111.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "[0.7395314]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7395314, 0.7328689]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bird, 104.7ms\n",
      "Speed: 3.0ms preprocess, 104.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7366976]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 92.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7366976, 0.7275728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.2ms preprocess, 92.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7257726]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7257726, 0.7258892]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 111.4ms\n",
      "Speed: 0.0ms preprocess, 111.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7205316]\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7205316, 0.723803]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 97.3ms\n",
      "Speed: 3.0ms preprocess, 97.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.71247333]\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71247333, 0.7267877]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 85.6ms\n",
      "Speed: 0.0ms preprocess, 85.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7130967]\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7130967, 0.72791237]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 98.8ms\n",
      "Speed: 4.4ms preprocess, 98.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7129687]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7129687, 0.738681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.6ms\n",
      "Speed: 0.0ms preprocess, 110.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.71506757]\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71506757, 0.750852]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 101.2ms\n",
      "Speed: 1.7ms preprocess, 101.2ms inference, 8.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.71766037]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71766037, 0.75404733]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.1ms\n",
      "Speed: 0.0ms preprocess, 102.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7162742]\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7162742, 0.7561728]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 101.5ms\n",
      "Speed: 3.6ms preprocess, 101.5ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7118069]\n",
      "1/1 [==============================] - 0s 59ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7118069, 0.7458236]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.3ms\n",
      "Speed: 0.0ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7159669]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7159669, 0.74165064]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.7ms\n",
      "Speed: 3.8ms preprocess, 107.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.71795106]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71795106, 0.7375139]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.6ms\n",
      "Speed: 4.3ms preprocess, 110.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7278135]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7278135, 0.73462784]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 119.5ms\n",
      "Speed: 0.0ms preprocess, 119.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "[0.73448515]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73448515, 0.73044413]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 105.2ms\n",
      "Speed: 0.0ms preprocess, 105.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7275739]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7275739, 0.73383504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.4ms\n",
      "Speed: 0.0ms preprocess, 104.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7251787]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7251787, 0.73102456]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 105.9ms\n",
      "Speed: 3.0ms preprocess, 105.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7270461]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 102.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7270461, 0.7293544]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 102.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "[0.73321545]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73321545, 0.73105454]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.9ms\n",
      "Speed: 3.2ms preprocess, 107.9ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7386239]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7386239, 0.7325841]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.2ms\n",
      "Speed: 0.0ms preprocess, 110.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73807156]\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73807156, 0.7320843]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.6ms\n",
      "Speed: 0.0ms preprocess, 110.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "[0.7392877]\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7392877, 0.7367374]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 86.1ms\n",
      "Speed: 4.5ms preprocess, 86.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7368853]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7368853, 0.7262645]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 106.2ms\n",
      "Speed: 4.1ms preprocess, 106.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73632735]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73632735, 0.71856695]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 114.7ms\n",
      "Speed: 0.0ms preprocess, 114.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7420453]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7420453, 0.71350557]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 103.5ms\n",
      "Speed: 4.5ms preprocess, 103.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.74423575]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74423575, 0.70487905]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.7ms\n",
      "Speed: 4.1ms preprocess, 107.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7465355]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7465355, 0.7070869]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.9ms\n",
      "Speed: 3.4ms preprocess, 104.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "[0.7415786]\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7415786, 0.7063177]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.0ms\n",
      "Speed: 0.0ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.73529714]\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73529714, 0.7084661]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 99.1ms\n",
      "Speed: 3.5ms preprocess, 99.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "[0.7251773]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7251773, 0.7115978]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.2ms\n",
      "Speed: 0.0ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7222754]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7222754, 0.70745516]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 119.5ms\n",
      "Speed: 0.0ms preprocess, 119.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7207965]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7207965, 0.7038584]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.1ms\n",
      "Speed: 0.0ms preprocess, 111.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7193019]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7193019, 0.70514166]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.0ms\n",
      "Speed: 0.0ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.71158475]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71158475, 0.703923]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 100.0ms\n",
      "Speed: 2.9ms preprocess, 100.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "[0.7087831]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7087831, 0.70784414]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.1ms\n",
      "Speed: 2.2ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.6988783]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6988783, 0.7121317]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.5ms\n",
      "Speed: 0.0ms preprocess, 107.5ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.68965054]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68965054, 0.7138648]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 103.6ms\n",
      "Speed: 0.0ms preprocess, 103.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.6810352]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6810352, 0.7250763]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 100.5ms\n",
      "Speed: 0.0ms preprocess, 100.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.687288]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.687288, 0.7254318]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.3ms\n",
      "Speed: 3.6ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.70015246]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70015246, 0.7238499]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.1ms\n",
      "Speed: 0.0ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.707772]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.707772, 0.72354496]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 101.3ms\n",
      "Speed: 0.0ms preprocess, 101.3ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.70724875]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70724875, 0.7246704]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.2ms\n",
      "Speed: 1.8ms preprocess, 109.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "[0.709207]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.709207, 0.7220402]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 106.0ms\n",
      "Speed: 0.0ms preprocess, 106.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n",
      "[0.7040877]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 101.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7040877, 0.7229398]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 4.1ms preprocess, 101.8ms inference, 13.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "[0.7082891]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7082891, 0.72888136]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 105.7ms\n",
      "Speed: 0.0ms preprocess, 105.7ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.71066064]\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71066064, 0.7318172]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 99.0ms\n",
      "Speed: 4.8ms preprocess, 99.0ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7044474]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 94.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7044474, 0.73476356]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 94.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "[0.69872606]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69872606, 0.7372726]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 108.0ms\n",
      "Speed: 0.0ms preprocess, 108.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.701865]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.701865, 0.73731524]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 121.3ms\n",
      "Speed: 4.0ms preprocess, 121.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7073852]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7073852, 0.73752564]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.4ms\n",
      "Speed: 4.0ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.71625155]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71625155, 0.74142545]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 108.6ms\n",
      "Speed: 4.2ms preprocess, 108.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7154629]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7154629, 0.7452216]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.0ms\n",
      "Speed: 3.4ms preprocess, 102.0ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.71565825]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71565825, 0.742423]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.2ms\n",
      "Speed: 0.0ms preprocess, 109.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.70873123]\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70873123, 0.7451816]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 108.5ms\n",
      "Speed: 2.7ms preprocess, 108.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7170031]\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7170031, 0.74572545]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.6ms\n",
      "Speed: 0.0ms preprocess, 104.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.72669816]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72669816, 0.743306]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 98.2ms\n",
      "Speed: 0.0ms preprocess, 98.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "[0.72717226]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72717226, 0.7398395]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.5ms\n",
      "Speed: 0.0ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "[0.717603]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 103.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.717603, 0.73699474]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.5ms preprocess, 103.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7214338]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7214338, 0.7400784]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.2ms\n",
      "Speed: 0.0ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.71595496]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71595496, 0.7406096]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 108.1ms\n",
      "Speed: 4.1ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7187237]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7187237, 0.7351935]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 103.7ms\n",
      "Speed: 0.0ms preprocess, 103.7ms inference, 9.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "[0.7137456]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7137456, 0.7314893]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 108.5ms\n",
      "Speed: 3.0ms preprocess, 108.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.71151006]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71151006, 0.7281702]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 113.6ms\n",
      "Speed: 5.1ms preprocess, 113.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.71241796]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71241796, 0.72645426]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.9ms\n",
      "Speed: 2.2ms preprocess, 109.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.71041924]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71041924, 0.72221017]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 110.3ms\n",
      "Speed: 0.0ms preprocess, 110.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7071153]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7071153, 0.72626185]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 104.1ms\n",
      "Speed: 6.1ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7098286]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7098286, 0.7231182]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 1 handbag, 111.3ms\n",
      "Speed: 3.5ms preprocess, 111.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.71297]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71297, 0.72153616]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 101.9ms\n",
      "Speed: 2.9ms preprocess, 101.9ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7091437]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7091437, 0.7285341]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 107.1ms\n",
      "Speed: 0.0ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.70636934]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70636934, 0.7260785]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 103.6ms\n",
      "Speed: 4.0ms preprocess, 103.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7087638]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7087638, 0.72208077]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 106.7ms\n",
      "Speed: 0.0ms preprocess, 106.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7047177]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7047177, 0.71647316]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 119.1ms\n",
      "Speed: 2.5ms preprocess, 119.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.6962826]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6962826, 0.7116648]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 dog, 104.6ms\n",
      "Speed: 0.0ms preprocess, 104.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "[0.69154096]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69154096, 0.70799834]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 dog, 106.0ms\n",
      "Speed: 3.0ms preprocess, 106.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[0.6969192]\n",
      "1/1 [==============================] - 0s 53ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6969192, 0.7048102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 111.1ms\n",
      "Speed: 2.6ms preprocess, 111.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 dog, 94.3ms\n",
      "Speed: 0.0ms preprocess, 94.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7161145]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 dog, 92.8ms\n",
      "Speed: 0.0ms preprocess, 92.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71551365]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 dog, 113.0ms\n",
      "Speed: 3.7ms preprocess, 113.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7170363]\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 dog, 94.6ms\n",
      "Speed: 0.0ms preprocess, 94.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7184263]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "[0.7397959]\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7397959, 0.69578433]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 124.1ms\n",
      "Speed: 0.0ms preprocess, 124.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7388523]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7388523, 0.7021866]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 100.3ms\n",
      "Speed: 3.6ms preprocess, 100.3ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73479515]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73479515, 0.7065928]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 118.0ms\n",
      "Speed: 4.1ms preprocess, 118.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73107874]\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73107874, 0.7028456]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 111.5ms\n",
      "Speed: 4.4ms preprocess, 111.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7320204]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 dog, 95.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7320204, 0.70221734]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 95.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.735997]\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.735997, 0.70063794]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 94.8ms\n",
      "Speed: 0.0ms preprocess, 94.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "[0.7302111]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7302111, 0.70160764]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 93.8ms\n",
      "Speed: 4.2ms preprocess, 93.8ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.7350883]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7350883, 0.7006386]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 103.4ms\n",
      "Speed: 2.5ms preprocess, 103.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.74262244]\n",
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 86.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74262244, 0.7024742]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 86.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7473671]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7473671, 0.7025167]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 1 bear, 105.4ms\n",
      "Speed: 0.0ms preprocess, 105.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.76584905]\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76584905, 0.71167916]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 1 frisbee, 95.0ms\n",
      "Speed: 4.0ms preprocess, 95.0ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.76801676]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76801676, 0.70272756]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 97.4ms\n",
      "Speed: 2.6ms preprocess, 97.4ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7777974]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7777974, 0.70902616]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 109.2ms\n",
      "Speed: 0.0ms preprocess, 109.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "[0.7877216]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7877216, 0.7157558]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 111.9ms\n",
      "Speed: 0.0ms preprocess, 111.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.76279885]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76279885, 0.718396]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 dog, 1 cow, 106.9ms\n",
      "Speed: 0.0ms preprocess, 106.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.7633054]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7633054, 0.7292345]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 110.4ms\n",
      "Speed: 0.0ms preprocess, 110.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.75375223]\n",
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75375223, 0.7379406]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 sheep, 104.1ms\n",
      "Speed: 0.0ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.75027496]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75027496, 0.7441644]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 107.2ms\n",
      "Speed: 4.4ms preprocess, 107.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.7514211]\n",
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7514211, 0.7452268]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 120.1ms\n",
      "Speed: 0.0ms preprocess, 120.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.7465001]\n",
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7465001, 0.7410558]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 106.3ms\n",
      "Speed: 0.0ms preprocess, 106.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.74584657]\n",
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74584657, 0.73972267]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 108.5ms\n",
      "Speed: 3.8ms preprocess, 108.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "[0.7512004]\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7512004, 0.74409825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 105.1ms\n",
      "Speed: 3.6ms preprocess, 105.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7427962]\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7427962, 0.7341036]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 117.5ms\n",
      "Speed: 2.7ms preprocess, 117.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "[0.7322329]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7322329, 0.7247339]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 118.1ms\n",
      "Speed: 0.0ms preprocess, 118.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "[0.7266612]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7266612, 0.71596557]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.2ms\n",
      "Speed: 0.0ms preprocess, 110.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.727653]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.727653, 0.7116743]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 99.5ms\n",
      "Speed: 2.5ms preprocess, 99.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.73098755]\n",
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73098755, 0.70392495]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 99.9ms\n",
      "Speed: 3.7ms preprocess, 99.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7308674]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7308674, 0.70710284]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 103.4ms\n",
      "Speed: 0.0ms preprocess, 103.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73691523]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73691523, 0.70526856]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.2ms\n",
      "Speed: 2.8ms preprocess, 111.2ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "[0.7307091]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7307091, 0.70566547]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.5ms\n",
      "Speed: 3.0ms preprocess, 107.5ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7327086]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 99.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7327086, 0.7046456]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.7ms preprocess, 99.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7423847]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7423847, 0.7042977]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 105.2ms\n",
      "Speed: 0.0ms preprocess, 105.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7378864]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7378864, 0.70497763]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 106.3ms\n",
      "Speed: 2.6ms preprocess, 106.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73828614]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73828614, 0.70067674]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.2ms\n",
      "Speed: 0.0ms preprocess, 110.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.729654]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.729654, 0.69774044]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 114.2ms\n",
      "Speed: 4.3ms preprocess, 114.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.73214585]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73214585, 0.6971364]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.7ms\n",
      "Speed: 3.5ms preprocess, 107.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73730844]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73730844, 0.6969295]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 106.3ms\n",
      "Speed: 3.6ms preprocess, 106.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73811793]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73811793, 0.699536]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.5ms\n",
      "Speed: 0.0ms preprocess, 111.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7359781]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7359781, 0.699071]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 106.3ms\n",
      "Speed: 0.0ms preprocess, 106.3ms inference, 14.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.74056065]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74056065, 0.7021088]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 100.9ms\n",
      "Speed: 2.2ms preprocess, 100.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.737414]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.737414, 0.7105725]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 115.6ms\n",
      "Speed: 0.0ms preprocess, 115.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7318063]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7318063, 0.7157558]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 100.3ms\n",
      "Speed: 4.5ms preprocess, 100.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.72982377]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72982377, 0.7224236]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.2ms\n",
      "Speed: 4.2ms preprocess, 110.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "[0.72978187]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72978187, 0.7326943]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 99.4ms\n",
      "Speed: 3.8ms preprocess, 99.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7295987]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7295987, 0.72837615]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 112.6ms\n",
      "Speed: 4.4ms preprocess, 112.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "[0.7284728]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7284728, 0.7264328]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 sheep, 112.2ms\n",
      "Speed: 3.5ms preprocess, 112.2ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.729522]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.729522, 0.73009074]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 114.8ms\n",
      "Speed: 5.3ms preprocess, 114.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.72800237]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72800237, 0.7278389]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.7ms\n",
      "Speed: 2.8ms preprocess, 109.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7266086]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7266086, 0.72925836]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 111.9ms\n",
      "Speed: 0.0ms preprocess, 111.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7189811]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7189811, 0.73050624]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 horse, 1 cow, 113.3ms\n",
      "Speed: 4.6ms preprocess, 113.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7133324]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7133324, 0.7291729]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bear, 123.5ms\n",
      "Speed: 0.0ms preprocess, 123.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.71382505]\n",
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71382505, 0.73252326]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 118.8ms\n",
      "Speed: 0.0ms preprocess, 118.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.71577376]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71577376, 0.7270493]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 horse, 116.4ms\n",
      "Speed: 0.0ms preprocess, 116.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.72879106]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72879106, 0.7283277]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 elephant, 107.3ms\n",
      "Speed: 3.6ms preprocess, 107.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[0.731629]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.731629, 0.72624]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 103.3ms\n",
      "Speed: 3.7ms preprocess, 103.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "[0.739904]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.739904, 0.72279227]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 112.8ms\n",
      "Speed: 4.9ms preprocess, 112.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "[0.74263376]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74263376, 0.7159322]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.6ms\n",
      "Speed: 0.0ms preprocess, 110.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.7440193]\n",
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7440193, 0.707549]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 116.5ms\n",
      "Speed: 3.1ms preprocess, 116.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7435882]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7435882, 0.71065855]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.8ms\n",
      "Speed: 0.0ms preprocess, 111.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7484601]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7484601, 0.6975116]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.2ms\n",
      "Speed: 0.0ms preprocess, 104.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7547528]\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7547528, 0.6994839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.8ms\n",
      "Speed: 0.0ms preprocess, 110.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "[0.7604099]\n",
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7604099, 0.6967682]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 98.9ms\n",
      "Speed: 3.4ms preprocess, 98.9ms inference, 9.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7552497]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7552497, 0.6961689]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.5ms\n",
      "Speed: 0.0ms preprocess, 109.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7484488]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7484488, 0.6948954]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.2ms\n",
      "Speed: 3.5ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7372235]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7372235, 0.6884734]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 121.3ms\n",
      "Speed: 0.0ms preprocess, 121.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7349857]\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7349857, 0.6859249]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 108.8ms\n",
      "Speed: 0.0ms preprocess, 108.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.72244114]\n",
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72244114, 0.6833782]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.4ms\n",
      "Speed: 0.0ms preprocess, 104.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7212913]\n",
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7212913, 0.6837075]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 119.7ms\n",
      "Speed: 3.5ms preprocess, 119.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step\n",
      "[0.71506196]\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71506196, 0.6860321]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.0ms\n",
      "Speed: 3.0ms preprocess, 111.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n",
      "[0.71767724]\n",
      "1/1 [==============================] - 0s 51ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71767724, 0.69416076]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.3ms\n",
      "Speed: 0.0ms preprocess, 111.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.71659297]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71659297, 0.69398487]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 121.2ms\n",
      "Speed: 0.0ms preprocess, 121.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[0.71205866]\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71205866, 0.69429487]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 97.7ms\n",
      "Speed: 0.0ms preprocess, 97.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.716159]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.716159, 0.7042032]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 100.8ms\n",
      "Speed: 4.0ms preprocess, 100.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7158603]\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7158603, 0.71478635]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 98.1ms\n",
      "Speed: 0.0ms preprocess, 98.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.70435315]\n",
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70435315, 0.72047687]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 113.2ms\n",
      "Speed: 5.0ms preprocess, 113.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "[0.7051824]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7051824, 0.72018045]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 101.0ms\n",
      "Speed: 4.4ms preprocess, 101.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7021012]\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7021012, 0.72181916]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.3ms\n",
      "Speed: 0.0ms preprocess, 110.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.70237184]\n",
      "1/1 [==============================] - 0s 52ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 102.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70237184, 0.72104037]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 102.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "[0.71447784]\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71447784, 0.713893]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 112.4ms\n",
      "Speed: 0.0ms preprocess, 112.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.71954364]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71954364, 0.7115371]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.1ms\n",
      "Speed: 3.4ms preprocess, 110.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7234361]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7234361, 0.7169855]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 112.0ms\n",
      "Speed: 0.0ms preprocess, 112.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7215196]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7215196, 0.715314]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.1ms\n",
      "Speed: 0.0ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7218213]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7218213, 0.71810603]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 99.2ms\n",
      "Speed: 4.8ms preprocess, 99.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "[0.72860783]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72860783, 0.7209194]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.1ms\n",
      "Speed: 5.3ms preprocess, 102.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.73490036]\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73490036, 0.71037847]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.1ms\n",
      "Speed: 0.0ms preprocess, 111.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "[0.74201006]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74201006, 0.7097737]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.4ms\n",
      "Speed: 0.0ms preprocess, 110.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "[0.74751127]\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74751127, 0.7147694]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 108.8ms\n",
      "Speed: 0.0ms preprocess, 108.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.74267805]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74267805, 0.7052827]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.8ms\n",
      "Speed: 0.0ms preprocess, 104.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73884785]\n",
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73884785, 0.7044163]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 101.5ms\n",
      "Speed: 4.6ms preprocess, 101.5ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "[0.73362494]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73362494, 0.70361406]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.1ms\n",
      "Speed: 3.5ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.72999066]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72999066, 0.6974465]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 115.3ms\n",
      "Speed: 3.7ms preprocess, 115.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.73470753]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73470753, 0.70048004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.9ms\n",
      "Speed: 3.0ms preprocess, 107.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7284449]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7284449, 0.70462734]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 105.8ms\n",
      "Speed: 3.0ms preprocess, 105.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7259884]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7259884, 0.70916086]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.1ms\n",
      "Speed: 4.1ms preprocess, 104.1ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7215077]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7215077, 0.7131958]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 103.5ms\n",
      "Speed: 3.5ms preprocess, 103.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "[0.71843535]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71843535, 0.7141957]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 95.3ms\n",
      "Speed: 3.5ms preprocess, 95.3ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7213263]\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7213263, 0.71027213]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.4ms\n",
      "Speed: 0.0ms preprocess, 109.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7279368]\n",
      "1/1 [==============================] - 0s 51ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7279368, 0.6941664]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.6ms\n",
      "Speed: 2.5ms preprocess, 110.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7230285]\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7230285, 0.697981]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 92.0ms\n",
      "Speed: 2.4ms preprocess, 92.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step\n",
      "[0.7212191]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7212191, 0.7019592]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 105.7ms\n",
      "Speed: 4.4ms preprocess, 105.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7237283]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7237283, 0.7121151]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 105.6ms\n",
      "Speed: 4.2ms preprocess, 105.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.72579366]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72579366, 0.7205971]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 105.7ms\n",
      "Speed: 4.2ms preprocess, 105.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7233821]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7233821, 0.7204212]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 118.9ms\n",
      "Speed: 0.0ms preprocess, 118.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7251831]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7251831, 0.7096771]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.7ms\n",
      "Speed: 0.0ms preprocess, 102.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n",
      "[0.7296476]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7296476, 0.7067367]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 105.3ms\n",
      "Speed: 3.5ms preprocess, 105.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.72732306]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72732306, 0.7041924]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 103.2ms\n",
      "Speed: 2.8ms preprocess, 103.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.731737]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.731737, 0.70930624]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 99.6ms\n",
      "Speed: 4.1ms preprocess, 99.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73223853]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73223853, 0.70917386]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 112.1ms\n",
      "Speed: 3.9ms preprocess, 112.1ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7369063]\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7369063, 0.71143484]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 126.9ms\n",
      "Speed: 0.0ms preprocess, 126.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.741919]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.741919, 0.70924056]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 96.8ms\n",
      "Speed: 3.2ms preprocess, 96.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.74341285]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74341285, 0.70465714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 103.3ms\n",
      "Speed: 0.0ms preprocess, 103.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.7411232]\n",
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7411232, 0.7037421]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 112.2ms\n",
      "Speed: 0.0ms preprocess, 112.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73803246]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73803246, 0.70469326]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 112.8ms\n",
      "Speed: 4.0ms preprocess, 112.8ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7366006]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7366006, 0.70179623]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.7ms\n",
      "Speed: 0.0ms preprocess, 109.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.73470145]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73470145, 0.6926369]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 104.4ms\n",
      "Speed: 0.0ms preprocess, 104.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7287136]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7287136, 0.6879924]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.8ms\n",
      "Speed: 4.7ms preprocess, 111.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7182342]\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7182342, 0.7005141]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.5ms\n",
      "Speed: 7.1ms preprocess, 111.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "[0.7142841]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7142841, 0.70022357]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 107.1ms\n",
      "Speed: 4.9ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "[0.71293324]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71293324, 0.69652176]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 102.6ms\n",
      "Speed: 3.9ms preprocess, 102.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7075099]\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7075099, 0.69210774]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.4ms\n",
      "Speed: 0.0ms preprocess, 110.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "[0.70143855]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70143855, 0.68823093]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 111.6ms\n",
      "Speed: 0.0ms preprocess, 111.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.6931892]\n",
      "1/1 [==============================] - 0s 55ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6931892, 0.68185174]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.6ms\n",
      "Speed: 4.0ms preprocess, 109.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.68965703]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68965703, 0.6813832]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 114.7ms\n",
      "Speed: 0.0ms preprocess, 114.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "[0.6899416]\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 103.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6899416, 0.68478376]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 103.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "[0.6988919]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6988919, 0.68495184]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 106.7ms\n",
      "Speed: 4.1ms preprocess, 106.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.70662624]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70662624, 0.6812765]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 112.8ms\n",
      "Speed: 2.5ms preprocess, 112.8ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.70601255]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70601255, 0.69055253]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 117.8ms\n",
      "Speed: 0.0ms preprocess, 117.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "[0.7029616]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7029616, 0.6931032]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.9ms\n",
      "Speed: 3.8ms preprocess, 109.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "[0.7086667]\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7086667, 0.6963928]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.0ms\n",
      "Speed: 0.0ms preprocess, 109.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.70921946]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70921946, 0.6927638]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 114.3ms\n",
      "Speed: 0.0ms preprocess, 114.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.71154326]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71154326, 0.684894]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 103.9ms\n",
      "Speed: 0.0ms preprocess, 103.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.70746535]\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70746535, 0.67717326]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 85.6ms\n",
      "Speed: 3.0ms preprocess, 85.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.715543]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.715543, 0.6744592]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 110.6ms\n",
      "Speed: 0.0ms preprocess, 110.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7236312]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7236312, 0.67135024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 109.5ms\n",
      "Speed: 0.0ms preprocess, 109.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "[0.725576]\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.725576, 0.67004186]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 101.4ms\n",
      "Speed: 3.1ms preprocess, 101.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.7228352]\n",
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7228352, 0.6721792]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 1 bear, 1 skateboard, 102.9ms\n",
      "Speed: 0.0ms preprocess, 102.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.72146857]\n",
      "1/1 [==============================] - 0s 55ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72146857, 0.67145264]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 111.8ms\n",
      "Speed: 0.0ms preprocess, 111.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.7181512]\n",
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7181512, 0.669106]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 111.7ms\n",
      "Speed: 0.0ms preprocess, 111.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.7169267]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7169267, 0.67303884]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 110.8ms\n",
      "Speed: 4.3ms preprocess, 110.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.7116483]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7116483, 0.6801917]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 122.0ms\n",
      "Speed: 0.0ms preprocess, 122.0ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "[0.71049494]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71049494, 0.6871096]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 106.5ms\n",
      "Speed: 3.3ms preprocess, 106.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[0.70865816]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70865816, 0.69362366]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 101.3ms\n",
      "Speed: 0.0ms preprocess, 101.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "[0.706796]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.706796, 0.6982557]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 117.8ms\n",
      "Speed: 0.0ms preprocess, 117.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.70240545]\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70240545, 0.69904184]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 122.8ms\n",
      "Speed: 0.0ms preprocess, 122.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "[0.701708]\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.701708, 0.7017285]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 1 handbag, 111.2ms\n",
      "Speed: 3.6ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.6970053]\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6970053, 0.7062033]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 1 handbag, 122.7ms\n",
      "Speed: 0.0ms preprocess, 122.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.70094347]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70094347, 0.70328236]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 handbag, 107.6ms\n",
      "Speed: 4.0ms preprocess, 107.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.70025474]\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70025474, 0.6986039]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 115.8ms\n",
      "Speed: 4.5ms preprocess, 115.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[0.7021142]\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7021142, 0.6984238]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cow, 98.6ms\n",
      "Speed: 4.2ms preprocess, 98.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "[0.7076982]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.7076982, 0.69875616]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# video = cv2.VideoCapture(\"/Users/gdiva/Downloads/gettyimages-1286237053-640_adpp.mp4\")\n",
    "video = cv2.VideoCapture(\"/Users/gdiva/Downloads/walking.mp4\")\n",
    "while True:\n",
    "\n",
    "    # Read the next frame from the video sequence\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    # If the frame is empty, break out of the loop\n",
    "    if not ret:\n",
    "        print(1)\n",
    "        break\n",
    "\n",
    "    result = model2(frame, stream=True)\n",
    "    detections = np.empty((0,5))\n",
    "    class_labels = []\n",
    "    for r in result:\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            class_id = r.names[box.cls[0].item()]\n",
    "            if (class_id == 'person') :\n",
    "                conf = math.ceil(box.conf[0]*100)/100\n",
    "                if (conf>0.4):\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                    current_array = np.array([x1,y1,x2,y2,conf])\n",
    "                    detections = np.vstack((detections, current_array))\n",
    "\n",
    "    resultTracker = mot_tracker.update(detections)\n",
    "\n",
    "    for r in resultTracker:\n",
    "        x1, y1, x2, y2, id = r\n",
    "        x1, y1, x2, y2, id = int(x1), int(y1), int(x2), int(y2), int(id)\n",
    "        image = np.copy(frame[y1:y2, x1:x2])\n",
    "        cv2.rectangle(frame, (x1,y1), (x2,y2), [0,0,255], 2)\n",
    "        coef = predict_image(image)\n",
    "        class_labels.append(coef)\n",
    "        print(class_labels)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        coef = str(coef)\n",
    "        frame = cv2.putText(frame, coef, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    class_labels = np.array(class_labels)\n",
    "    if np.any(class_labels > 0.91):\n",
    "        frame = cv2.putText(frame, \"Fall\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    else:\n",
    "        frame = cv2.putText(frame, \"No Fall\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Detected Faces', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "video.release()\n",
    "\n",
    "# Destroy all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae56f3e-7402-4634-965d-c24b1bd6d56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 135.9ms\n",
      "Speed: 4.1ms preprocess, 135.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 188.6ms\n",
      "Speed: 2.0ms preprocess, 188.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 131.6ms\n",
      "Speed: 6.9ms preprocess, 131.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 3 laptops, 1 mouse, 1 remote, 1 book, 136.7ms\n",
      "Speed: 0.0ms preprocess, 136.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 135.9ms\n",
      "Speed: 4.7ms preprocess, 135.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 131.6ms\n",
      "Speed: 6.0ms preprocess, 131.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 125.4ms\n",
      "Speed: 0.0ms preprocess, 125.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 146.6ms\n",
      "Speed: 1.5ms preprocess, 146.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 126.8ms\n",
      "Speed: 4.9ms preprocess, 126.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 133.8ms\n",
      "Speed: 5.0ms preprocess, 133.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 147.4ms\n",
      "Speed: 0.0ms preprocess, 147.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 131.0ms\n",
      "Speed: 6.0ms preprocess, 131.0ms inference, 8.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 135.5ms\n",
      "Speed: 0.0ms preprocess, 135.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 124.3ms\n",
      "Speed: 5.1ms preprocess, 124.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 126.4ms\n",
      "Speed: 0.0ms preprocess, 126.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 120.9ms\n",
      "Speed: 4.0ms preprocess, 120.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 143.2ms\n",
      "Speed: 0.0ms preprocess, 143.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 120.5ms\n",
      "Speed: 0.0ms preprocess, 120.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 126.0ms\n",
      "Speed: 3.5ms preprocess, 126.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 127.1ms\n",
      "Speed: 0.0ms preprocess, 127.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 120.9ms\n",
      "Speed: 4.0ms preprocess, 120.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 127.3ms\n",
      "Speed: 4.6ms preprocess, 127.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 117.8ms\n",
      "Speed: 2.3ms preprocess, 117.8ms inference, 9.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 127.2ms\n",
      "Speed: 0.0ms preprocess, 127.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 117.2ms\n",
      "Speed: 4.9ms preprocess, 117.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 118.1ms\n",
      "Speed: 4.8ms preprocess, 118.1ms inference, 9.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 2 laptops, 1 mouse, 1 remote, 1 book, 126.9ms\n",
      "Speed: 3.5ms preprocess, 126.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 126.9ms\n",
      "Speed: 0.0ms preprocess, 126.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 126.9ms\n",
      "Speed: 3.0ms preprocess, 126.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 118.1ms\n",
      "Speed: 5.1ms preprocess, 118.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 127.1ms\n",
      "Speed: 0.2ms preprocess, 127.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 123.6ms\n",
      "Speed: 4.8ms preprocess, 123.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 117.7ms\n",
      "Speed: 0.0ms preprocess, 117.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 122.6ms\n",
      "Speed: 0.0ms preprocess, 122.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 122.1ms\n",
      "Speed: 4.4ms preprocess, 122.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 bottle, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 119.1ms\n",
      "Speed: 0.0ms preprocess, 119.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 bottle, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 136.3ms\n",
      "Speed: 4.2ms preprocess, 136.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 bottle, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 126.4ms\n",
      "Speed: 2.0ms preprocess, 126.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 123.3ms\n",
      "Speed: 4.3ms preprocess, 123.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 134.3ms\n",
      "Speed: 0.0ms preprocess, 134.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 133.9ms\n",
      "Speed: 0.0ms preprocess, 133.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 126.8ms\n",
      "Speed: 0.0ms preprocess, 126.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 2 dining tables, 1 mouse, 1 remote, 1 book, 119.5ms\n",
      "Speed: 0.0ms preprocess, 119.5ms inference, 15.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 2 dining tables, 1 mouse, 1 remote, 1 book, 126.6ms\n",
      "Speed: 0.0ms preprocess, 126.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 bottle, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 127.5ms\n",
      "Speed: 0.0ms preprocess, 127.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 2 dining tables, 1 mouse, 1 remote, 1 book, 129.4ms\n",
      "Speed: 5.0ms preprocess, 129.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 bottle, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 126.0ms\n",
      "Speed: 0.0ms preprocess, 126.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 2 dining tables, 1 mouse, 1 remote, 1 book, 143.6ms\n",
      "Speed: 7.0ms preprocess, 143.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 2 dining tables, 1 mouse, 1 remote, 1 book, 126.5ms\n",
      "Speed: 0.0ms preprocess, 126.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 130.9ms\n",
      "Speed: 0.0ms preprocess, 130.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 remote, 1 book, 128.1ms\n",
      "Speed: 4.7ms preprocess, 128.1ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 117.8ms\n",
      "Speed: 0.0ms preprocess, 117.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 142.9ms\n",
      "Speed: 0.0ms preprocess, 142.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 2 dining tables, 1 mouse, 1 remote, 1 book, 124.7ms\n",
      "Speed: 0.0ms preprocess, 124.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 135.8ms\n",
      "Speed: 0.0ms preprocess, 135.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 135.8ms\n",
      "Speed: 0.0ms preprocess, 135.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 book, 134.8ms\n",
      "Speed: 7.1ms preprocess, 134.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 2 dining tables, 1 mouse, 1 remote, 1 book, 130.0ms\n",
      "Speed: 5.2ms preprocess, 130.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 2 dining tables, 1 mouse, 1 book, 126.8ms\n",
      "Speed: 0.0ms preprocess, 126.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 book, 125.5ms\n",
      "Speed: 3.3ms preprocess, 125.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 book, 119.2ms\n",
      "Speed: 3.6ms preprocess, 119.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 book, 125.4ms\n",
      "Speed: 4.3ms preprocess, 125.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 book, 119.2ms\n",
      "Speed: 0.0ms preprocess, 119.2ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 book, 131.8ms\n",
      "Speed: 0.0ms preprocess, 131.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 book, 120.8ms\n",
      "Speed: 0.0ms preprocess, 120.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 book, 118.7ms\n",
      "Speed: 4.6ms preprocess, 118.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 remote, 1 book, 123.9ms\n",
      "Speed: 0.0ms preprocess, 123.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 book, 127.7ms\n",
      "Speed: 5.1ms preprocess, 127.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 book, 127.2ms\n",
      "Speed: 4.0ms preprocess, 127.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 2 dining tables, 1 mouse, 1 book, 117.8ms\n",
      "Speed: 0.0ms preprocess, 117.8ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 book, 124.9ms\n",
      "Speed: 2.4ms preprocess, 124.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 remote, 1 book, 148.6ms\n",
      "Speed: 1.6ms preprocess, 148.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 book, 127.0ms\n",
      "Speed: 0.0ms preprocess, 127.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 121.4ms\n",
      "Speed: 5.0ms preprocess, 121.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 119.3ms\n",
      "Speed: 0.0ms preprocess, 119.3ms inference, 15.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 136.0ms\n",
      "Speed: 0.0ms preprocess, 136.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 119.2ms\n",
      "Speed: 0.0ms preprocess, 119.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 127.4ms\n",
      "Speed: 3.5ms preprocess, 127.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 136.0ms\n",
      "Speed: 0.0ms preprocess, 136.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 127.0ms\n",
      "Speed: 6.6ms preprocess, 127.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 book, 118.8ms\n",
      "Speed: 5.8ms preprocess, 118.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 book, 124.5ms\n",
      "Speed: 0.0ms preprocess, 124.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 book, 135.2ms\n",
      "Speed: 0.0ms preprocess, 135.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 135.5ms\n",
      "Speed: 3.1ms preprocess, 135.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 135.4ms\n",
      "Speed: 5.5ms preprocess, 135.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 127.0ms\n",
      "Speed: 0.0ms preprocess, 127.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 127.2ms\n",
      "Speed: 0.0ms preprocess, 127.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 book, 135.2ms\n",
      "Speed: 0.0ms preprocess, 135.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 remote, 1 book, 142.5ms\n",
      "Speed: 0.0ms preprocess, 142.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 2 dining tables, 1 mouse, 1 book, 125.2ms\n",
      "Speed: 8.0ms preprocess, 125.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 2 dining tables, 1 mouse, 1 book, 126.8ms\n",
      "Speed: 0.0ms preprocess, 126.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 chair, 2 dining tables, 1 mouse, 1 remote, 1 book, 123.8ms\n",
      "Speed: 5.4ms preprocess, 123.8ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 145.5ms\n",
      "Speed: 4.0ms preprocess, 145.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 120.0ms\n",
      "Speed: 5.5ms preprocess, 120.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 127.2ms\n",
      "Speed: 3.0ms preprocess, 127.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 135.5ms\n",
      "Speed: 3.1ms preprocess, 135.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 115.0ms\n",
      "Speed: 3.9ms preprocess, 115.0ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 129.9ms\n",
      "Speed: 4.9ms preprocess, 129.9ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 127.0ms\n",
      "Speed: 0.0ms preprocess, 127.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 124.2ms\n",
      "Speed: 0.0ms preprocess, 124.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 151.2ms\n",
      "Speed: 0.0ms preprocess, 151.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 book, 119.0ms\n",
      "Speed: 3.5ms preprocess, 119.0ms inference, 15.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 book, 133.4ms\n",
      "Speed: 0.0ms preprocess, 133.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 125.9ms\n",
      "Speed: 0.0ms preprocess, 125.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 119.7ms\n",
      "Speed: 6.8ms preprocess, 119.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 4 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 126.8ms\n",
      "Speed: 11.9ms preprocess, 126.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 4 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 121.7ms\n",
      "Speed: 0.0ms preprocess, 121.7ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 121.7ms\n",
      "Speed: 4.7ms preprocess, 121.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 128.5ms\n",
      "Speed: 4.2ms preprocess, 128.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 2 dining tables, 1 mouse, 1 remote, 1 book, 135.9ms\n",
      "Speed: 0.0ms preprocess, 135.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 141.7ms\n",
      "Speed: 0.0ms preprocess, 141.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 139.5ms\n",
      "Speed: 9.4ms preprocess, 139.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 cup, 1 dining table, 1 mouse, 1 remote, 126.2ms\n",
      "Speed: 0.0ms preprocess, 126.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 131.6ms\n",
      "Speed: 0.0ms preprocess, 131.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 142.3ms\n",
      "Speed: 0.0ms preprocess, 142.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 126.1ms\n",
      "Speed: 1.0ms preprocess, 126.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 143.7ms\n",
      "Speed: 4.3ms preprocess, 143.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 126.4ms\n",
      "Speed: 0.0ms preprocess, 126.4ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 142.3ms\n",
      "Speed: 0.0ms preprocess, 142.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 138.5ms\n",
      "Speed: 3.3ms preprocess, 138.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 130.2ms\n",
      "Speed: 11.0ms preprocess, 130.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 129.7ms\n",
      "Speed: 5.0ms preprocess, 129.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 126.0ms\n",
      "Speed: 4.4ms preprocess, 126.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 126.5ms\n",
      "Speed: 4.1ms preprocess, 126.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 167.3ms\n",
      "Speed: 0.0ms preprocess, 167.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 135.0ms\n",
      "Speed: 3.0ms preprocess, 135.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 142.0ms\n",
      "Speed: 0.0ms preprocess, 142.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 cup, 1 dining table, 1 laptop, 1 mouse, 1 remote, 1 book, 131.8ms\n",
      "Speed: 0.0ms preprocess, 131.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 tie, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 134.9ms\n",
      "Speed: 0.0ms preprocess, 134.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 130.1ms\n",
      "Speed: 3.5ms preprocess, 130.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 1 book, 122.4ms\n",
      "Speed: 3.2ms preprocess, 122.4ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 123.4ms\n",
      "Speed: 0.0ms preprocess, 123.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 130.2ms\n",
      "Speed: 5.0ms preprocess, 130.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 remote, 127.7ms\n",
      "Speed: 0.0ms preprocess, 127.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 1 cup, 1 dining table, 1 mouse, 1 book, 106.5ms\n",
      "Speed: 5.3ms preprocess, 106.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 mouse, 1 book, 134.4ms\n",
      "Speed: 0.0ms preprocess, 134.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 mouse, 125.9ms\n",
      "Speed: 3.4ms preprocess, 125.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 remote, 1 book, 126.2ms\n",
      "Speed: 0.0ms preprocess, 126.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 cup, 1 dining table, 1 book, 120.7ms\n",
      "Speed: 5.0ms preprocess, 120.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 book, 141.0ms\n",
      "Speed: 2.9ms preprocess, 141.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 113.6ms\n",
      "Speed: 6.1ms preprocess, 113.6ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 cup, 1 dining table, 1 mouse, 134.7ms\n",
      "Speed: 0.0ms preprocess, 134.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 dog, 1 bottle, 1 cup, 1 dining table, 1 mouse, 125.2ms\n",
      "Speed: 4.6ms preprocess, 125.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 1 bottle, 1 cup, 1 dining table, 134.7ms\n",
      "Speed: 0.0ms preprocess, 134.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 cup, 1 chair, 1 dining table, 141.2ms\n",
      "Speed: 0.0ms preprocess, 141.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 cup, 1 chair, 1 dining table, 110.8ms\n",
      "Speed: 5.2ms preprocess, 110.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 dog, 1 cup, 1 dining table, 125.0ms\n",
      "Speed: 0.0ms preprocess, 125.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 dog, 1 cup, 1 dining table, 118.5ms\n",
      "Speed: 0.0ms preprocess, 118.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 cup, 3 dining tables, 1 remote, 1 cell phone, 115.8ms\n",
      "Speed: 2.0ms preprocess, 115.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 dogs, 1 cup, 2 dining tables, 1 cell phone, 129.7ms\n",
      "Speed: 0.0ms preprocess, 129.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 2 dogs, 1 bottle, 1 cup, 1 dining table, 126.0ms\n",
      "Speed: 0.0ms preprocess, 126.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 cup, 1 dining table, 1 remote, 135.0ms\n",
      "Speed: 0.0ms preprocess, 135.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n",
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 dining table, 1 remote, 1 cell phone, 127.4ms\n",
      "Speed: 4.9ms preprocess, 127.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 1 dog, 1 bottle, 1 cup, 1 chair, 1 dining table, 135.4ms\n",
      "Speed: 0.0ms preprocess, 135.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 dog, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 mouse, 125.1ms\n",
      "Speed: 0.0ms preprocess, 125.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 remote, 135.9ms\n",
      "Speed: 4.7ms preprocess, 135.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 remote, 125.9ms\n",
      "Speed: 0.0ms preprocess, 125.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 remote, 135.0ms\n",
      "Speed: 0.0ms preprocess, 135.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 mouse, 117.3ms\n",
      "Speed: 5.6ms preprocess, 117.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 1 dining table, 149.0ms\n",
      "Speed: 0.0ms preprocess, 149.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 dog, 1 bottle, 1 cup, 1 chair, 1 dining table, 2 remotes, 127.2ms\n",
      "Speed: 0.0ms preprocess, 127.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bird, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 remote, 135.1ms\n",
      "Speed: 5.8ms preprocess, 135.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bird, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 remote, 119.5ms\n",
      "Speed: 3.6ms preprocess, 119.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bird, 1 bottle, 1 cup, 1 chair, 1 dining table, 2 remotes, 126.1ms\n",
      "Speed: 6.7ms preprocess, 126.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 1 dining table, 2 remotes, 129.1ms\n",
      "Speed: 6.4ms preprocess, 129.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 remote, 129.5ms\n",
      "Speed: 5.6ms preprocess, 129.5ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 remote, 135.5ms\n",
      "Speed: 0.0ms preprocess, 135.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Fall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 3 persons, 1 bottle, 1 cup, 1 chair, 1 dining table, 1 mouse, 1 remote, 137.4ms\n",
      "Speed: 5.9ms preprocess, 137.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "No Fall\n"
     ]
    }
   ],
   "source": [
    "# cap = cv2.VideoCapture(\"/Users/gdiva/Downloads/gettyimages-1286237053-640_adpp.mp4\")\n",
    "# while True:\n",
    "#     # Read a frame from the video source\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     if not ret:\n",
    "#         break  # Break the loop if the video has ended\n",
    "\n",
    "#     # Resize the frame (optional)\n",
    "#     frame = cv2.resize(frame, (640, 480))\n",
    "#     results = model2.predict(frame, boxes=True)\n",
    "#     result = results[0]\n",
    "#     class_labels = []\n",
    "#     for box in result.boxes:\n",
    "#         class_id = result.names[box.cls[0].item()]\n",
    "#         if (class_id == 'person') :\n",
    "#             cords = box.xyxy[0].tolist()\n",
    "#             cords = [round(x) for x in cords]\n",
    "#             [x1, y1, x2, y2] = cords\n",
    "#             cv2.rectangle(frame, (x1,y1), (x2,y2), [0,0,255], 2)\n",
    "#             image = np.copy(frame[y1:y2, x1:x2])\n",
    "#             class_label = predict_image(image)\n",
    "#             class_labels.append(class_label)\n",
    "#             print(class_label)\n",
    "\n",
    "#     class_labels = np.array(class_labels)\n",
    "#     if np.any(class_labels == \"Fall\"):\n",
    "#         frame = cv2.putText(frame, \"Fall\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "#     else:\n",
    "#         frame = cv2.putText(frame, \"No Fall\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "#     cv2.imshow('frame', frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
